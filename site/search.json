{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"Linux/filesystem/management/","level":1,"title":"Filesystem Management Guide","text":"","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#overview","level":2,"title":"Overview","text":"<p>General idea on managing filesystems. This shows specifically <code>ext4</code> and <code>xfs</code> as that's what I used, but there are plently of other options. </p>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#1-identification","level":2,"title":"1. Identification","text":"<p>Before modifying any disks, identifying the correct device identifiers is critical to prevent data loss.</p> <pre><code>lsblk -o NAME,MODEL,SIZE,TYPE,FSTYPE\n</code></pre> <p>Reference Output:</p> <pre><code>NAME                      MODEL                            SIZE TYPE FSTYPE\nsda                       SanDisk SD8TB8U2               238.5G disk \n└─sda1                                                   238.5G part ext4\nsdb                       WDC WD120EFGX-68                10.9T disk \n└─sdb1                                                    10.9T part xfs\nsdc                       WDC WD120EFGX-68                10.9T disk \n└─sdc1                                                    10.9T part xfs\nsdd                       WDC WD120EFGX-68                10.9T disk \n└─sdd1                                                    10.9T part xfs\nnvme0n1                   WDC PC SN730 SDBQNTY-256G-1001 238.5G disk \n├─nvme0n1p1                                                  1M part \n├─nvme0n1p2                                                  2G part ext4\n└─nvme0n1p3                                              236.5G part LVM2_member\n  └─ubuntu--vg-ubuntu--lv                                  100G lvm  ext4\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#2-partitioning-formatting","level":2,"title":"2. Partitioning &amp; Formatting","text":"<p>You can choose between a scriptable command-line approach or an interactive visual tool.</p>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#option-a-command-line-scriptable","level":3,"title":"Option A: Command Line (Scriptable)","text":"","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#ssd-setup-ext4","level":4,"title":"SSD Setup (ext4)","text":"<p>Target: <code>/dev/sda</code> (Appdata/Cache)</p> <pre><code># 1. Wipe old filesystem signatures\nsudo wipefs -a /dev/sda\n\n# Use parted for GPT specifically (Recommended for drives &gt;2TB)\nsudo parted -s /dev/sda mklabel gpt \n\n# 3. Format to EXT4\nsudo mkfs.ext4 -L \"appdata\" /dev/sda1\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#hdd-setup-xfs","level":4,"title":"HDD Setup (XFS)","text":"<p>Target: <code>/dev/sdb</code>, <code>/dev/sdc</code>, <code>/dev/sdd</code> (Mass Storage)</p> <pre><code># Drive 1 (sdb)\nsudo wipefs -a /dev/sdb\nsudo parted -s /dev/sdb mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"disk1\" /dev/sdb1\n\n# Drive 2 (sdc)\nsudo wipefs -a /dev/sdc\nsudo parted -s /dev/sdc mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"disk2\" /dev/sdc1\n\n# Drive 3 (sdd)\nsudo wipefs -a /dev/sdd\nsudo parted -s /dev/sdd mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"parity1\" /dev/sdd1\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#option-b-interactive-visual","level":3,"title":"Option B: Interactive (Visual)","text":"<p>For a visual interface, use <code>cfdisk</code>.</p> <p>Formatting Required</p> <p><code>cfdisk</code> only creates the partition. You must still run the <code>mkfs</code> commands listed in Option A to format the filesystem after creating the partitions.</p> <ol> <li>Launch Tool: <code>sudo cfdisk /dev/sda</code> (Repeat for <code>sdb</code>, <code>sdc</code>, <code>sdd</code>).</li> <li>Label Type: Select gpt.</li> <li>Delete: Remove existing partitions if necessary.</li> <li>New: Select [ New ] -&gt; [ Enter ] (Use max size).</li> <li>Write: Select [ Write ] -&gt; Type <code>yes</code>.</li> <li>Quit: Select [ Quit ].</li> </ol>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#3-mounting-persistence","level":2,"title":"3. Mounting &amp; Persistence","text":"","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#get-uuids","level":3,"title":"Get UUIDs","text":"<p>Use UUIDs for mounting. Unlike <code>/dev/sdX</code> names, UUIDs do not change if drive order changes. You'll need to map them using these or they may not show on reboot</p> <pre><code>ls -l /dev/disk/by-uuid/\n</code></pre> <p>Output:</p> <pre><code>lrwxrwxrwx 1 root root 10 Dec  7 05:34 02de315f-8ced-4745-bb4f-2d24c46efa48 -&gt; ../../sdb1\nlrwxrwxrwx 1 root root 15 Dec  7 04:33 3c26dce0-7f10-426c-bd67-e9f2d77889ee -&gt; ../../nvme0n1p2\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 603999da-c2dd-484d-80bf-ab4977680e90 -&gt; ../../sdc1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 844cae61-9599-4a57-9431-18c0a33904e4 -&gt; ../../sda1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 9860056e-b314-4cb9-acc5-30e76281795b -&gt; ../../sdd1\nlrwxrwxrwx 1 root root 10 Dec  7 04:33 d5ac7faa-876c-43ff-b27a-94c53bf79bb2 -&gt; ../../dm-0\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#create-mount-points","level":3,"title":"Create Mount Points","text":"<p>Create the directories where the drives will be accessed.</p> <pre><code>sudo mkdir -p /mnt/disk1\nsudo mkdir -p /mnt/disk2\nsudo mkdir -p /mnt/parity1\nsudo mkdir -p /mnt/appdata\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#edit-fstab","level":3,"title":"Edit fstab","text":"<p>Add the following to the bottom of <code>/etc/fstab</code> to ensure drives mount automatically at boot.</p> <pre><code># ---------------------------------------------------------\n# DATA DRIVES (XFS) - 12TB WD Reds\n# ---------------------------------------------------------\n# Disk 1 (sdb1)\nUUID=02de315f-8ced-4745-bb4f-2d24c46efa48 /mnt/disk1 xfs defaults,noatime 0 0\n\n# Disk 2 (sdc1)\nUUID=603999da-c2dd-484d-80bf-ab4977680e90 /mnt/disk2 xfs defaults,noatime 0 0\n\n# Parity Drive (sdd1)\nUUID=9860056e-b314-4cb9-acc5-30e76281795b /mnt/parity1 xfs defaults,noatime 0 0\n\n# ---------------------------------------------------------\n# FAST STORAGE (EXT4) - 240GB SSD\n# ---------------------------------------------------------\n# Appdata/Cache (sda1)\nUUID=844cae61-9599-4a57-9431-18c0a33904e4 /mnt/appdata ext4 defaults,noatime 0 0\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#reload-configuration","level":3,"title":"Reload Configuration","text":"<p>After editing <code>fstab</code>, verify the syntax and reload system daemons.</p> <pre><code>sudo findmnt --verify\nsudo systemctl daemon-reload\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#4-verification","level":2,"title":"4. Verification","text":"<p>Perform these checks before rebooting to ensure the configuration is valid.</p> <ol> <li> <p>Test Mount: <pre><code># No output is good news\nsudo mount -a\n</code></pre></p> </li> <li> <p>Check Sizing: <pre><code>df -h\n</code></pre></p> </li> <li> <p><code>disk1</code>, <code>disk2</code>, <code>parity1</code>: Should be ~11T</p> </li> <li> <p><code>appdata</code>: Should be ~220G</p> </li> <li> <p>Check Permissions: <pre><code>ls -ld /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> </ol> <p>If listed as <code>root</code>, change ownership to your specific user (e.g., <code>goose</code>): <pre><code>sudo chown -R goose:goose /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#5-docker-integration-reference","level":2,"title":"5. Docker Integration Reference","text":"<p>Once mounted, pass these paths to containers via <code>compose.yaml</code>.</p>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#filebrowser-file-management","level":3,"title":"FileBrowser (File Management)","text":"<pre><code>services:\n  filebrowser:\n    volumes:\n      # Map Host Path : Container Path\n      - /mnt/nvme_storage:/mnt/nvme_storage\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#beszel-system-monitoring","level":3,"title":"Beszel (System Monitoring)","text":"<pre><code>services:\n  beszel-agent:\n    volumes:\n      - /mnt/nvme_storage:/mnt/nvme_storage:ro\n    environment:\n      # Comma-separated list of mount points to monitor\n      - EXTRA_FILESYSTEMS=/mnt/nvme_storage\n</code></pre>","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#6-general-cli-cheat-sheet","level":2,"title":"6. General CLI Cheat Sheet","text":"","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#identification-hardware-info","level":3,"title":"Identification &amp; Hardware Info","text":"Command Description <code>lsblk</code> List Block Devices. The best \"at a glance\" view of disks, partitions, and mount points. <code>lsblk -f</code> Lists devices plus filesystem types (ext4, ntfs) and UUIDs. <code>sudo fdisk -l</code> Detailed low-level partition table dump. Good for verifying sector sizes. <code>sudo lshw -class disk</code> Hardware details (model numbers, serial numbers, firmware). <code>sudo blkid</code> Prints the UUIDs and labels for all block devices.","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#disk-usage-space","level":3,"title":"Disk Usage &amp; Space","text":"Command Description <code>df -h</code> Disk Free. Shows total/used/avail space in \"Human Readable\" format (GB/TB). <code>du -sh /path/to/dir</code> Disk Usage. Calculates the size of a specific directory. <code>ncdu</code> Interactive Usage. A visual, navigable tool to find large files. (Requires install).","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#health-maintenance","level":3,"title":"Health &amp; Maintenance","text":"Command Description <code>sudo smartctl -a /dev/sda</code> SMART Data. Checks physical health, temp, and error logs (Requires <code>smartmontools</code>). <code>sudo fsck /dev/sda1</code> File System Check. Repairs corruption. Only run on unmounted drives.","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/management/#mounting-operations","level":3,"title":"Mounting Operations","text":"Command Description <code>sudo mount /dev/sdb1 /mnt/usb</code> Manually mounts a partition to a folder. <code>sudo umount /mnt/usb</code> Unmounts the drive safely. <code>sudo mount -a</code> Mounts everything listed in <code>/etc/fstab</code> that isn't already mounted.","path":["Linux","Filesystem","Filesystem Management Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/","level":1,"title":"MergerFS Configuration Guide","text":"","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#overview","level":2,"title":"Overview","text":"<p>MergerFS is a union filesystem designed to simplify storage management. It pools multiple storage devices (branches) into a single unified directory (mount point).</p> <p>Unlike RAID, MergerFS does not strip data across drives. Files exist intact on the individual backing drives.</p> <ul> <li>Flexibility: Drives of different sizes, filesystems, and speeds can be mixed.</li> <li>Resilience: If one drive fails, only the data on that specific drive is lost; the rest of the pool remains accessible.</li> <li>Dynamic: Drives can be added or removed from the pool without rebuilding the array.</li> </ul>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#installation","level":2,"title":"Installation","text":"<p>MergerFS is available in most standard Linux repositories, though building from source ensures the latest feature set.</p> <pre><code># Standard installation\nsudo apt install mergerfs fuse\n</code></pre> <p>For latest versions/source: GitHub Repository</p>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#branch-setup-preparation","level":2,"title":"Branch Setup &amp; Preparation","text":"<p>Warning</p> <p>Never write data directly to the <code>/mnt/disk1</code> folders if you can help it. Always write to <code>/mnt/storage</code>. If you modify files on the backing disks directly while MergerFS is running, you might see cache inconsistencies until you remount.</p> <p>Before creating the pool, the underlying mount points (branches) should be organized and secured.</p>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#1-naming-convention","level":3,"title":"1. Naming Convention","text":"<p>Using a consistent naming structure helps identify drive types within the pool.</p> <ul> <li>HDDs: <code>/mnt/hdd/Serial-Number</code></li> <li>SATA SSDs: <code>/mnt/ssd/Serial-Number</code></li> <li>NVMe: <code>/mnt/nvme/Serial-Number</code></li> <li>Remote Shares: <code>/mnt/remote/Share-Name</code></li> </ul> <p>Example Layout:</p> <pre><code>/mnt/\n├── hdd/\n│   ├── 10T-01234567\n│   └── 20T-12345678\n└── nvme/\n    └── 1T-ABCDEFGH\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#2-permissions-locking-critical","level":3,"title":"2. Permissions &amp; Locking (Critical)","text":"","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#mount-points","level":4,"title":"mount points","text":"<p>To ensure the directory is only used as a point to mount another filesystem it is good to lock it down as much as possible. Be sure to do this before mounting a filesystem to it.</p> <p>Run these commands for each underlying drive mount point:</p> <pre><code># 1. Set ownership to root\nsudo chown root:root /mnt/hdd/10T-XYZ\n\n# 2. Remove read/write/execute permissions (makes it inaccessible directly)\nsudo chmod 0000 /mnt/hdd/10T-XYZ\n\n# 3. Mark as a specific branch mount (prevents mounting if drive is missing)\nsudo setfattr -n user.mergerfs.branch_mounts_here\n</code></pre> <p>The extended attribute user.mergerfs.branch_mounts_here is used by the branches-mount-timeout option to recognize whether or not a mergerfs branch path points to the intended filesystem.</p> <p>The chattr is likely to only work on EXT{2,3,4} filesystems but will restrict even root from modifying the directory or its content but is still able to be a mount point.</p>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#mounted-filesystems","level":4,"title":"mounted filesystems","text":"<p>For those new to Linux, intending to be the primary individual logged into the system, or simply want to simplify permissions it is recommended to set the root of mounted filesystems like /tmp/ is set to. Owned by root, ugo+rwx and sticky bit set.</p> <p>This must be done after mounting the filesystem to the target mount point.</p> <pre><code>$ sudo chown root:root /mnt/hdd/10T-SERIALNUM\n$ sudo chmod 1777 /mnt/hdd/10T-SERIALNUM\n$ sudo setfattr -n user.mergerfs.branch /mnt/hdd/10T-SERIALNUM\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#pool-configuration","level":2,"title":"Pool Configuration","text":"","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#1-create-pool-directory","level":3,"title":"1. Create Pool Directory","text":"<p>Info</p> <p>For Linux v6.6 and above (defaults - quick start)</p> <p>-cache.files=off</p> <p>-category.create=pfrd</p> <p>-func.getattr=newest</p> <p>-dropcacheonclose=false</p> <p>Create the unified mount point where applications will access the data.</p> <pre><code>sudo mkdir -p /mnt/storage\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#2-command-line-test","level":3,"title":"2. Command Line Test","text":"<p>Test the pool creation manually before making it persistent.</p> <p>Note</p> <p>These settings are just what I used. Your situation may benefit from different layout. Read the docs to see what would work best. </p> <pre><code>mergerfs -o cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=200G,moveonenospc=true /mnt/disk*/mnt /mnt/storage\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#3-persistence-etcfstab","level":3,"title":"3. Persistence (<code>/etc/fstab</code>)","text":"<p>Add the configuration to <code>/etc/fstab</code> to ensure the pool mounts at boot.</p> <p>Recommended Options (Linux Kernel v6.6+):</p> <ul> <li><code>cache.files=off</code>: Disables page caching (reduces RAM usage/complexity).</li> <li><code>category.create=mspmfs</code>: Most Space, Path Most Free Space. Writes new files to the drive with the most free space, unless the path already exists on another drive.</li> <li><code>func.getattr=newest</code>: Returns file attributes from the file with the newest <code>mtime</code>. Critical for apps like Plex/Kodi to detect changes.</li> <li><code>minfreespace=200G</code>: Prevents filling a drive completely; moves to the next drive when 200GB remains.</li> </ul> <p>Add to <code>/etc/fstab</code>:</p> <pre><code># &lt;branches&gt; &lt;mountpoint&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;\n/mnt/hdd/WD-B00WHLZD:/mnt/hdd/WD-B00WUHXD /mnt/storage mergerfs cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=200G,moveonenospc=true,fsname=mergerfs 0 0\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#4-verification","level":3,"title":"4. Verification","text":"<p>Reload the system daemon and mount the pool.</p> <pre><code># Verify syntax\nsudo findmnt --verify\n\n# Reload fstab\nsudo systemctl daemon-reload\n\n# Mount all\nsudo mount -a\n\n# Check size (Should equal sum of all drives)\ndf -h\n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#docker-integration","level":2,"title":"Docker Integration","text":"<p>When configuring containers, always use the unified <code>/mnt/storage</code> path. Never bind volumes to the individual disks (e.g., <code>/mnt/disk1</code>), as this bypasses MergerFS logic.</p> <p>Example <code>compose.yaml</code>:</p> <pre><code>services:\n  plex:\n    image: linuxserver/plex\n    volumes:\n      # Correct: Uses the pool\n      - /mnt/storage/media/movies:/movies\n      - /mnt/storage/media/tv:/tv\n\n      # INCORRECT: Do not map directly to backing disks\n      # - /mnt/disk1/media/movies:/movies \n</code></pre>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#maintenance-troubleshooting","level":2,"title":"Maintenance &amp; Troubleshooting","text":"","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#handling-cacheddeleted-files","level":3,"title":"Handling Cached/Deleted Files","text":"<p>If a file is deleted from the pool but appears to persist (ghost file), or if changes aren't reflecting immediately, the filesystem cache may need clearing.</p> <pre><code>sync &amp;&amp; echo 3 | sudo tee /proc/sys/vm/drop_caches\n</code></pre> <ul> <li>Note: Deleting a file from the pool removes it from the underlying drive.</li> </ul>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/mergerfs/#common-issues","level":3,"title":"Common Issues","text":"<ul> <li>Permissions: Run MergerFS as <code>root</code>. Running as a standard user can cause unpredictable behavior.</li> <li>Missing Files: If directories appear missing or permissions seem erratic, ensure the permissions on the underlying physical drives are identical. Use <code>mergerfs.fsck</code> to audit synchronization.</li> <li>Scanner Lag (Plex/Kodi): If media scanners miss new files, ensure <code>func.getattr=newest</code> is enabled in the options. This forces the pool to report the most recent modification time found across all branches.</li> </ul>","path":["Linux","Filesystem","MergerFS Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/","level":1,"title":"SnapRAID Configuration Guide","text":"<p>SnapRAID Official Docs</p>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#overview","level":2,"title":"Overview","text":"<p>SnapRAID is a backup program for disk arrays. It stores parity information of the data to recover from up to six disk failures.</p> <p>Unlike traditional RAID 5 or 6, SnapRAID is not real-time; it is a snapshot-based system.</p> <ul> <li>Provide fault tolerance to protect against (inevitable) hard drive failure</li> <li>Checksum files to guard against bitrot</li> <li>Support hard drives of differing / mismatched sizes</li> <li>Enable incremental upgrading of hard drives in batches as small as one</li> <li>Each drive should have a separately readable filesystem with no striping of data</li> </ul>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#the-parity-concept","level":3,"title":"The Parity Concept","text":"<p>SnapRAID requires dedicating specific disks to \"Parity.\"</p> <ul> <li>Parity Level: One parity disk protects against one drive failure. Two parity disks protect against two failures, and so on.</li> <li>Sizing: The parity disk must be equal to or larger than the largest single data disk in the array.</li> <li>Storage: Parity disks contain only the large <code>.parity</code> file; they cannot be used for standard data storage.</li> </ul>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#installation","level":2,"title":"Installation","text":"<p>To ensure the latest features and compatibility, SnapRAID is often compiled from source.</p>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#compile-from-source","level":3,"title":"Compile from Source","text":"<p>Run the following commands to download, compile, and install version 13.0.</p> <pre><code># Download source\nwget https://github.com/amadvance/snapraid/releases/download/v13.0/snapraid-13.0.tar.gz\n\n# Extract archive\ntar xzvf snapraid-13.0.tar.gz\n\n# Enter directory\ncd snapraid\n\n# Compile and Install\n./autogen.sh\n./configure\nmake\nmake check\nsudo make install\n\n# Verify installation\nsnapraid -V\n</code></pre>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#configuration","level":2,"title":"Configuration","text":"<p>SnapRAID is configured via a single text file located at <code>/etc/snapraid.conf</code>. This file defines the parity volumes, content files, and data disks.</p>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#key-components","level":3,"title":"Key Components","text":"<ol> <li>Parity Files: The destination for redundancy data.</li> <li>Content Files: These act as the \"index\" or \"database\" of the array. They contain checksums and file maps.</li> <li> <p>Critical: Multiple copies should be stored on separate physical disks (e.g., one on the boot drive, others on data drives) to ensure the index survives a disk failure.</p> </li> <li> <p>Data Disks: The actual storage drives containing the media/files to protect.</p> </li> </ol>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#configuration-example-etcsnapraidconf","level":3,"title":"Configuration Example (<code>/etc/snapraid.conf</code>)","text":"<p>Create or edit the file using a text editor:</p> <pre><code># --- Parity Location --- \n# The parity file must be on the dedicated parity drive\nparity /mnt/parity1/snapraid.parity \n\n# --- Content files ---\n# Store copies on multiple physical drives for safety\ncontent /var/snapraid/snapraid.content\ncontent /mnt/disk1/snapraid.content\ncontent /mnt/disk2/snapraid.content\n\n# --- Data Disks ---\n# Assign a name (d1, d2) and a mount point for each disk\ndata d1 /mnt/disk1\ndata d2 /mnt/disk2\n\n# --- Excludes ---\n# Prevent temporary files from breaking the sync\nexclude *.unrecoverable\nexclude /tmp/\nexclude /lost+found/\n</code></pre>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#usage-maintenance","level":2,"title":"Usage &amp; Maintenance","text":"<p>Because SnapRAID is snapshot-based, the array is not protected until a \"sync\" is performed.</p>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#syncing","level":3,"title":"Syncing","text":"<p>Run the <code>sync</code> command to update the parity information. This reads data from the data disks and computes the parity.</p> <pre><code>snapraid sync\n</code></pre> <ul> <li>Note: The first sync may take several hours. Subsequent syncs only process changed data.</li> <li>Interruptible: The process can be stopped (<code>Ctrl+C</code>) and resumed later.</li> </ul>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#scrubbing","level":3,"title":"Scrubbing","text":"<p>Scrubbing checks the data and parity for silent corruption (bitrot).</p> <pre><code>snapraid scrub\n</code></pre> <ul> <li>Default Behavior: Checks approximately 8% of the array that hasn't been scrubbed in the last 10 days.</li> <li>Custom Plan: To check a specific percentage (e.g., 5%) of blocks older than 20 days: <pre><code>snapraid -p 5 -o 20 scrub\n</code></pre></li> </ul>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#automation","level":3,"title":"Automation","text":"<p>Since SnapRAID requires manual commands, it is highly recommended to automate these tasks using scripts. The SnapRAID AIO Script is a popular community solution for automating syncs, scrubs, and monitoring.</p>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#recovery","level":2,"title":"Recovery","text":"","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#undeleting-files","level":3,"title":"Undeleting Files","text":"<p>If a file is accidentally deleted, SnapRAID can restore it to its previous state (like a backup).</p> <p>Restore specific file:</p> <pre><code>snapraid fix -f /path/to/file\n</code></pre> <p>Restore specific directory:</p> <pre><code>snapraid fix -f /path/to/directory/\n</code></pre> <p>Restore only missing files: Use the <code>-m</code> flag to recover deleted files without overwriting existing changed files.</p> <pre><code>snapraid fix -m -f /path/to/directory/\n</code></pre>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/filesystem/snapraid/#recovering-a-failed-drive","level":3,"title":"Recovering a Failed Drive","text":"<p>In the event of a total disk failure, follow this procedure strictly.</p> <p>1. Halt Operations Disable any automated tasks (cron jobs, AIO scripts) and stop writing new data to the array.</p> <p>2. Reconfigure Install the replacement disk and mount it. Edit <code>/etc/snapraid.conf</code> to point the failed disk identifier to the new mount point.</p> <ul> <li>Example: Change <code>data d1 /mnt/failed_disk</code> to <code>data d1 /mnt/new_disk</code>.</li> </ul> <p>3. Fix (Restore Data) Run the fix command targeting the specific disk identifier (<code>-d</code>). Log the output to a file on a different drive for review.</p> <pre><code>snapraid -d d1 -l fix.log fix\n</code></pre> <ul> <li>Note: This process will read from all other disks to reconstruct the missing data. It runs at the speed of the slowest drive.</li> </ul> <p>4. Review Check <code>fix.log</code> for \"unrecoverable\" errors. If files were modified since the last sync, they may not be fully recoverable.</p> <p>5. Verification (Optional) Run a check on the restored disk to verify integrity.</p> <pre><code>snapraid -d d1 -a check\n</code></pre> <p>6. Resync Once verified, run a standard sync to update the array status.</p> <pre><code>snapraid sync\n</code></pre>","path":["Linux","Filesystem","SnapRAID Configuration Guide"],"tags":[]},{"location":"Linux/install/nassetup/","level":1,"title":"Server Setup: NAS","text":"<ul> <li>SnapRAID Manual</li> <li>MergerFS Documentation</li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#you-can-find-more-in-depth-info-in-the-related-guides-here","level":2,"title":"You can find more in depth info in the related guides here.","text":"<ul> <li> <p>This is just how I did it. There are plenty of other ways to do it using different configurations (ZFS, Proxmox, Unraid, TrueNAS, OMV, etc). All options have pros and cons. </p> </li> <li> <p>For my configuration I created an Ubuntu server with MergerFS + SnapRAID. </p> <p>MergerFS: Pool drives and read them as one drive. FUSE system. Various configurations, read the official docs. THe docs are very literal.</p> <p>SnapRAID: Create routine snapshots of drives to restore them in case of failure. No striping, easy to expand, straightforward to manage. </p> </li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#mount-points","level":2,"title":"Mount points","text":"<p>Branches Docs</p>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#naming-convention","level":3,"title":"Naming Convention","text":"<p>HDDs: <code>/mnt/hdd</code> SATA SSDs: <code>/mnt/ssd</code> NVME/M2: <code>/mnt/nvme</code> REMOTE: <code>/mnt/remote</code></p>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#example","level":5,"title":"Example","text":"<pre><code>$ ls -lh /mnt/\ntotal 16K\ndrwxr-xr-x 8 root root 4.0K Aug 18  2024 hdd\ndrwxr-xr-x 6 root root 4.0K Oct  8  2024 nvme\ndrwxr-xr-x 3 root root 4.0K Aug 24  2024 remote\ndrwxr-xr-x 3 root root 4.0K Jul 14  2024 ssd\n\n$ ls -lh /mnt/hdd/\ntotal 8K\nd--------- 2 root root 4.0K Apr 14 15:58 10T-01234567\nd--------- 2 root root 4.0K Apr 12 20:51 20T-12345678\n\n$ ls -lh /mnt/nvme/\ntotal 8K\nd--------- 2 root root 4.0K Apr 14 16:00 1T-ABCDEFGH\nd--------- 2 root root 4.0K Apr 14 23:24 1T-BCDEFGHI\n\n$ ls -lh /mnt/remote/\ntotal 8K\nd--------- 2 root root 4.0K Apr 12 20:23 foo-sshfs\nd--------- 2 root root 4.0K Apr 12 20:24 bar-nfs\n\n\n# You can find the serial number of a drive using lsblk\n$ lsblk -d -o NAME,PATH,SIZE,SERIAL\nNAME    PATH           SIZE SERIAL\nsda     /dev/sda       9.1T 01234567\nsdb     /dev/sdb      18.2T 12345678\nnvme0n1 /dev/nvme0n1 953.9G ABCDEFGH\nnvme1n1 /dev/nvme1n1 953.9G BCDEFGHI\n</code></pre>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#1-drive-preparation","level":2,"title":"1. Drive Preparation","text":"<p>Before pooling, drives must be partitioned and formatted.</p> <ul> <li>Note: Identify your disks carefully using <code>lsblk</code> or <code>sudo fdisk -l</code>.</li> <li>Strategy: Use EXT4 for simplicity and reliability on Ubuntu.</li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#formatting-ssd-as-ext4","level":3,"title":"Formatting SSD as ext4","text":"<pre><code># 1. Create a GPT partition table (Destructive!)\nsudo wipefs -a /dev/sda\n\n# 2. Create the partition (100% of disk)\nsudo parted -a opt /dev/sdb mkpart primary ext4 0% 100%\n\n# 3. Format to EXT4\n# -m 0: Reserve 0% for root (max space for data)\n# -L: Label the disk (disk1, disk2, parity1) to make it easy to identify\nsudo mkfs.ext4 -m 0 -L disk1 /dev/sdb1\n</code></pre>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#formatting-3x-hdds-as-xfs","level":3,"title":"Formatting 3x HDDs as XFS","text":"<pre><code># Drive 1 (sdb)\n# wipe fs\nsudo wipefs -a /dev/sdb\n# create partition\nsudo parted -s /dev/sdb mkpart primary xfs 0% 100%\n# list partition\nsudo fdisk -l /dev/sdb\n# format partition\nsudo mkfs.xfs -f -L \"disk1\" /dev/sdb1\n\n# Drive 2 (sdc)\nsudo wipefs -a /dev/sdc\nsudo parted -s /dev/sdc mkpart primary xfs 0% 100%\nsudo fdisk -l /dev/sdc\nsudo mkfs.xfs -f -L \"disk2\" /dev/sdc1\n\n# Drive 3 (sdd)\nsudo wipefs -a /dev/sdd\nsudo parted -s /dev/sdd mkpart primary xfs 0% 100%\nsudo fdisk -l /dev/sdd  \nsudo mkfs.xfs -f -L \"parity1\" /dev/sdd1\n</code></pre> <p>(Repeat for <code>disk2</code>, <code>parity1</code>, etc.)</p> <ul> <li> <p>Get UUIDs <pre><code>ls -l /dev/disk/by-uuid/\n</code></pre></p> </li> <li> <p>Output will be: <pre><code>lrwxrwxrwx 1 root root 10 Dec  7 05:34 02de315f-8ced-4745-bb4f-2d24c46efa48 -&gt; ../../sdb1\nlrwxrwxrwx 1 root root 15 Dec  7 04:33 3c26dce0-7f10-426c-bd67-e9f2d77889ee -&gt; ../../nvme0n1p2\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 603999da-c2dd-484d-80bf-ab4977680e90 -&gt; ../../sdc1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 844cae61-9599-4a57-9431-18c0a33904e4 -&gt; ../../sda1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 9860056e-b314-4cb9-acc5-30e76281795b -&gt; ../../sdd1\nlrwxrwxrwx 1 root root 10 Dec  7 04:33 d5ac7faa-876c-43ff-b27a-94c53bf79bb2 -&gt; ../../dm-0\n</code></pre></p> </li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#mounting","level":3,"title":"Mounting","text":"<ul> <li> <p>Create Mount points: <pre><code># Must make the folder to mount drives to\nsudo mkdir -p /mnt/disk1\nsudo mkdir -p /mnt/disk2\nsudo mkdir -p /mnt/parity1\nsudo mkdir -p /mnt/appdata\n\n# Mount Parition\nsudo mount /dev/sdb1 /mnt/parity1\nsudo mount /dev/sdc1 /mnt/disk1\nsudo mount /dev/sdd1 /mnt/disk2\n\n# Verify Mounting \ndf -h \n# Filesystem      Size  Used Avail Use% Mounted on\n# ...\n# /dev/sda1       7.3T   93M  6.9T   1% /mnt/mydrive\n</code></pre></p> </li> <li> <p>Edit fstab</p> </li> <li>Scroll to bottom of fstab and add:  <pre><code># ---------------------------------------------------------\n# DATA DRIVES (XFS) - 12TB WD Reds\n# ---------------------------------------------------------\n# Disk 1 (sdb1)\nUUID=02de315f-8ced-4745-bb4f-2d24c46efa48 /mnt/disk1 xfs defaults,noatime 0 0\n\n# Disk 2 (sdc1)\nUUID=603999da-c2dd-484d-80bf-ab4977680e90 /mnt/disk2 xfs defaults,noatime 0 0\n\n# Parity Drive (sdd1)\nUUID=9860056e-b314-4cb9-acc5-30e76281795b /mnt/parity1 xfs defaults,noatime 0 0\n\n# ---------------------------------------------------------\n# FAST STORAGE (EXT4) - 240GB SSD\n# ---------------------------------------------------------\n# Appdata/Cache (sda1)\nUUID=844cae61-9599-4a57-9431-18c0a33904e4 /mnt/appdata ext4 defaults,noatime 0 0\n</code></pre></li> <li> </li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#note-must-run-after-editing-fstab","level":5,"title":"NOTE:  MUST RUN AFTER EDITING fstab","text":"<pre><code># verify syntax \nsudo findmnt --verify\n\n# reload fstab\nsudo systemctl daemon-reload\n</code></pre>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#verify","level":3,"title":"Verify","text":"<ul> <li> <p>Test this BEFORE reboot - no output is good <pre><code>sudo mount -a\n</code></pre></p> </li> <li> <p>Check correct sizing: <pre><code>df -h\n</code></pre></p> </li> </ul> <p>You should see:</p> <ul> <li> <p><code>disk1</code>, <code>disk2</code>, <code>parity1</code>: ~11T</p> </li> <li> <p><code>appdata</code>: ~220G</p> </li> <li> <p>Check ownership: <pre><code>ls -ld /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> <li> <p>If owner listed as <code>root</code> (and doesnt need to be) run : <pre><code>sudo chown -R goose:goose /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#2-install-mergerfs","level":2,"title":"2. Install MergerFS","text":"<p>Read the official docs. There is a lot to go over with mergerFS. </p> <pre><code>sudo apt update &amp;&amp; sudo apt install mergerfs -y\n</code></pre> <p>Configure Pool in Fstab Add this line to the bottom of <code>/etc/fstab</code>:</p> <pre><code># Syntax: /mnt/disk* (all mounts starting with disk) -&gt; /mnt/storage\n/mnt/hdd/WD-B00WHLZD:/mnt/hdd/WD-B00WUHXD /mnt/storage fuse.mergerfs cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=100G,moveonenospc=true,fsname=mergerfs 0 0\n</code></pre> <ul> <li>minfreespace=50G: Prevents filling a drive completely, which creates fragmentation issues.</li> <li>cache.files=off: Disables page caching (reduces RAM usage/complexity).</li> <li>category.create=mspmfs: Most Space, Path Most Free Space. Writes new files to the drive with the most free space, unless the path already exists on another drive.</li> <li>func.getattr=newest: Returns file attributes from the file with the newest mtime. Critical for apps like Plex/Kodi to detect changes.</li> <li>minfreespace=200G: Prevents filling a drive completely; moves to the next drive when 200GB remains.</li> </ul> <p>C. Activate</p> <pre><code>sudo mkdir -p /mnt/storage\nsudo mount -a\n</code></pre>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#3-snapraid","level":2,"title":"3. SnapRAID","text":"<p>SnapRAID calculates parity (like RAID 5) but does it on a schedule (Snapshot), not in real-time. Ideally suited for media servers where files rarely change.</p> <p>A. Install SnapRAID Ubuntu repos often have outdated versions. It is safer to build from source or download the latest .deb.</p> <pre><code># Check version in repo or download from official site\nsudo apt install snapraid -y\nsnapraid --version\n</code></pre> <p>**B. Configure <code>/etc/snapraid.conf**</code> Backup the default and create your own: <code>sudo nano /etc/snapraid.conf</code></p> <pre><code># 1. Parity Location (The dedicated large drive)\nparity /mnt/parity1/snapraid.parity\n\n# 2. Content Files (Index of all your files)\n# Save multiple copies on different physical disks!\ncontent /var/snapraid.content\ncontent /mnt/disk1/.snapraid.content\ncontent /mnt/disk2/.snapraid.content\n\n# 3. Data Disks ( The drives you want to protect)\ndata d1 /mnt/disk1/\ndata d2 /mnt/disk2/\n\n# 4. Excludes (Temp files/Trash)\nexclude *.unrecoverable\nexclude /tmp/\nexclude /lost+found/\n</code></pre> <p>C. Initialize Run the first sync to calculate parity. This will take hours depending on data size.</p> <pre><code>sudo snapraid sync\n</code></pre>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#5-automation-maintenance","level":3,"title":"5. Automation &amp; Maintenance","text":"<p>SnapRAID is manual by default. You must automate it to stay protected.</p> <p>A. Basic Crontab Edit cron: <code>sudo crontab -e</code> Add a nightly sync and weekly scrub (checks for bit-rot).</p> <pre><code># Sync every night at 3 AM\n0 3 * * * snapraid sync &gt;&gt; /var/log/snapraid.log\n\n# Scrub (check data integrity) every Sunday at 5 AM\n0 5 * * 0 snapraid scrub -p 5 &gt;&gt; /var/log/snapraid_scrub.log\n</code></pre> <ul> <li>-p 5: Scrubs 5% of the array each week, ensuring a full check every ~5 months.</li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/nassetup/#troubleshooting","level":3,"title":"Troubleshooting","text":"<ul> <li>\"Disk is full\" during sync: Your parity drive must be equal to or larger than your single largest data drive.</li> <li>Data Recovery: If <code>disk1</code> fails, replace it, mount to <code>/mnt/disk1</code>, and run <code>snapraid fix -d d1</code>.</li> </ul>","path":["Linux","Install","Server Setup: NAS"],"tags":[]},{"location":"Linux/install/pcsetup/","level":1,"title":"Server Setup: App Server","text":"<ul> <li>Debian 12 Bookworm Release Notes </li> <li>Debian Network Setup </li> <li>Debian SSH Documentation </li> </ul>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#1-preparation-media-creation","level":2,"title":"1. Preparation &amp; Media Creation","text":"<p>A. Download ISO Get the Small Installation Image (Netinst).</p> <p>Architecture</p> <p>For Lenovo M920q/M70q nodes, ensure you download the amd64 ISO.</p> <p>B. Flash to USB Use BalenaEtcher, Rufus, or <code>dd</code> to write the ISO to a USB drive (4GB+).</p>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#2-bios-uefi-configuration","level":2,"title":"2. BIOS / UEFI Configuration","text":"<p>Mini PCs often ship with settings optimized for Windows. Adjust these for a Linux server.</p> <ol> <li>Enter BIOS: Power on and rapidly tap the setup key (<code>F1</code> for Lenovo).</li> <li>Secure Boot: Set to Disabled.</li> <li> <p>Reasoning: While Debian supports Secure Boot, disabling it prevents headaches with third-party drivers (Nvidia) or unsigned kernel modules later.</p> </li> <li> <p>Power Behavior: Set \"After Power Loss\" to Power On.</p> </li> <li>Boot Order: Prioritize the USB drive.</li> </ol>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#3-installation-process","level":2,"title":"3. Installation Process","text":"<p>Boot from USB. The Debian installer uses a classic text-based interface. Navigate with <code>Arrow Keys</code>, select with <code>Enter</code>, and toggle options with <code>Space</code>.</p> <p>A. Initial Settings</p> <ul> <li>Install: Select <code>Graphical Install</code> (easier) or <code>Install</code> (text-only).</li> <li>Language/Location/Keyboard: Select defaults.</li> <li>Hostname: <code>vanth-node-01</code> (or your preference).</li> <li>Domain Name: Leave blank (unless you have a local domain).</li> </ul> <p>B. User &amp; Password (Crucial)</p> <p>Sudo Configuration</p> <p>Leave the 'Root password' field BLANK.</p> <pre><code>If you leave the root password blank, the installer will automatically install `sudo` and add your new user to the sudo group. If you set a root password now, you will have to manually configure sudo later.\n</code></pre> <ul> <li>Full name / Username: Enter your details (e.g., <code>goose</code>).</li> <li>Password: Set a strong password for this user.</li> </ul> <p>C. Partitioning</p> <ul> <li>Method: \"Guided - use entire disk and set up LVM\".</li> <li>Partition Scheme: \"All files in one partition\" (easiest for beginners).</li> <li>Confirm: Select \"Finish partitioning and write changes to disk\" -&gt; Yes.</li> </ul> <p>D. Software Selection (Tasksel)</p> <p>The installer will install the base system and then ask for additional software.</p> <ul> <li>Scan extra media? No.</li> <li>Package Manager: Select a mirror close to you (e.g., <code>deb.debian.org</code>).</li> <li>Software selection:</li> <li> Debian desktop environment (Uncheck this)</li> <li> GNOME (Uncheck this)</li> <li> SSH server (Check this)</li> <li> Standard system utilities (Check this)</li> </ul>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#4-post-install-configuration","level":2,"title":"4. Post-Install Configuration","text":"<p>Remove the USB and reboot. Log in via the physical terminal one last time to get the IP.</p>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#a-network-configuration-static-ip","level":3,"title":"A. Network Configuration (Static IP)","text":"<p>Debian uses <code>/etc/network/interfaces</code> by default, not Netplan.</p> <ol> <li> <p>Check Interface Name: <pre><code>ip link\n# Look for eno1, eth0, or enp3s0\n</code></pre></p> </li> <li> <p>Edit Config: <pre><code>sudo nano /etc/network/interfaces\n</code></pre></p> </li> <li> <p>Modify: Replace <code>allow-hotplug eno1</code> and <code>iface eno1 inet dhcp</code> with: <pre><code># The primary network interface\nauto eno1\niface eno1 inet static\n    address 192.168.1.50/24\n    gateway 192.168.1.1\n    # DNS servers\n    dns-nameservers 1.1.1.1 8.8.8.8\n</code></pre></p> </li> <li> <p>Apply: <pre><code>sudo systemctl restart networking\n</code></pre></p> </li> </ol>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#b-connect-via-ssh","level":3,"title":"B. Connect via SSH","text":"<p>Switch to your main PC terminal:</p> <pre><code>ssh goose@192.168.1.50\n</code></pre>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#c-update-system","level":3,"title":"C. Update System","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#5-quality-of-life-security","level":2,"title":"5. Quality of Life &amp; Security","text":"","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#a-install-qemu-guest-agent-if-vm","level":3,"title":"A. Install QEMU Guest Agent (If VM)","text":"<p>If this is running as a VM inside Proxmox:</p> <pre><code>sudo apt install qemu-guest-agent -y\nsudo systemctl enable --now qemu-guest-agent\n</code></pre>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#b-prevent-sleep","level":3,"title":"B. Prevent Sleep","text":"<p>Prevent the Mini PC from suspending when idle:</p> <pre><code>sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target\n</code></pre>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#c-install-vital-tools","level":3,"title":"C. Install Vital Tools","text":"<p>Debian \"Standard Utilities\" is very barebones.</p> <pre><code>sudo apt install curl wget git htop vim tmux -y\n</code></pre>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"Linux/install/pcsetup/#troubleshooting","level":2,"title":"Troubleshooting","text":"<ul> <li>\"Username is not in the sudoers file\": You likely set a root password during install.</li> <li> <p>Fix: Switch to root (<code>su -</code>), then run <code>usermod -aG sudo your_username</code>. Reboot.</p> </li> <li> <p>SSH Connection Refused: Check if the service is running (<code>sudo systemctl status ssh</code>). If missing, <code>sudo apt install openssh-server</code>.</p> </li> <li>DNS failures: Check <code>/etc/resolv.conf</code>. It should contain <code>nameserver 1.1.1.1</code>. If not, your static IP config in <code>/etc/network/interfaces</code> might have a syntax error.</li> </ul>","path":["Linux","Install","Server Setup: App Server"],"tags":[]},{"location":"ansible/1ansible/","level":1,"title":"Ansible","text":"<p>Ansible  </p>","path":["Ansible"],"tags":[]},{"location":"ansible/1ansible/#overview","level":2,"title":"Overview","text":"<p>--</p> <p>Ansible automates the management of remote systems and controls their desired state.</p> <p>Ansible provides open-source automation that reduces complexity and runs everywhere. Using Ansible lets you automate virtually any task. Here are some common use cases for Ansible:</p> <ul> <li> <p>Eliminate repetition and simplify workflows</p> </li> <li> <p>Manage and maintain system configuration</p> </li> <li> <p>Continuously deploy complex software</p> </li> <li> <p>Perform zero-downtime rolling updates</p> </li> </ul> <p>Ansible uses simple, human-readable scripts called playbooks to automate your tasks. You declare the desired state of a local or remote system in your playbook. Ansible ensures that the system remains in that state.</p> <p></p> <p>As shown in the preceding figure, most Ansible environments have three main components:</p> <p>Control node</p> <ul> <li>A system on which Ansible is installed. You run Ansible commands such as <code>ansible</code> or <code>ansible-inventory</code> on a control node.</li> </ul> <p>Inventory</p> <ul> <li>A list of managed nodes that are logically organized. You create an inventory on the control node to describe host deployments to Ansible.</li> </ul> <p>Managed node</p> <ul> <li>A remote system, or host, that Ansible controls.</li> </ul>","path":["Ansible"],"tags":[]},{"location":"ansible/1ansible/#initial-setup","level":2,"title":"Initial Setup","text":"<p>This will cover the installation and structure required for using Ansible efficiently. </p> <p>Ansible be run from one computer (control node) to manage the configuration of multiple other servers (mananged nodes).</p> <p>This requires a few things to work.</p>","path":["Ansible"],"tags":[]},{"location":"ansible/1ansible/#ssh-keys","level":2,"title":"SSH Keys","text":"<p>Generate an SSH on the control node. Add a passphrase, this is your main, secure key. </p> <pre><code>ssh-keygen -t ed25519 -C \"controlhostname\"\n</code></pre> <p>Copy that SSH key to the mananged node(s).</p> <pre><code>ssh-copy-id -i ~/.ssh/id_ed25519.pub &lt;IP Address&gt;\n</code></pre> <p>Generate an SSH key that is specifically for Ansible. No passphrase. This is ansible specific key.</p> <pre><code>ssh-keygen -t ed25519 -C \"ansible\"\n</code></pre> <p>Copy that SSH key to the mananged node(s).</p> <pre><code>ssh-copy-id -i ~/.ssh/ansible.pub &lt;IP Address&gt;\n</code></pre> <p>Connecting using an SSH key. </p> <pre><code>ssh -i .ssh/&lt;key_name&gt; &lt;IP Address&gt;\n</code></pre> <p>Cache the passphrase</p> <pre><code>eval $(ssh-agent)\nssh-add\n</code></pre> <p>Bash alias for caching the passphrase</p> <pre><code>alias ssha='eval $(ssh-agent) &amp;&amp; ssh-add'\n</code></pre>","path":["Ansible"],"tags":[]},{"location":"ansible/1ansible/#git-repository","level":2,"title":"Git Repository","text":"<p>Check if git is installed</p> <pre><code>which git\n# This will return `/usr/bin/git` if installed.\n</code></pre> <p>Install git</p> <pre><code>sudo apt update\nsudo apt install git\n</code></pre> <p>Create user config for git</p> <pre><code>git config --global user.name \"First Last\"\ngit config --global user.email \"somebody@somewhere.net\"\n</code></pre> <p>Check the status of your git repository</p> <pre><code>git status\n</code></pre> <p>Stage the README.md file (after making changes) to be included in the next git commit</p> <pre><code>git add README.md\n</code></pre> <p>Set up the README.md file to be included in a commit</p> <pre><code>git commit -m \"Updated readme file, initial commit\"\n</code></pre> <p>Send the commit to Github</p> <pre><code>git push origin master\n</code></pre>","path":["Ansible"],"tags":[]},{"location":"ansible/2ansibleinstallation/","level":1,"title":"Ansible Installation","text":"<p>Official Installation instructions can be found here. For OS specific installation, go here</p> <p>This will show installation on Ubuntu.</p>","path":["Ansible","Ansible Installation"],"tags":[]},{"location":"ansible/2ansibleinstallation/#installing-ansible-on-ubuntu","level":2,"title":"Installing Ansible on Ubuntu","text":"<p>Ubuntu provides Ansible packages through a Personal Package Archive (PPA) that contains more recent versions than the standard repositories.</p> <p>Ubuntu builds are available in a PPA here.</p> <p>Configure the PPA on your system and install Ansible:</p> <pre><code>$ sudo apt update\n$ sudo apt install software-properties-common\n$ sudo add-apt-repository --yes --update ppa:ansible/ansible\n$ sudo apt install ansible\n</code></pre> <p>Note</p> <p>On older Ubuntu distributions, “software-properties-common” is called “python-software-properties”. You may want to use <code>apt-get</code> rather than <code>apt</code> in older versions. Also, only newer distributions (18.04, 18.10, and later) have a <code>-u</code> or <code>--update</code> flag. Adjust your script as needed.</p>","path":["Ansible","Ansible Installation"],"tags":[]},{"location":"ansible/3ansibleinventory/","level":1,"title":"Building Inventories &amp; Running Ad Hoc Commands","text":"<p>Official Docs  </p> <p>An inventory is a list of managed nodes, or hosts, that Ansible deploys and configures.</p> <p>Create an <code>inventory</code> file</p> <pre><code>nano inventory\n</code></pre> <p>Within the file add the IPs to be configured</p> <pre><code>10.10.30.10\n10.10.30.20\n10.10.30.30\n</code></pre> <p>Add the inventory file to version control</p> <pre><code>git add inventory\n</code></pre> <p>Commit the changes</p> <pre><code>git commit -m \"first version of the inventory file, added three hosts.\"\n</code></pre> <p>Push commit to Github</p> <pre><code>git push origin master\n</code></pre> <p>Test Ansible is working</p> <pre><code>ansible all --key-file ~/.ssh/ansible -i inventory -m ping\n# -i INVETORY \n# -m MODULE_NAME (action to execute, in this case the ping command) - note that `ping` here is an actual ssh connection. Not a normal ping command.\n</code></pre> <ul> <li>This command will send a <code>ping</code> to all the hosts inside the <code>inventory</code> file</li> <li>If successful, this should return something like: </li> </ul> <pre><code>  10.10.30.20 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>Create ansible config file</p> <p>Inside your repository directory:</p> <pre><code>nano ansible.cfg\n\n[defaults]\ninventory = inventory \nprivate_key_file = ~/.ssh/ansible\n</code></pre> <p>Now the ansible command can be simplified</p> <pre><code>ansible all -m ping\n</code></pre> <p>List all of the hosts in the inventory</p> <pre><code>ansible all --list-hosts\n</code></pre> <p>Gather facts about your hosts</p> <pre><code>ansible all -m gather_facts\n</code></pre> <p>This will be very long. You can search for something specific by filtering with <code>grep</code></p> <p>Gather facts about your hosts, but limit it to just one host</p> <pre><code>ansible all -m gather_facts --limit 172.16.250.132\n</code></pre>","path":["Ansible","Building Inventories &amp; Running Ad Hoc Commands"],"tags":[]},{"location":"ansible/4ansiblecommands/","level":1,"title":"Elevated Commands","text":"<p>Official Docs  </p> <p>https://www.youtube.com/watch?v=FPU9_KDTa8A&amp;list=PLT98CRl2KxKEUHie1m24-wkyHpEsa4Y70&amp;index=5</p> <p>Unlike the commands listed in the inventory and ad hoc guide, elevated commands will make system changes on the managed nodes.</p> <p>Tell ansible to use sudo (become)</p> <p>This is needed in order to actually become a <code>sudo</code> user on the managed node. </p> <p>The <code>--become --ask-become-pass</code> is what makes the preceding command possible to run. </p> <pre><code>ansible all -m apt -a update_cache=true --become --ask-become-pass\n</code></pre> <p>When prompted for password, input the password for the <code>sudo</code> account user. </p> <p>Note</p> <p>By default, <code>become</code> will be using <code>sudo</code>. There are different approaches when using become, but that is not covered here. </p> <p>Install a package via the apt module</p> <pre><code>ansible all -m apt -a name=vim-nox --become --ask-become-pass\n</code></pre> <p>Install a package via the apt module, and also make sure it’s the latest version available</p> <pre><code>ansible all -m apt -a \"name=nano state=latest\" --become --ask-become-pass\n</code></pre> <p>Upgrade all the package updates that are available</p> <pre><code>ansible all -m apt -a upgrade=dist --become --ask-become-passxx\n</code></pre>","path":["Ansible","Elevated Commands"],"tags":[]},{"location":"ansible/5ansibleplaybook/","level":1,"title":"Playbooks","text":"<p>Official Docs  </p> <p>Ansible Playbooks provide a repeatable, reusable, simple configuration management and multimachine deployment system that is well suited to deploying complex applications. If you need to execute a task with Ansible more than once, you can write a playbook and put the playbook under source control. You can then use the playbook to push new configurations or confirm the configuration of remote systems.</p> <p>Playbooks allow you to perform the following actions:</p> <ul> <li> <p>Declare configurations.</p> </li> <li> <p>Orchestrate steps of any manual ordered process on multiple sets of machines in a defined order.</p> </li> <li> <p>Launch tasks synchronously or asynchronously.</p> </li> </ul>","path":["Ansible","Playbooks"],"tags":[]},{"location":"ansible/5ansibleplaybook/#creating-a-playbook","level":2,"title":"Creating a playbook","text":"<p>Examples below. Create this in the same directory as your <code>inventory</code> file. </p> <p>install_apache.yml</p> <pre><code> --- # be sure to add these 3 hyphens\n\n - hosts: all # Note the - (hyphen) at beginning of block. \n   become: true # run as sudo\n   tasks: # begin list of tasks\n\n   - name: install apache2 package # keep descriptive, what task does\n     apt: # module to run\n       name: apache2 # what to install \n</code></pre> <p>Run the playbook</p> <pre><code>ansible-playbook --ask-become-pass install_apache.yml\n</code></pre> <p>install_apache.yml (second version)</p> <p><pre><code> ---\n\n - hosts: all\n   become: true\n   tasks:\n\n   - name: update repository index\n     apt:\n       update_cache: yes\n\n   - name: install apache2 package\n     apt:\n       name: apache2\n</code></pre> install_apache.yml (third version)</p> <p><pre><code> ---\n\n - hosts: all\n   become: true\n   tasks:\n\n   - name: update repository index\n     apt:\n       update_cache: yes\n\n   - name: install apache2 package\n     apt:\n     name: apache2\n\n   - name: add php support for apache\n     apt:\n       name: libapache2-mod-php\n</code></pre> install_apache.yml (fourth version)</p> <p><pre><code> ---\n\n - hosts: all\n   become: true\n   tasks:\n\n   - name: update repository index\n     apt:\n       update_cache: yes\n\n   - name: install apache2 package\n     apt:\n       name: apache2\n       state: latest\n\n   - name: add php support for apache\n     apt:\n       name: libapache2-mod-php\n       state: latest\n</code></pre> remove_apache.yml</p> <pre><code> ---\n\n - hosts: all\n   become: true\n   tasks:\n\n   - name: remove apache2 package\n     apt:\n       name: apache2\n       state: absent\n\n   - name: remove php support for apache\n     apt:\n       name: libapache2-mod-php\n       state: absent\n</code></pre>","path":["Ansible","Playbooks"],"tags":[]},{"location":"docker/docker/","level":1,"title":"Install Docker Engine on Debian","text":"<p>Docker Docs  </p>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#uninstall-old-versions","level":2,"title":"Uninstall old versions","text":"<p>Before you can install Docker Engine, you need to uninstall any conflicting packages.</p> <p>Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.</p> <pre><code>$ sudo apt remove $(dpkg --get-selections docker.io docker-compose docker-doc podman-docker containerd runc | cut -f1)\n</code></pre>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#installation-methods","level":2,"title":"Installation Methods","text":"","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#install-using-the-apt-repository","level":3,"title":"Install using the apt repository","text":"<ol> <li>Set up the <code>apt</code> repository.</li> </ol> <p><pre><code># Add Docker's official GPG key:\nsudo apt update\nsudo apt install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\nsudo tee /etc/apt/sources.list.d/docker.sources &lt;&lt;EOF\nTypes: deb\nURIs: https://download.docker.com/linux/debian\nSuites: $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\nComponents: stable\nSigned-By: /etc/apt/keyrings/docker.asc\nEOF\n\nsudo apt update\n</code></pre> 2. Install the Docker packages.</p> <p>To install latest version:</p> <pre><code>sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Note</p> <p>The Docker service starts automatically after installation. To verify that Docker is running, use:</p> <p><code>sudo systemctl status docker</code></p> <p>Some systems may have this behavior disabled and will require a manual start:</p> <p><code>sudo systemctl start docker</code></p> <ol> <li>Verify that the installation is successful by running the hello-world image:</li> </ol> <p><pre><code>sudo docker run hello-world\n</code></pre> This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.</p>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#install-using-the-convenience-script","level":3,"title":"Install using the convenience script","text":"<p>Docker provides a convenience script at https://get.docker.com/ to install Docker into development environments non-interactively. The convenience script isn't recommended for production environments, but it's useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. The source code for the script is open source, and you can find it in the <code>docker-install</code> repository on GitHub.</p> <p>Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:</p> <ul> <li>The script requires <code>root</code> or <code>sudo</code> privileges to run.</li> <li>The script attempts to detect your Linux distribution and version and configure your package management system for you.</li> <li>The script doesn't allow you to customize most installation parameters.</li> <li>The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.</li> <li>By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.</li> <li>The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.</li> </ul> <p>Tip</p> <p>Preview script steps before running. You can run the script with the <code>--dry-run</code> option to learn what steps the script will run when invoked:</p> <pre><code>$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh ./get-docker.sh --dry-run\n</code></pre> <p>This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:</p> <pre><code>$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\nExecuting docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737\n&lt;...&gt;\n</code></pre> <p>You have now successfully installed and started Docker Engine. The <code>docker</code> service starts automatically on Debian based distributions. On <code>RPM</code> based distributions, such as CentOS, Fedora or RHEL, you need to start it manually using the appropriate <code>systemctl</code> or <code>service</code> command. As the message indicates, non-root users can't run Docker commands by default.</p> <p>Use Docker as a non-privileged user, or install in rootless mode?</p> <p>The installation script requires <code>root</code> or <code>sudo</code> privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker without <code>root</code> privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).</p>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#install-pre-releases","level":4,"title":"Install pre-releases","text":"<p>Docker also provides a convenience script at https://test.docker.com/ to install pre-releases of Docker on Linux. This script is equal to the script at <code>get.docker.com</code>, but configures your package manager to use the test channel of the Docker package repository. The test channel includes both stable and pre-releases (beta versions, release-candidates) of Docker. Use this script to get early access to new releases, and to evaluate them in a testing environment before they're released as stable.</p> <p>To install the latest version of Docker on Linux from the test channel, run:</p> <pre><code>$ curl -fsSL https://test.docker.com -o test-docker.sh\n$ sudo sh test-docker.sh\n</code></pre>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#upgrade-docker-after-using-the-convenience-script","level":4,"title":"Upgrade Docker after using the convenience script","text":"<p>If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.</p>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/docker/#uninstall-docker-engine","level":2,"title":"Uninstall Docker Engine","text":"<ol> <li> <p>Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:</p> <pre><code>$ sudo apt purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n</code></pre> </li> <li> <p>Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:</p> <pre><code>$ sudo rm -rf /var/lib/docker\n$ sudo rm -rf /var/lib/containerd\n</code></pre> </li> <li> <p>Remove source list and keyrings</p> <pre><code>$ sudo rm /etc/apt/sources.list.d/docker.sources\n$ sudo rm /etc/apt/keyrings/docker.asc\n</code></pre> </li> </ol> <p>You have to delete any edited configuration files manually.</p>","path":["Docker","Install Docker Engine on Debian"],"tags":[]},{"location":"docker/dockerpostinstall/","level":1,"title":"Linux post-installation steps for Docker Engine","text":"<p>These optional post-installation procedures describe how to configure your Linux host machine to work better with Docker.</p>","path":["Docker","Linux post-installation steps for Docker Engine"],"tags":[]},{"location":"docker/dockerpostinstall/#manage-docker-as-a-non-root-user","level":2,"title":"Manage Docker as a non-root user","text":"<p>The Docker daemon binds to a Unix socket, not a TCP port. By default it's the <code>root</code> user that owns the Unix socket, and other users can only access it using <code>sudo</code>. The Docker daemon always runs as the <code>root</code> user.</p> <p>If you don't want to preface the <code>docker</code> command with <code>sudo</code>, create a Unix group called <code>docker</code> and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the <code>docker</code> group. On some Linux distributions, the system automatically creates this group when installing Docker Engine using a package manager. In that case, there is no need for you to manually create the group.</p> <p>Warning</p> <p>The <code>docker</code> group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.</p> <p>Note</p> <p>To run Docker without root privileges, see Run the Docker daemon as a non-root user (Rootless mode).</p> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li>Create the <code>docker</code> group.</li> </ol> <pre><code> sudo groupadd docker\n</code></pre> <ol> <li>Add your user to the <code>docker</code> group.</li> </ol> <pre><code> sudo usermod -aG docker $USER\n</code></pre> <ol> <li>Log out and log back in so that your group membership is re-evaluated.</li> </ol> <p>If you're running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.</p> <p>You can also run the following command to activate the changes to groups:</p> <pre><code> newgrp docker\n</code></pre> <ol> <li>Verify that you can run <code>docker</code> commands without <code>sudo</code>.</li> </ol> <pre><code> docker run hello-world\n</code></pre> <p>This command downloads a test image and runs it in a container. When the    container runs, it prints a message and exits.</p> <p>If you initially ran Docker CLI commands using <code>sudo</code> before adding your user    to the <code>docker</code> group, you may see the following error:</p> <pre><code>WARNING: Error loading config file: /home/user/.docker/config.json -\nstat /home/user/.docker/config.json: permission denied\n</code></pre> <p>This error indicates that the permission settings for the <code>~/.docker/</code>    directory are incorrect, due to having used the <code>sudo</code> command earlier.</p> <p>To fix this problem, either remove the <code>~/.docker/</code> directory (it's recreated    automatically, but any custom settings are lost), or change its ownership and    permissions using the following commands:</p> <pre><code> sudo chown \"$USER\":\"$USER\" /home/\"$USER\"/.docker -R\n sudo chmod g+rwx \"$HOME/.docker\" -R\n</code></pre>","path":["Docker","Linux post-installation steps for Docker Engine"],"tags":[]},{"location":"docker/dockerpostinstall/#configure-docker-to-start-on-boot-with-systemd","level":2,"title":"Configure Docker to start on boot with systemd","text":"<p>Many modern Linux distributions use systemd to manage which services start when the system boots. On Debian and Ubuntu, the Docker service starts on boot by default. To automatically start Docker and containerd on boot for other Linux distributions using systemd, run the following commands:</p> <pre><code> sudo systemctl enable docker.service\n sudo systemctl enable containerd.service\n</code></pre> <p>To stop this behavior, use <code>disable</code> instead.</p> <pre><code> sudo systemctl disable docker.service\n sudo systemctl disable containerd.service\n</code></pre> <p>You can use systemd unit files to configure the Docker service on startup, for example to add an HTTP proxy, set a different directory or partition for the Docker runtime files, or other customizations. For an example, see Configure the daemon to use a proxy.</p>","path":["Docker","Linux post-installation steps for Docker Engine"],"tags":[]},{"location":"docker/dockerpostinstall/#configure-default-logging-driver","level":2,"title":"Configure default logging driver","text":"<p>Docker provides logging drivers for collecting and viewing log data from all containers running on a host. The default logging driver, <code>json-file</code>, writes log data to JSON-formatted files on the host filesystem. Over time, these log files expand in size, leading to potential exhaustion of disk resources.</p> <p>To avoid issues with overusing disk for log data, consider one of the following options:</p> <ul> <li>Configure the <code>json-file</code> logging driver to turn on   log rotation.</li> <li>Use an   alternative logging driver   such as the \"local\" logging driver   that performs log rotation by default.</li> <li>Use a logging driver that sends logs to a remote logging aggregator.</li> </ul> <p>To use the json-file driver as the default logging driver, set the log-driver and log-opts keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config on Windows Server. If the file does not exist, create it first. For more information about configuring Docker using daemon.json, see daemon.json.</p> <p>The following example sets the log driver to json-file and sets the max-size and max-file options to enable automatic log-rotation.</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n</code></pre> <p>Note</p> <p>log-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for max-file in the example above) must therefore be enclosed in quotes (\").</p> <p>Restart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.</p> <p>You can set the logging driver for a specific container by using the --log-driver flag to docker container create or docker run:</p> <pre><code> docker run \\\n      --log-driver json-file --log-opt max-size=10m \\\n      alpine echo hello world\n</code></pre>","path":["Docker","Linux post-installation steps for Docker Engine"],"tags":[]},{"location":"docker/dockerpostinstall/#next-steps","level":2,"title":"Next steps","text":"<ul> <li>Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.</li> </ul>","path":["Docker","Linux post-installation steps for Docker Engine"],"tags":[]},{"location":"docker/z-cheatsheet/","level":1,"title":"Docker Cheat Sheet &amp; Workflows","text":"<p>tags:</p> <ul> <li>docker</li> <li>service</li> <li>cheat_sheet</li> <li>infrastructure</li> </ul>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#1-command-reference","level":2,"title":"1. Command Reference","text":"","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#11-project-management-docker-compose","level":3,"title":"1.1. Project Management (Docker Compose)","text":"<p>Primary method for managing stacks.</p> Command Description <code>docker compose up -d</code> Create/Start services in detached mode. <code>docker compose up -d --force-recreate</code> Force container recreation (useful if config changed but image didn't). <code>docker compose restart</code> Stop then restart containers. Does not rebuild/pull updated images. <code>docker compose down</code> Stop and remove containers/networks. <code>docker compose down -v</code> DANGER: Removes containers AND standard volumes. <code>docker compose down --volumes --rmi all --remove-orphans</code> DANGER: Removes all volumes, images built by container, and orphaned compose files. <code>docker compose stop</code> Stops services without removing containers. <code>docker compose pull</code> Downloads latest images defined in YAML. <code>docker compose logs -f</code> Stream logs for all services in the stack. <code>docker compose config</code> Validate YAML syntax and print the resolved configuration. <code>docker compose exec &lt;svc&gt; &lt;cmd&gt;</code> Run command inside service (e.g., <code>docker compose exec pihole bash</code>).","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#12-container-resource-monitoring","level":3,"title":"1.2. Container &amp; Resource Monitoring","text":"<p>Debugging individual containers or checking resource load.</p> Command Description <code>docker ps</code> List running containers. <code>docker ps -a</code> List all containers (including stopped/exited). <code>docker stats</code> Live stream of CPU, RAM, and Net I/O usage per container. <code>docker logs -f &lt;container&gt;</code> Follow log output for a specific container (e.g., <code>omada-controller</code>). <code>docker inspect &lt;container&gt;</code> JSON dump of container config (IP address, mounts, env vars). <code>docker top &lt;container&gt;</code> Display the running processes of a container.","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#13-storage-volumes","level":3,"title":"1.3. Storage &amp; Volumes","text":"<p>Managing persistence. Critical for the \"External Volume\" workflow.</p> Command Description <code>docker volume ls</code> List all volumes. <code>docker volume create &lt;name&gt;</code> Manually create a volume (required for <code>external: true</code>). <code>docker volume inspect &lt;name&gt;</code> Show volume mount point (usually <code>/var/lib/docker/volumes/...</code>). <code>docker volume rm &lt;name&gt;</code> Delete a volume. Data loss permanent. <code>docker volume prune</code> Remove all unused local volumes. <code>docker run --rm -v &lt;vol&gt;:/data busybox ls /data</code> Quickly list files inside a volume without attaching to the main app.","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#14-networking","level":3,"title":"1.4. Networking","text":"<p>Essential for debugging communication issues.</p> Command Description <code>docker network ls</code> List all created networks. <code>docker network inspect &lt;net_name&gt;</code> Show containers and IP addresses assigned to a specific network. <code>docker network create &lt;name&gt;</code> Create a manual bridge network. <code>docker network prune</code> Remove all unused networks.","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#15-system-maintenance-housekeeping","level":3,"title":"1.5. System Maintenance (Housekeeping)","text":"<p>Periodic cleanup.</p> Command Description <code>docker image prune -a</code> Remove all unused images (frees disk space). <code>docker system df</code> Show docker disk usage summary. <code>docker system prune</code> Cleanup: Removes stopped containers, unused networks, and dangling images. <code>docker system prune -a --volumes</code> Nuclear: Wipes everything not currently running.","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#2-workflows","level":2,"title":"2. Workflows","text":"","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#21-deploying-new-stack-external-volume-method","level":3,"title":"2.1. Deploying New Stack (External Volume Method)","text":"<p>Standard procedure for stateful services (Omada, Databases).</p> <ol> <li> <p>Pre-create Volumes: <pre><code>docker volume create service_data\ndocker volume create service_config\n</code></pre></p> </li> <li> <p>Deploy: <pre><code># Navigate to project folder\ndocker compose up -d\n</code></pre></p> </li> <li> <p>Verify Persistence: <pre><code>docker volume inspect service_data\n</code></pre></p> </li> </ol>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#22-updating-a-stack-zero-downtime-attempt","level":3,"title":"2.2. Updating a Stack (Zero-Downtime Attempt)","text":"<p>Routine update cycle.</p> <pre><code>cd /opt/docker/project_name\n\n# 1. Pull updates\ndocker compose pull\n\n# 2. Recreate only changed containers\ndocker compose up -d\n\n# 3. Clean up old images to save space\ndocker image prune -f\n</code></pre>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#23-hot-backup-of-a-volume","level":3,"title":"2.3. \"Hot\" Backup of a Volume","text":"<p>Backup data without stopping the container (using <code>tar</code>).</p> <pre><code># Syntax: docker run --rm -v &lt;volume_name&gt;:/data -v &lt;host_backup_dir&gt;:/backup alpine tar czf /backup/&lt;filename&gt;.tar.gz -C /data .\n\ndocker run --rm \\\n  -v omada_data:/data \\\n  -v $(pwd):/backup \\\n  alpine tar czf /backup/omada_data_backup_$(date +%F).tar.gz -C /data .\n</code></pre>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#24-debugging-connectivity","level":3,"title":"2.4. Debugging Connectivity","text":"<p>If services (e.g., Pi-hole, Unbound) are not responding.</p> <pre><code># 1. Check if container is healthy\ndocker ps \n\n# 2. Check internal IP assignment\ndocker network inspect bridge_network_name\n\n# 3. Test resolution from INSIDE the container\ndocker exec -it pihole nslookup google.com 127.0.0.1\n</code></pre>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#3-environment-variables-env","level":2,"title":"3. Environment Variables (<code>.env</code>)","text":"<p>Separates configuration (passwords, versions, ports) from the code (<code>compose.yaml</code>).</p>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#31-directory-location","level":3,"title":"3.1. Directory Location","text":"<p>The <code>.env</code> file must be located in the same directory as the <code>compose.yaml</code> file for Docker Compose to automatically read it.</p> <p>File Structure:</p> <pre><code>~/homelab/docker/\n├── .gitignore             # Global git ignore\n├── core/                  # Project Stack (e.g., Homepage, Glances)\n│   ├── .env               # &lt;--- GOES HERE (Specific to Core)\n│   └── compose.yaml\n├── omada/                 # Project Stack (Network)\n│   ├── .env               # &lt;--- GOES HERE (Specific to Omada)\n│   └── compose.yaml\n└── couchdb/               # Project Stack (Obsidian Sync)\n    ├── .env               # &lt;--- GOES HERE (Specific to CouchDB)\n    └── compose.yaml\n</code></pre>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#32-syntax-rules","level":3,"title":"3.2. Syntax Rules","text":"<ul> <li>Format: <code>KEY=VALUE</code></li> <li>No Spaces: <code>PASSWORD = 123</code> is invalid. Use <code>PASSWORD=123</code>.</li> <li>Comments: Use <code>#</code> for comments.</li> <li>Special Characters: If a password contains <code>#</code>, <code>$</code>, or spaces, wrap the value in quotes: <code>DB_PASS=\"Secure#Pass$123\"</code>.</li> </ul>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#33-workflow-example-couchdb","level":3,"title":"3.3. Workflow Example (CouchDB)","text":"<ol> <li> <p>Create the file: <pre><code>nano ~/homelab/docker/couchdb/.env\n</code></pre></p> </li> <li> <p>Define Variables: <pre><code># User Configuration\nCOUCH_USER=admin\nCOUCH_PASS=MySecretPassword123!\n\n# Network Configuration\nHOST_PORT=5984\n</code></pre></p> </li> <li> <p>Implement in <code>compose.yaml</code>: Use the <code>${VARIABLE_NAME}</code> syntax. <pre><code>services:\n  couchdb:\n    image: couchdb:latest\n    environment:\n      - COUCHDB_USER=${COUCH_USER}      # Pulls 'admin'\n      - COUCHDB_PASSWORD=${COUCH_PASS}  # Pulls 'MySecretPassword123!'\n    ports:\n      - ${HOST_PORT}:5984               # Maps port 5984\n</code></pre></p> </li> </ol>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#34-verification","level":3,"title":"3.4. Verification","text":"<p>To verify that Docker is reading the variables correctly without actually starting the container:</p> <pre><code>docker compose config\n</code></pre> <p>This prints the \"resolved\" YAML to the terminal, showing the actual values instead of the <code>${VAR}</code> placeholders.</p>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docker/z-cheatsheet/#35-security-git","level":3,"title":"3.5. Security (Git)","text":"<p>NEVER commit <code>.env</code> files to GitHub. They contain secrets.</p> <p>Ensure the global <code>.gitignore</code> includes:</p> <pre><code>**/.env\n**/.env.*\n</code></pre>","path":["Docker","Docker Cheat Sheet &amp; Workflows"],"tags":[]},{"location":"docs/","level":1,"title":"Get started","text":"<p>For full documentation visit zensical.org.</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#commands","level":2,"title":"Commands","text":"<ul> <li><code>zensical new</code> - Create a new project</li> <li><code>zensical serve</code> - Start local web server</li> <li><code>zensical build</code> - Build your site</li> </ul>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#examples","level":2,"title":"Examples","text":"","path":["Docs","Get started"],"tags":[]},{"location":"docs/#admonitions","level":3,"title":"Admonitions","text":"<p>Go to documentation</p> <p>Note</p> <p>This is a note admonition. Use it to provide helpful information.</p> <p>Warning</p> <p>This is a warning admonition. Be careful!</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#details","level":3,"title":"Details","text":"<p>Go to documentation</p> Click to expand for more info <p>This content is hidden until you click to expand it. Great for FAQs or long explanations.</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#code-blocks","level":2,"title":"Code Blocks","text":"<p>Go to documentation</p> Code blocks<pre><code>def greet(name):\n    print(f\"Hello, {name}!\") # (1)!\n\ngreet(\"Python\")\n</code></pre> <ol> <li> <p>Go to documentation</p> <p>Code annotations allow to attach notes to lines of code.</p> </li> </ol> <p>Code can also be highlighted inline: <code>print(\"Hello, Python!\")</code>.</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#content-tabs","level":2,"title":"Content tabs","text":"<p>Go to documentation</p> PythonRust <pre><code>print(\"Hello from Python!\")\n</code></pre> <pre><code>println!(\"Hello from Rust!\");\n</code></pre>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#diagrams","level":2,"title":"Diagrams","text":"<p>Go to documentation</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#footnotes","level":2,"title":"Footnotes","text":"<p>Go to documentation</p> <p>Here's a sentence with a footnote.<sup>1</sup></p> <p>Hover it, to see a tooltip.</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#formatting","level":2,"title":"Formatting","text":"<p>Go to documentation</p> <ul> <li>This was marked (highlight)</li> <li>This was inserted (underline)</li> <li>This was deleted (strikethrough)</li> <li>H<sub>2</sub>O</li> <li>A<sup>T</sup>A</li> <li>Ctrl+Alt+Del</li> </ul>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#icons-emojis","level":2,"title":"Icons, Emojis","text":"<p>Go to documentation</p> <ul> <li> <code>:sparkles:</code></li> <li> <code>:rocket:</code></li> <li> <code>:tada:</code></li> <li> <code>:memo:</code></li> <li> <code>:eyes:</code></li> </ul>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#maths","level":2,"title":"Maths","text":"<p>Go to documentation</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\] <p>Needs configuration</p> <p>Note that MathJax is included via a <code>script</code> tag on this page and is not configured in the generated default configuration to avoid including it in a pages that do not need it. See the documentation for details on how to configure it on all your pages if they are more Maths-heavy than these simple starter pages.</p>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#task-lists","level":2,"title":"Task Lists","text":"<p>Go to documentation</p> <ul> <li> Install Zensical</li> <li> Configure <code>zensical.toml</code></li> <li> Write amazing documentation</li> <li> Deploy anywhere</li> </ul>","path":["Docs","Get started"],"tags":[]},{"location":"docs/#tooltips","level":2,"title":"Tooltips","text":"<p>Go to documentation</p> <p>Hover me</p> <ol> <li> <p>This is the footnote. ↩</p> </li> </ol>","path":["Docs","Get started"],"tags":[]},{"location":"docs/markdown/","level":1,"title":"Markdown in 5min","text":"","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#headers","level":2,"title":"Headers","text":"<pre><code># H1 Header\n## H2 Header\n### H3 Header\n#### H4 Header\n##### H5 Header\n###### H6 Header\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#text-formatting","level":2,"title":"Text formatting","text":"<pre><code>**bold text**\n*italic text*\n***bold and italic***\n~~strikethrough~~\n`inline code`\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#links-and-images","level":2,"title":"Links and images","text":"<pre><code>[Link text](https://example.com)\n[Link with title](https://example.com \"Hover title\")\n![Alt text](image.jpg)\n![Image with title](image.jpg \"Image title\")\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#lists","level":2,"title":"Lists","text":"<pre><code>Unordered:\n- Item 1\n- Item 2\n  - Nested item\n\nOrdered:\n1. First item\n2. Second item\n3. Third item\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#blockquotes","level":2,"title":"Blockquotes","text":"<pre><code>&gt; This is a blockquote\n&gt; Multiple lines\n&gt;&gt; Nested quote\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#code-blocks","level":2,"title":"Code blocks","text":"<pre><code>```javascript\nfunction hello() {\n  console.log(\"Hello, world!\");\n}\n```\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#tables","level":2,"title":"Tables","text":"<pre><code>| Header 1 | Header 2 | Header 3 |\n|----------|----------|----------|\n| Row 1    | Data     | Data     |\n| Row 2    | Data     | Data     |\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#horizontal-rule","level":2,"title":"Horizontal rule","text":"<pre><code>---\nor\n***\nor\n___\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#task-lists","level":2,"title":"Task lists","text":"<pre><code>- [x] Completed task\n- [ ] Incomplete task\n- [ ] Another task\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#escaping-characters","level":2,"title":"Escaping characters","text":"<pre><code>Use backslash to escape: \\* \\_ \\# \\`\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/markdown/#line-breaks","level":2,"title":"Line breaks","text":"<pre><code>End a line with two spaces  \nto create a line break.\n\nOr use a blank line for a new paragraph.\n</code></pre>","path":["Docs","Markdown in 5min"],"tags":[]},{"location":"docs/zensical-cloudflare/","level":1,"title":"Hosting Zensical on Cloudflare Pages","text":"","path":["Docs","Hosting Zensical on Cloudflare Pages"],"tags":[]},{"location":"docs/zensical-cloudflare/#1-init-your-fresh-zensical-project-locally","level":2,"title":"1. Init your fresh Zensical project locally","text":"<pre><code>pip install zensical\nzensical new my-docs\ncd my-docs\ngit init\ngit remote add origin https://github.com/yourusername/my-docs.git\ngit push -u origin main\n</code></pre>","path":["Docs","Hosting Zensical on Cloudflare Pages"],"tags":[]},{"location":"docs/zensical-cloudflare/#2-connect-to-cloudflare-pages-one-time","level":3,"title":"2. Connect to Cloudflare Pages (one-time)","text":"<ol> <li>Cloudflare Dashboard → Workers &amp; Pages → Create application → Pages → Connect to Git</li> <li>Authorize GitHub, select your repo</li> <li>Set build settings:</li> <li>Framework preset: None</li> <li>Build command: <code>pip install zensical &amp;&amp; zensical build --clean</code></li> <li>Build output directory: <code>site</code></li> <li>Click Save and Deploy</li> </ol>","path":["Docs","Hosting Zensical on Cloudflare Pages"],"tags":[]},{"location":"docs/zensical-cloudflare/#3-add-your-custom-domain","level":3,"title":"3. Add your custom domain","text":"<p>Once the first deploy succeeds, go to your Pages project → Custom domains → Set up a custom domain → enter <code>docs.mydomain.me</code>. Since your domain is already on Cloudflare, it adds the DNS record for you automatically.</p>","path":["Docs","Hosting Zensical on Cloudflare Pages"],"tags":[]},{"location":"media%20server/gluetun/","level":1,"title":"Gluetun - ProtonVPN + Qbittorrent","text":"<p>Gluetun  </p> <p>Gluetun + ProtonVPN  </p> <p>Gluetun is a VPN client in a thin Docker container for multiple VPN providers, written in Go, and using OpenVPN or Wireguard, DNS over TLS, with a few proxy servers built-in. </p>","path":["Media server","Gluetun - ProtonVPN + Qbittorrent"],"tags":[]},{"location":"media%20server/gluetun/#gluetun-configuration-for-protonvpn-qbittorrent","level":2,"title":"Gluetun Configuration for ProtonVPN + QBittorrent","text":"<p>Reddit Guide  </p> <p>For this I'm using a Wireguard setup instead of OpenVPN.     * It is lighter, faster, and easier to configure. </p> <p>In ProtonVPN webpage:      * go into the Downloads section and create a new WireGuard configuration. Select Router, no filtering, and \"NAT-PMP (Port Forwarding)\". Deselect VPN accelerator. When you click Create, a popup of the config will display. Copy the PrivateKey.</p> <p>Use this <code>compose.yaml</code> to setup the gluetun container for ProtonVPN and bind it to QB. The <code>.env</code> is below it. No need to configure the <code>compose.yaml</code>. It can all be done via the <code>.env</code>.</p> <p><pre><code>services:\n### -------------------------------------- ###\n### ----------- Download W/ VPN ---------- ###\n### -------------------------------------- ###\n\n### --- Qbittorrent: Torrent Client --- ###\n  qbittorrent:\n    image: lscr.io/linuxserver/qbittorrent:latest\n    container_name: qbittorrent\n    depends_on:\n      gluetun:\n        condition: service_healthy\n    environment:\n      - PUID=${PUID}\n      - PGID=${GUID}\n      - TZ=${TZ}\n      - WEBUI_PORT=8080\n      # - TORRENTING_PORT=8694 # Make sure to port forward this port in your router so you can seed more effectively\n    volumes:\n      - ${MEDIA_DATA}:/data\n      - ${APPDATA_PATH}/qbittorrent/config:/config\n\n    restart: unless-stopped\n    network_mode: \"service:gluetun\"\n\n\n### --- Glutun: VPN for Downloading --- ###\n# https://www.reddit.com/r/gluetun/comments/1kpbfs2/the_definitive_howto_for_setting_up_protonvpn/\n  gluetun:\n    image: qmcgaw/gluetun:v3\n    container_name: gluetun\n    cap_add:\n      - NET_ADMIN\n    devices:\n      - /dev/net/tun:/dev/net/tun\n    ports:\n      - 8080:8080/tcp # qbittorrent\n    environment: \n      - TZ=${TZ}\n      - UPDATER_PERIOD=24h\n      - VPN_SERVICE_PROVIDER=protonvpn\n      - VPN_TYPE=${VPN_TYPE}\n      - BLOCK_MALICIOUS=off\n      - OPENVPN_USER=${OPENVPN_USER}\n      - OPENVPN_PASSWORD=${OPENVPN_PASSWORD}\n      - OPENVPN_CIPHERS=AES-256-GCM\n      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}\n      - PORT_FORWARD_ONLY=on\n      - VPN_PORT_FORWARDING=on\n      - VPN_PORT_FORWARDING_UP_COMMAND=/bin/sh -c 'wget -O- --retry-connrefused --post-data \"json={\\\"listen_port\\\":{{PORTS}}}\" http://127.0.0.1:8080/api/v2/app/setPreferences 2&gt;&amp;1'\n      - SERVER_COUNTRIES=${SERVER_COUNTRIES}\n    volumes:\n      - ${APPDATA_PATH}/gluetun/config:/gluetun\n    restart: unless-stopped\n</code></pre> .env</p> <pre><code># Fill in either the OpenVPN or Wireguard sections. The choice of vpn is made with VPN_TYPE. Choose 'wireguard' or 'openvpn'. The settings for the other vpn type will be ignored. \n# Alter the TZ, MEDIA_DIR, and SERVER_COUNTRIES to your preference. Run 'docker run --rm -v eraseme:/gluetun qmcgaw/gluetun format-servers -protonvpn' to get a list of server countries\n\n# Base config\nTZ=Australia/Brisbane\nMEDIA_DIR=/media\n\n# Gluetun config\nVPN_TYPE=wireguard #openvpn\nSERVER_COUNTRIES=Albania,Algeria,Angola,Argentina,Australia,Austria,Azerbaijan\n\n# OpenVPN config\nOPENVPN_USER=username+pmp\nOPENVPN_PASSWORD=password\n\n# Wireguard config (example key)\nWIREGUARD_PRIVATE_KEY=privatekey12345\n</code></pre> <p>Run <code>docker compose up -d</code> to start it. </p> <p>Warning</p> <p>This WILL fail to set the port on first run. Fix below.</p> <p>Login to the Qbittorrent WebUI.      * Settings &gt; WebUI tab     * Set user/pass     * Check 'Bypass authentication for clients on localhost'     * Save. </p> <p>Restart the stack, check the container logs, and it should work. </p> <p>You can test a download from here https://webtorrent.io/free-torrents.</p>","path":["Media server","Gluetun - ProtonVPN + Qbittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/","level":1,"title":"qBittorrent","text":"<p>TRaSH's Guide   - this is copy from here</p> <p>Note</p> <p>Settings that aren't covered means you can change them to your own liking or just leave them on default.</p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#downloads","level":2,"title":"Downloads","text":"<p><code>Tools</code> =&gt; <code>Options</code> =&gt; <code>Downloads</code> (Or click on the cogwheel to access the options)</p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#when-adding-a-torrent","level":3,"title":"When adding a torrent","text":"<ol> <li> <p>For consistency with other torrents, we recommend leaving this on <code>Original</code>.</p> <p>Suggested: <code>Original</code></p> </li> <li> <p>Delete the .torrent file after it has been added to qBittorrent.</p> <p>Suggested: <code>Personal preference</code></p> </li> <li> <p>Pre-allocated disk space for the added torrents limits fragmentation and also makes sure if you use a cache drive or a feeder disk that the space is available.</p> <p>Suggested: <code>Personal Preferences</code></p> <p>Important: Disable Pre-allocation in qBittorrent if you're using unRaid with a cache drive</p> <p>Go to qBittorrent → Options → Downloads and disable this option:</p> <p><code>Pre-allocate disk space for all files</code></p> <p>When this option is enabled, it keeps the reserved space locked (in use) until you quit qBittorrent.</p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#saving-management","level":3,"title":"Saving Management","text":"<ol> <li> <p>Make sure this is set to <code>Automatic</code>. Your downloads will not go into the category folder otherwise.</p> <p>Suggested: <code>Automatic</code></p> </li> <li> <p>This helps you to manage your file location based on categories.</p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Same as <code>Step 2</code></p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Your download root path (Download folder/location).</p> <p>Read the <code>ATTENTION</code> block below</p> </li> <li> <p>If you enable this, your incomplete downloads will be placed in this directory until completed. This could be useful if you want your downloads to use a separate SSD/Feeder disk[^1], but this also results in extra unnecessary moves or in worse cases a slower and more I/O intensive copy + delete.</p> <p>Suggested: <code>Personal preference</code></p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#attention","level":4,"title":"ATTENTION","text":"<p>{! include-markdown \"../../../includes/downloaders/warning-path-location.md\" !}</p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#connection","level":2,"title":"Connection","text":"","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#listening-port","level":3,"title":"Listening Port","text":"<ol> <li> <p>Set this to TCP for the best performance</p> <p>Suggested: <code>TCP</code></p> </li> <li> <p>Your port used for incoming connections, this is the port you opened in your router/firewall or port forwarded at your VPN provider to make sure you're connectable.</p> <p>Suggested: <code>The port you opened in your router/firewall or port forwarded at your VPN provider</code></p> </li> <li> <p>This should be disabled in your router for several security reasons.</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Make sure this is disabled so you don't mess up the forwarded port.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#connections-limits","level":3,"title":"Connections Limits","text":"<p>The best settings for this depend on many factors so we won't be covering this.</p> <p>Suggested: <code>personal preference based on your setup and connection.</code></p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#proxy-server","level":3,"title":"Proxy Server","text":"<p>This is where you would add for example your SOCKS5 settings from your VPN provider.</p> <p>Suggested: <code>I personally don't recommend this insecure option being it's unencrypted and only spoofs your IP.</code></p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#speed","level":2,"title":"Speed","text":"","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#global-rate-limits","level":3,"title":"Global Rate Limits","text":"<p>Here you can set your global rate limits, meaning your maximum download/upload speed used by qBittorrent. (For all torrents)</p> <p>The best settings depend on many factors.</p> <ul> <li>Your ISP speed.</li> <li>Your hardware used.</li> <li> <p>Bandwidth needed by other services in your home network.</p> <p>Suggested: <code>For a home connection that you use with others it's best practice to set the upload/download rate to about 70-80% of your maximum upload/download speed.</code></p> </li> </ul>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#alternative-rate-limits","level":3,"title":"Alternative Rate Limits","text":"<p>When enabled, it basically does the same as above, but with the option to set up a schedule.</p> <p>Examples:</p> <ul> <li>Limit your upload/download rate during the daytime when you make the most use of it, and unlimited it during nighttime when no one is using the connection.</li> <li> <p>If you have an internet connection that's limited during specific hours (unlimited bandwidth during the night, but limited during the day)</p> <p>Suggested: <code>Personal preference</code></p> </li> </ul>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#rate-limits-settings","level":3,"title":"Rate Limits Settings","text":"<p>Not going to cover the technical part of what it does, but the following settings are recommended for best speeds (in most cases).</p> <ol> <li> <p>Prevents you from being flooded if the uTP protocol is used for any reason.</p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Apply rate limit to transport overhead</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Apply rate limit to peers on LAN</p> <p>Suggested: <code>Enabled</code></p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#bittorrent","level":2,"title":"Bittorrent","text":"","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#privacy","level":3,"title":"Privacy","text":"<ol> <li> <p>These settings are mainly used for public trackers (and should be enabled for them) and not for private trackers, decent private trackers use a private flag where they ignore these settings.</p> <p>Suggested: <code>Personal preference</code></p> </li> <li> <p>Recommended setting <code>Allow encryption</code> rather than enforcing it allows more peers to connect and is recommended on underpowered systems as it will allow for lower overhead.</p> <p>Suggested: <code>Allow encryption</code></p> </li> <li> <p>Anonymous mode hides the client's (qBittorrent) fingerprint from the peer-ID, sets the ‘User-Agent’ to Null and doesn’t share your IP address directly with trackers (though peers will still see your IP address). If using private trackers, it's recommended to <code>disable</code> this. We also got reports from people who are using this that they had worse speeds.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#torrent-queueing","level":3,"title":"Torrent Queueing","text":"<p>These options allow you to control the number of active torrents being downloaded and uploaded.</p> <p>Suggested: <code>personal preference based on your setup and connection.</code></p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#seeding-limits","level":3,"title":"Seeding Limits","text":"<ol> <li> <p>Your maximum seeding ratio preference. (When both ratio and seeding time are enabled it will trigger the action on whatever happens first.)</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Your maximum seeding time preference (When both ratio and seeding time are enabled it will trigger the action on whatever happens first.)</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>What to do when ratio or seeding time is reached.</p> <p>Suggested: <code>Paused and Disabled</code></p> </li> </ol> <p>Tip</p> <p>We recommend using the seeding goals in your Starr Apps indexer settings (enable advanced), or use qBit Manage</p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#automatically-add-these-trackers-to-new-downloads","level":3,"title":"Automatically add these trackers to new downloads","text":"<p>Recommendation: <code>Disabled</code></p> <p>Warning</p> <p> NEVER USE THIS OPTION ON (Semi-)PRIVATE TRACKERS </p>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#web-ui","level":2,"title":"Web UI","text":"","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#authentication","level":3,"title":"Authentication","text":"<ol> <li>When enabled there will be no authentication required for clients on localhost.</li> <li>When enabled there will be no authentication required for clients in the <code>step.3</code> whitelist.</li> <li>Add all IP subnets that you want to bypass authentication.</li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/qbittorrent/#security","level":3,"title":"Security","text":"<ol> <li> <p>In some cases when this is enabled it could result in issues.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>","path":["Media server","qBittorrent"],"tags":[]},{"location":"media%20server/radarr/","level":1,"title":"Radarr","text":"<p>Servarr Wiki   - This is just a copy from here</p> <p>This page is still in progress and not complete. Contributions are welcome</p> <p>For a more detailed breakdown of all the settings, check Radarr =&gt;Settings</p> <p>In this guide we will try to explain the basic setup you need to do to get started with Radarr. We're going to skip some options that you may see on the screen. If you want to dive deeper into those, please see the appropriate page in the FAQ and docs for a full explanation.</p> <p>Important</p> <p>Please note that within the screenshots and GUI settings in <code>orange</code> are advanced options, so you will need to click <code>Show Advanced</code> at the top of the page to make them visible.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#startup","level":1,"title":"Startup","text":"<p>After installation and starting up, you open a browser and go to <code>http://{your_ip_here}:7878</code></p> <p></p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#media-management","level":1,"title":"Media Management","text":"<p>First we’re going to take a look at the <code>Media Management</code> settings where we can setup our preferred naming and file management settings.</p> <p><code>Settings</code> =&gt; <code>Media Management</code></p> <p></p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#movie-naming","level":2,"title":"Movie Naming","text":"<ol> <li>Enable/Disable Renaming of your movies (as opposed to leaving the names that are currently there or as they were when you downloaded them).</li> <li>If you want illegal characters replaced or removed (<code>\\ / : * ? \" &lt; &gt; | ~ # % &amp; + { }</code>).</li> <li>This setting will dictate how Radarr handles colons within the movie file.</li> <li>Here you will select the naming convention for the actual movie files.</li> <li>(Advanced Option) This is where you will set the naming convention for the folder that contains the video file.</li> </ol> <p>If you want a recommended naming scheme and examples take a look TRaSH's Recommended Naming Schemes.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#importing","level":2,"title":"Importing","text":"<ol> <li>(Advanced Option) Enable <code>Use Hard links instead of Copy</code> more info how and why with examples TRaSH's Hard links Guide.</li> <li>(Advanced Option) Import matching extra files (subtitles, nfo, etc) after importing a file.</li> </ol>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#file-management","level":2,"title":"File Management","text":"<ol> <li>Movies deleted from disk are automatically unmonitored in Radarr.<ul> <li>You may want to delete a movie but do not want Radarr to re-download the movie. You would use this option.</li> </ul> </li> <li>(Advanced Option) Designate a location for deleted files to go to (just in case you want to retrieve them before the bin is taken out).</li> <li>(Advanced Option) This is how old a given file can be before it is deleted permanently.</li> </ol>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#root-folders","level":2,"title":"Root Folders","text":"<p>Here we will add the root folder that Radarr will be using to import your existing organized media library and where Radarr will be importing (copy/hardlink/move) your media after your download client has downloaded it.</p> <ul> <li>Non-Windows: If you're using an NFS mount ensure <code>nolock</code> is enabled.  </li> <li>If you're using an SMB mount ensure <code>nobrl</code> is enabled.</li> </ul> <p>The user and group you configured Radarr to run as must have read &amp; write access to this location.</p> <p>Your download client downloads to a download folder and Radarr imports it to your media folder (final destination) that your media server uses.</p> <p>Your download folder and media (library / root) folder can’t be the same location</p> <p>Don’t forget to save your changes</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#profiles","level":1,"title":"Profiles","text":"<p><code>Settings</code> =&gt; <code>Profiles</code></p> <p></p> <p>Here you’ll be allowed to configure profiles for which you can have for the quality, preferred language, and custom format scoring of a movie you’re looking to download.</p> <p>We recommend you to create your own profiles and only select the Quality Sources and Languages you actually want.</p> <p>For more information on foreign titles and languages see this FAQ entry</p> <p>Many users find TRaSH's Custom Format Language Guide helpful to specify the languages of movies they want.</p> <p>Profiles is also where Custom Format Scores are configured. It's strongly recommended to add the below Custom Formats from TRaSH's Guides to avoid unwanted downloads. Refer to the linked TRaSH Guide Custom Format article and additional referenced 3 TRaSH Custom Format Guides on the top of the Collection of Custom Formats page for more information.</p> <ul> <li>DV (WEB-DL) will avoid grabbing releases with Dolby Vision (DV) that have a green hue if DV is not supported.</li> <li>BR-DISK to avoid grabbing poorly named BR-DISKs that do not match the BR-DISK quality parsing.</li> </ul> <p>More info at Settings =&gt; Profiles. To see what the difference is between the Quality Sources look at our Quality Definitions.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#quality","level":1,"title":"Quality","text":"<p><code>Settings</code> =&gt; <code>Quality</code></p> <p></p> <p>Here you’re able to change/fine tune the min and max size of your wanted media files (when using Usenet keep in mind the RAR/PAR2 files)</p> <p>If you need some help with what to use for a Quality Settings check TRaSH's size recommendations for a tested example.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#indexers","level":1,"title":"Indexers","text":"<p><code>Settings</code> =&gt; <code>Indexers</code></p> <p></p> <p>Here you’ll be adding the indexer/tracker that you’ll be using to actually download any of your files.</p> <p>Once you’ve clicked the + button to add a new indexer you’ll be presented with a new window with many different options. For the purposes of this wiki Radarr considers both Usenet Indexers and Torrent Trackers as “Indexers”.</p> <p>There are two sections here: Usenet and Torrents. Based upon what download client you’ll be using you’ll want to select the type of indexer you’ll be going with.</p> <p>For torrent trackers - almost all require the use of Prowlarr or Jackett.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#download-clients","level":1,"title":"Download Clients","text":"<p><code>Settings</code> =&gt; <code>Download Clients</code></p> <p></p> <p>Downloading and importing is where most people experience issues. From a high level perspective, the software needs to be able to communicate with your download client and to have read &amp; write access to the location the download client reports files the client downloads. There is a large variety of supported download clients and an even bigger variety of setups. This means that while there are some common setups there isn’t one right setup and everyone’s setup can be a little different. But there are many wrong setups.</p> <p>See the settings page, at the More Info (Supported) page for this section, and TRaSH's Download Client Guides for more information.</p> <ul> <li>Radarr will send a download request to your client, and associate it with a label or category name that you have configured in the download client settings.<ul> <li>Examples: movies, tv, series, music, etc.</li> </ul> </li> <li>Radarr will monitor your download clients active downloads that use that category name. It monitors this via your download client's API.</li> <li>When the download is completed, Radarr will know the final file location as reported by your download client. This file location can be almost anywhere, as long as it is somewhere separate from your media folder and accessible by Radarr</li> <li>Radarr will scan that completed file location for files that Radarr can use. It will parse the file name to match it against the requested media. If it can do that, it will rename the file according to your specifications, and move it to the specified media location.</li> <li>Atomic Moves (instant moves) are enabled by default. The file system and mounts must be the same for your completed download directory and your media library. If the the atomic move fails or your setup does not support hard links and atomic moves then Radarr will fall back and copy the file then delete from the source which is IO intensive.</li> <li>If the \"Completed Download Handling - Remove\" option is enabled in Radarr's settings leftover files from the download will be sent to your trash or recycling via a request to your client to delete/remove the release.</li> </ul>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#how-to-import-your-existing-organized-media-library","level":1,"title":"How to import your existing organized media library","text":"<p>Note that Radarr does not regularly search for Movies. See the How does Radarr work? FAQ Entry for details to understand how Radarr works.</p> <p>After setting up your profiles/quality sizes and added your indexers and download client(s) it’s time to import your existing organized media library.</p> <p><code>Movies</code></p> <p></p> <p>Select <code>Import Existing Movies</code> or select <code>Import</code> from the sidebar.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#import-movies","level":2,"title":"Import movies","text":"<p>Select the root path you added earlier in the root folders section.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#importing-existing-media","level":2,"title":"Importing Existing Media","text":"<p>Depending how well you got your existing movie folders named Radarr will try to match it with the correct movie as seen at Nr.5 If all your movies are in a single directory follow this guide</p> <ol> <li> <p>Your movie folder name.</p> </li> <li> <p>Monitor - How you want the movie to be added to Radarr.</p> <ul> <li>None - Do not monitor the movie nor collection for new releases</li> <li>Movie Only - Only Monitor the movie for new releases</li> <li>Movie &amp; Collection - Monitor both the movie for new releases &amp; add and monitor any movies in the movie's collection (if exists)</li> <li> <p>Availability - When will Radarr consider a movie is available.</p> </li> <li> <p>Announced: Radarr shall consider movies available as soon as they are added to Radarr. This setting is recommended if you have good private trackers that do not have fakes.</p> </li> <li>In Cinemas: Radarr shall consider movies available as soon as movies they hit cinemas. This option is not recommended.</li> <li>Released: Radarr shall consider movies available as soon as the Blu-ray is released. This option is recommended if your indexers contain fakes often.</li> <li>Quality Profile - Select your preferred profile to use.</li> </ul> </li> <li> <p>Movie - What Radarr thinks the movie matched for. It is imperative that you review this and edit/search if the match is not correct. Mismatches are often caused by poorly named folders.</p> </li> <li> <p>Mass select Monitor status.</p> </li> <li> <p>Mass select Minimum Availability.</p> </li> <li> <p>Mass select Quality Profile.</p> </li> <li> <p>Start Importing your existing media library.</p> </li> </ol> <p>Once a movie is added to Radarr, Radarr will scan the movie's folder and attempt to match a video file in the folder to the movie. The most common cause for Radarr not matching the file and the movie thus having a Radarr Status of Missing is the filename does not have the year in it. Radarr requires the year in the filename for it to be parsable.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#no-match-found","level":3,"title":"No match found","text":"<p>If you’re getting a error like this</p> <p></p> <p>Then you probably made a mistake with your movie folder naming.</p> <p>To fix this issue you can try the following</p> <p>Expand the error message</p> <p></p> <p>and check on the themoviedb if the year or title matches. in this example you will notice that the year is wrong and you can fix it by changing the year and click on the refresh icon.</p> <p></p> <p>Or you can just use the tmdb:id or imdb:id (if tmdb is linked to imdb) and then select the found movie if matched.</p> <p></p> <p></p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#fix-faulty-folder-name-after-import","level":3,"title":"Fix faulty folder name after import","text":"<p>You will notice after the fix we did during the import that the folder name still has the wrong year in it. To fix this we’re going to do a little magic trick.</p> <p>Go to you movie overview</p> <p><code>Movies</code></p> <p>On the top click on <code>Movie Editor</code></p> <p></p> <p>After activating it you select the movie(s) from where you want to have the folder(s) to be renamed.</p> <p></p> <ol> <li>If you want all your movie folders renamed to your folder naming scheme you set earlier movie naming section.</li> <li>Select the movie(s) from where you want to have the folder(s) to be renamed.</li> <li>Choose the same <code>Root Folder</code></li> </ol> <p>A new popup will be shown</p> <p></p> <p>Select <code>Yes, Move the files</code></p> <p>Then Magic</p> <p></p> <p>As you can see the folder has been renamed to the correct year following your naming scheme.</p>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/radarr/#how-to-add-a-movie","level":2,"title":"How to add a movie","text":"<p>After you imported your existing well organized media library it’s time to add the movies you want.</p> <p><code>Movies</code> =&gt; <code>Add New</code></p> <p></p> <p>Type in the box the movie you want or use the tmdb:id or imdb:id.</p> <p>When typing out the movie name you will see it will start showing you results.</p> <p></p> <p>When you see the movie you want click on it.</p> <p></p> <ol> <li> <p>Root Folder - Radarr will add the movie to the Root Folder you’ve setup in the root folders section</p> </li> <li> <p>Monitor - How you want the movie to be added to Radarr.</p> <ul> <li>Movie Only = Radarr will monitor the RSS feed for the movie in your library that you do not have (yet) or upgrade the existing movie to a better quality.</li> <li>Movie &amp; Collection = Radarr will monitor the RSS feed for the movie in your library that you do not have (yet) or upgrade the existing movie to a better quality. It will also add all movies in this movie's collection (if any) with your selected settings.</li> <li>None = Radarr will not monitor the RSS feed, any upgrades or new movies will be ignored and have to be manually done. All searches for unmonitored movies must be manually triggered searches or interactive searches.</li> <li>Availability - When Radarr shall consider a movie is available.</li> </ul> <p>For More Information on TMDB's Dates that impact the below Availabilities See How Does Radarr Determine the Year of the Movie</p> <ul> <li>Announced: Radarr shall consider movies available as soon as they are added to Radarr. This setting is recommended if you have good private trackers (or really good public ones, e.g. rarbg.to) that do not have fakes.</li> <li>In Cinemas: Radarr shall consider movies available as soon as movies hit cinemas (Theatrical Date on TMDb) This option is not recommended.</li> <li>Released: Radarr shall consider movies available as soon as the Blu-Ray or streaming version is released (Digital and Physical dates on TMDb) This option is recommended and likely should be combined with an Availability Delay of <code>-14</code> or <code>-21</code> days.<ul> <li>If TMDb is not populated with a date, it is assumed 90 days after <code>Theatrical Date</code> (Oldest in theater's date) the movie is available in web or physical services.</li> </ul> </li> <li>Quality Profile - Select your profile to use for this movie</li> </ul> </li> <li> <p>Tags - Here you can add certain tags for advanced usage.</p> </li> <li> <p>Search on Add - Make sure you enable this if you want Radarr search for the missing movie when added to Radarr more info</p> </li> <li> <p>Click on <code>Add Movie</code> to add the movie to Radarr.</p> <ul> <li>If you get an error of \"path is already configured\" see this FAQ entry</li> </ul> </li> </ol>","path":["Media server","Radarr"],"tags":[]},{"location":"media%20server/sonarr/","level":1,"title":"Sonarr","text":"<p>Servarr Wiki   - this is just a copy from here</p> <p>This page is still in progress and not complete. Contributions are welcome</p> <p>For a more detailed breakdown of all the settings, check Sonarr =&gt;Settings</p> <p>In this guide we will try to explain the basic setup you need to do to get started with Sonarr. We're going to skip some options that you may see on the screen. If you want to dive deeper into those, please see the appropriate page in the FAQ and docs for a full explanation.</p> <p>Important</p> <p>Please note that within the screenshots and GUI settings in <code>orange</code> are advanced options, so you will need to click <code>Show Advanced</code> at the top of the page to make them visible.</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#startup","level":2,"title":"Startup","text":"<p>After installation and starting up, you open a browser and go to <code>http://{your_ip_here}:8989</code></p> <p></p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#media-management","level":2,"title":"Media Management","text":"<p>First we’re going to take a look at the <code>Media Management</code> settings where we can setup our preferred naming and file management settings.</p> <p>Click on <code>Settings</code> =&gt; <code>Media Management</code> on the left menu.</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#episode-naming","level":3,"title":"Episode Naming","text":"<ul> <li>Check the box to enable Rename Episodes.</li> <li>Decide on your Standard, Daily, and Anime episode naming conventions. You should review the recommended naming conventions in the TRaSH Guides documentation.</li> </ul> <p>If you choose not to include quality/resolution or release group, this is information you cannot regain later. It is highly recommended that you include those in your naming scheme.</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#importing","level":3,"title":"Importing","text":"<ul> <li>(Advanced Option) If you want TBA episodes to be imported immediately, change Episode Title Required to \"Never\".</li> <li>(Advanced Option) Enable <code>Use Hard links instead of Copy</code> more info how and why with examples TRaSH's Hard links Guide.</li> <li>Check the box to import extra files, and add at least <code>.srt</code> to the list.</li> </ul>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#root-folders","level":3,"title":"Root Folders","text":"<p>Here we will add the root folder that Sonarr will be using to import your existing organized media library and where Sonarr will be importing (copy/hardlink/move) your media after your download client has downloaded it. This is the folder where your series and episodes are stored for your media player to play them. It is NOT where you download files to!</p> <ul> <li>Non-Windows Users: If you're using an NFS mount ensure <code>nolock</code> is enabled.  </li> <li>If you're using an SMB mount ensure <code>nobrl</code> is enabled.</li> </ul> <p>The user and group you configured Sonarr to run as must have read &amp; write access to this location.</p> <p>Your download folder and media folder can’t be the same location</p> <p>Don’t forget to save your changes!</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#profiles","level":2,"title":"Profiles","text":"<p><code>Settings</code> =&gt; <code>Profiles</code></p> <p>We recommend you to create your own profiles and only select the Quality Sources you actually want. However, there are several prefilled quality profiles available to choose from as well, if one of those fits. If you need more information about Profiles, please see the appropriate wiki page for that section.</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#indexers","level":2,"title":"Indexers","text":"<p><code>Settings</code> =&gt; <code>Indexers</code></p> <p>Here you’ll be adding the indexer/trackers that you’ll be using to actually download any of your files.</p> <p>Once you’ve clicked the + button to add a new indexer, you’ll be presented with a new window with many different options. For the purposes of this wiki Sonarr considers both Usenet Indexers and Torrent Trackers as “Indexers”.</p> <p>There are two sections here: Usenet and Torrents. Based upon what download client you’ll be using you’ll want to select the type of indexer you’ll be going with.</p> <p>Most usenet indexers require an API key, which can be found in your Profile page on the indexer's website.</p> <p>Most torrent trackers require Prowlarr or Jackett to be used in Sonarr</p> <p>Add at least one indexer in order for Sonarr to work properly.</p> <p>See the settings page and at the More Info (Supported) page for this section for more information.</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#download-clients","level":2,"title":"Download Clients","text":"<p><code>Settings</code> =&gt; <code>Download Clients</code></p> <p>Downloading and importing is where most people experience issues. From a high level perspective, the software needs to be able to communicate with your download client and have access to the files it downloads. There is a large variety of supported download clients and an even bigger variety of setups. This means that while there are some common setups there isn’t one right setup and everyone’s setup can be a little different. But there are many wrong setups.</p> <p>See the settings page, at the More Info (Supported) page for this section, and TRaSH's Download Client Guides for more information.</p> <ul> <li>Sonarr will send a download request to your client, and associate it with a label or category name that you have configured in the download client settings.<ul> <li>Examples: movies, tv, series, music, etc.</li> </ul> </li> <li>Sonarr will monitor your download clients active downloads that use that category name. It monitors this via your download client's API.</li> <li>When the download is completed, Sonarr will know the final file location as reported by your download client. This file location can be almost anywhere, as long as it is somewhere separate from your media folder and accessible by Sonarr</li> <li>Sonarr will scan that completed file location for files that Sonarr can use. It will parse the file name to match it against the requested media. If it can do that, it will rename the file according to your specifications, and move it to the specified media location.</li> <li>Atomic Moves (instant moves) are enabled by default. The file system and mounts must be the same for your completed download directory and your media library. If the the atomic move fails or your setup does not support hard links and atomic moves then Sonarr will fall back and copy the file then delete from the source which is IO intensive.</li> <li>If the \"Completed Download Handling - Remove\" option is enabled in Sonarr's settings leftover files from the download will be sent to your trash or recycling via a request to your client to delete/remove the release.</li> </ul>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#how-to-import-your-existing-organized-media-library","level":2,"title":"How to import your existing organized media library","text":"<p>Note that Sonarr does not regularly search for Episodes. See the FAQ Entry for details to understand how Sonarr works. How does Sonarr find episodes?</p> <p>After setting up your profiles/quality sizes and added your indexers and download client(s) it’s time to import your existing organized media library.</p> <p>Coming soon - Contributions Welcome</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#importing-existing-media","level":3,"title":"Importing Existing Media","text":"<p>Depending how well your existing series folders are named, Sonarr will try to match it with the correct series. You should review this list carefully before importing.</p> <p>Library Import is only to be used on an existing organized library and shall not be used on a download folder or to ad-hoc import media.</p> <ol> <li>Navigate to Library Import</li> <li>Read and understand the Library Import Help Text</li> <li>Select or add the root (library) folder to import series from</li> <li>Review Sonarr's mapping/matching of Series Folders to TVDb series</li> <li>Set your monitoring settings and quality profile as appropriate</li> <li>Click Start Import</li> </ol>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#no-match-found","level":4,"title":"No match found","text":"<ol> <li>Search the series name or TVDbId in the series selection box</li> <li>See this FAQ entry if the series cannot be found</li> </ol>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#fix-faulty-folder-name-after-import","level":4,"title":"Fix faulty folder name after import","text":"<ol> <li>Remove the Series from Sonarr</li> <li>Library Import</li> <li>Ensure the series is mapped correctly</li> </ol>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#add-new-series","level":2,"title":"Add New Series","text":"<p>Refer to the Library Page for additional information</p>","path":["Media server","Sonarr"],"tags":[]},{"location":"media%20server/sonarr/#import-episodes","level":2,"title":"Import Episodes","text":"<ul> <li>Use Wanted =&gt; Manual Import to import episode files to their series folders on an ad-hoc basis</li> <li>Use Manage Episodes on a series' page to remap or map existing episode files in a series folder</li> </ul>","path":["Media server","Sonarr"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/","level":1,"title":"NGINX Proxy Mananger","text":"","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/#github","level":2,"title":"Github","text":"","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/#overview","level":2,"title":"Overview","text":"<p>NGINX Proxy Mananger is a user-friendly, web-based interface for configuring Nginx as a reverse proxy, simplifying the management of multiple web services on a single IP address, especially for home labs and self-hosted applications, by offering easy setup for proxy hosts, Let's Encrypt SSL certificates, redirects, access controls, and stream proxying, all within Docker containers. It removes the need for manual Nginx configuration, allowing quick deployment and management of secure web access to internal services through a simple dashboard. </p> <p>Instead of remembering 8000 different <code>IP-ADDR:PORT</code> combos, you can use NPM to set a reverse proxy to <code>service.domain.me</code>. </p>","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/#installation","level":2,"title":"Installation","text":"","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/#docker-compose","level":3,"title":"Docker Compose","text":"<pre><code>services:\n  npm:\n    image: jc21/nginx-proxy-manager:latest\n    container_name: npm\n    restart: unless-stopped\n    ports:\n      - \"80:80\"     # HTTP\n      - \"81:81\"     # Admin UI\n      - \"443:443\"   # HTTPS\n    environment:\n      DB_SQLITE_FILE: \"/data/database.sqlite\"\n    volumes:\n      - ${APPDATA}/ingress/npm/data:/data\n      - ${APPDATA}/ingress/npm/letsencrypt:/etc/letsencrypt\n</code></pre>","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/overview/#usage","level":2,"title":"Usage","text":"<ul> <li> <p>Navigate to you <code>IP-ADDR:PORT</code> address, using the Admin UI port set in your <code>compose.yaml</code> above. </p> </li> <li> <p>Login with:</p> <ul> <li>EMAIL: EMAIL@example.com</li> <li>Password: changeme</li> </ul> <p>(probably best to change the password immediately.)</p> </li> <li> <p>Go to SSL Certificates, \"Add Certificates\", select \"Let's Encrypt\". </p> </li> <li> <p>Enter your DDNS address, valid email, enable \"Use a DNS challenge\", choose your DNS provider, add your DDNS token, accept TOS and save.</p> </li> </ul> <p>Note</p> <p>You can get a DDNS from multiple providers including DuckDNS, cloudflare, or porkbun. </p> <p>To set a Host: </p> <ul> <li> <p>Select \"Hosts\" &gt; \"Proxy Hosts\" </p> </li> <li> <p>Add Proxy Host</p> </li> <li> <p>Under Domain Names input desired domain</p> <ul> <li> <p>service.domain.com</p> </li> <li> <p>or whatever you want it to be called</p> </li> </ul> </li> <li> <p>Scheme: HTTP (the reverse proxy w/ your SSL cert will forward to HTTPS).</p> </li> <li> <p>Forward your hostname / IP of the hosting machine/service. </p> </li> <li> <p>Forward Port - use port defined in service. This would be the host port defined in a container. </p> <ul> <li>e.g. a <code>compose.yaml</code> with: <pre><code>ports:\n  - '3005:8080'\n</code></pre> Would use port 3005 as the forwarded IP, not 8080. </li> </ul> </li> <li> <p>Choose your ACL rule. </p> </li> <li> <p>Choose \"Block COmmon Exploits\" and \"Websockets support\" </p> </li> <li> <p>Select the SSL tab at the top</p> <ul> <li> <p>Select the SSL cert made earlier. </p> </li> <li> <p>Force SSL and HTTP/2 support. </p> </li> </ul> </li> <li> <p>Save</p> </li> </ul> <p>Now you can access <code>service.domain.com</code> instead of <code>192.168.3.20:3005</code>.</p>","path":["Networking","Nginx Proxy Manager","NGINX Proxy Mananger"],"tags":[]},{"location":"networking/Nginx%20Proxy%20Manager/pihole/","level":1,"title":"NPM with Pi-Hole","text":"<p>Turns out setting up Pi-Hole with NPM requires extra work and took me entirely too goddamn long to figure that out, so here is the fix. Found this on reddit thread.</p> <p>Nginx Setup</p> <p>On the nginx web gui, add a new Proxy Host.</p> <pre><code>Details\n\n    Domain names = pihole.mydomain.com\n\n    Scheme = http\n\n    Forward Hostname/IP = &lt;pihole_ip&gt;\n\n    Forward Port = 80\n\nSSL\n\nYour Let's Encrypt cert. *.mydomain.com\n\nForce SSL = Yes\n\nHTTP/2 Support = Yes\n</code></pre> <p>In the custom confg (this is the magic) <pre><code> location / {\n proxy_pass http://&lt;pihole_ip&gt;:80/admin/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n\n location /admin/ {\n proxy_pass http://&lt;pihole_ip&gt;:80/admin/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n\n location /api/ {\n proxy_pass http://&lt;pihole_ip&gt;:80/api/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n</code></pre> It seems you have to define /, /admin/, and /api/ locations with the full URL with no nginx variables.</p> <p>With this setup, I am able to access Pihole via pihole.mydomain.com on my local network with the full dashboard and graphs working. 10</p> <p>Reddit link</p>","path":["Networking","Nginx Proxy Manager","NPM with Pi-Hole"],"tags":[]},{"location":"networking/cloudflare/tunnels/","level":1,"title":"Cloudflare Tunnels","text":"<p>Cloudflare docs </p> <p>This covers creating a cloudflare tunnel in docker, serving a container with it, and securing it using an access policy. This is assuming you already have a domain and CNAME DNS record setup with cloudflare. </p>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#creating-a-tunnel","level":2,"title":"Creating A Tunnel","text":"<p>First, you need to generate a tunnel and get its Tunnel Token.</p> <ol> <li> <p>Login to the Cloudflare Zero Trust Dashboard</p> </li> <li> <p>Navigate to Networks &gt; Tunnels.</p> </li> <li> <p>Click Create a tunnel, select cloudflared, and give it a name.</p> </li> <li> <p>On the \"Install connector\" page, select Docker.</p> </li> <li> <p>Copy the Token from the provided command (it's the long string after --token).</p> </li> </ol>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#deploy-the-connector","level":2,"title":"Deploy the Connector","text":"<p>There are multiple ways to deploy a tunnel. This is using a docker <code>compose.yaml</code> file. This allows you to easily manage the container as well as the tunnel together or seperately. </p> <pre><code>services:\n  cloudflared:\n    image: cloudflare/cloudflared:latest\n    container_name: cloudflared\n    restart: unless-stopped\n    environment:\n      - TUNNEL_TOKEN=your_token_here  # Paste your token from step 1\n    command: tunnel --no-autoupdate run\n    networks:\n      - tunnel-net\n\n  my-app:\n    image: your-app-image:latest\n    container_name: my-app\n    networks:\n      - tunnel-net\n    # No ports need to be exposed to the host!\n\nnetworks:\n  tunnel-net:\n    name: tunnel-net\n</code></pre> <p>NOTE: The <code>cloudflared</code> container must be on the same Docker network as the service you want to expose.</p>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#route-traffic-public-hostname","level":2,"title":"Route Traffic (Public Hostname)","text":"<p>Now that the connector is running, tell Cloudflare where to send the traffic:</p> <ol> <li> <p>Back in the Cloudflare Dashboard, go to the Public Hostname tab for your tunnel.</p> </li> <li> <p>Click Add a public hostname.</p> </li> <li> <p>Fill in your details:</p> <pre><code>Subdomain: app\n\nDomain: yourdomain.com\n\nService Type: HTTP\n\nURL: my-app:80 (Use the Docker container name and internal port).\n</code></pre> </li> <li> <p>Save the configuration.</p> </li> </ol>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#secure-tunnel","level":2,"title":"Secure Tunnel","text":"<p>Using an access policy, you can secure the initial access to the tunnel. Multiple options are available including email, Indent providers (github, google, etc) and more. Im using an email for simplicity here. </p>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#create-the-access-application","level":3,"title":"Create the Access Application","text":"<ol> <li> <p>In the Zero Trust Dashboard, navigate to Access &gt; Applications.</p> </li> <li> <p>lick Add an application and select Self-hosted.</p> </li> <li> <p>Application Configuration:</p> <p>Application name: (e.g., My App Protection)</p> <p>Session Duration: How long a user stays logged in.</p> <p>Application domain: Enter the Subdomain and Domain that matches exactly what you set up in your Tunnel (e.g., app.yourdomain.com).</p> </li> <li> <p>Click Next.</p> </li> </ol>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/cloudflare/tunnels/#create-access-policy","level":3,"title":"Create Access Policy","text":"<ol> <li> <p>This defines who is allowed to get through the lock.</p> <p>Policy Name: (e.g., Allow Casey Only)</p> <p>Action: Ensure this is set to Allow.</p> <p>Assign a group (Optional): You can skip this for a simple rule.</p> <p>Configure rules:</p> <pre><code>Include: Select Emails or Email ending in.\n\nRequire: Select Emails.\n\nValue: Enter your specific email address.\n</code></pre> </li> <li> <p>Click Next through the Setup page and then click Add application.</p> </li> </ol> <p>Your app, served at app.domain.com should now be forwarded via a cloudflare tunnel, and secure with an inital check by cloudflare. </p>","path":["Networking","Cloudflare","Cloudflare Tunnels"],"tags":[]},{"location":"networking/opnsense/opnsense/","level":1,"title":"OPNSense","text":"<p>OPNSense docs  </p> <p>OPNsense® is an open source, easy-to-use and easy-to-build FreeBSD based firewall and routing platform.</p> <p>OPNsense includes most of the features available in expensive commercial firewalls, and more in many cases. It brings the rich feature set of commercial offerings with the benefits of open and verifiable sources.</p>","path":["Networking","Opnsense","OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsense/#installation","level":2,"title":"Installation","text":"<p>For my installation I used an M920q. This would require 2 things in order to allow for further network connectivity. </p> <ul> <li> <p>Proprietary Lenovo PCIe Riser Card - P/N: 01AJ940</p> </li> <li> <p>INTEL I350T4 1GbE Quad Port Ethernet Server Adapter</p> </li> </ul> <p>For the software installation, the standard installation method can be followed here.</p> <p>Note</p> <p>Pay attention to the inital network interface gievn after installation. This will be needed to access later. </p>","path":["Networking","Opnsense","OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsense/#initial-configuration","level":2,"title":"Initial Configuration","text":"<p>After installation, the system will prompt you for the interface assignment. If you ignore this, then the default settings will be applied. Installation ends with the login prompt.</p> <p>By default you have to log in to enter the console.</p> <p>Welcome message</p> <pre><code>* * * Welcome to OPNsense [OPNsense 15.7.25 (amd64/OpenSSL) on OPNsense * * *\n\nWAN (em1)     -&gt; v4/DHCP4: 192.168.2.100/24\nLAN (em0)     -&gt; v4: 192.168.1.1/24\n\nFreeBSD/10.1 (OPNsense.localdomain) (ttyv0)\n\nlogin:\n</code></pre> <p>Tip</p> <p>A user can login to the console menu with his credentials. The default credentials after a fresh install are username “root” and password “opnsense”.</p> <p>VLANs and assigning interfaces</p> <p>If you choose to do manual interface assignment or when no config file can be found then you are asked to assign Interfaces and VLANs. VLANs are optional. If you do not need VLANs then choose no. You can always configure VLANs at a later time.</p> <p>LAN, WAN and optional interfaces</p> <p>The first interface is the LAN interface. Type the appropriate interface name, for example “em0”. The second interface is the WAN interface. Type the appropriate interface name, e.g. “em1” . Possible additional interfaces can be assigned as OPT interfaces. If you assigned all your interfaces you can press [ENTER] and confirm the settings. OPNsense will configure your system and present the login prompt when finished.</p> <p>Important</p> <p>The interface is especially important to note as the WAN/LAN port configuration needs to be followed to correctly interface to router/network. </p> <p>Console</p> <p>The console menu shows 13 options.</p> <pre><code>0)     Logout                              7)      Ping host\n1)     Assign interfaces                   8)      Shell\n2)     Set interface(s) IP address         9)      pfTop\n3)     Reset the root password             10)     Filter logs\n4)     Reset to factory defaults           11)     Restart web interface\n5)     Reboot system                       12)     Upgrade from console\n6)     Halt system                         13)     Restore a configuration\n</code></pre> <p>opnsense-update</p> <p>OPNsense features a command line interface (CLI) tool “opnsense-update”. Via menu option 8) Shell, the user can get to the shell and use opnsense-update.</p> <p>For help, type man opnsense-update and press [Enter].</p> <p>Upgrade from console</p> <p>The other method to upgrade the system is via console option 12) Upgrade from console</p> <p>GUI</p> <p>An update can be done through the GUI via <code>System ‣ Firmware ‣ Updates.</code></p>","path":["Networking","Opnsense","OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsense/#migration-to-opnsense","level":2,"title":"Migration to OPNSense","text":"<p>To allow for ease of transitioning to OPNSense from an existing router/firewall (in my case a TP-Link ER605), it is best to setup a few things before installtion the OPNSense router. This can be done with the setup wizard, as well as direct settings.</p> <p>Note</p> <p>The default IP for access is 192.168.1.1. Wizard can be re-run under `System ‣ Configuration ‣ Wizard.</p> <ul> <li> <p>1) Under <code>Interfaces ‣ [WAN]</code>: Ensure your IPv4 configuration Type is set to <code>DHCP</code> (same for IPv6) [if applicable].</p> </li> <li> <p>2) Under <code>Interfaces ‣ [LAN] ‣ Static IPv4 configuration</code>: Set the IPv4 address to the same IP as current router. </p> </li> <li> <p>3) Remove existing router, install new OPNSense router. May need flush DNS cache following. </p> </li> </ul>","path":["Networking","Opnsense","OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsense/#tailscale-dns-migration","level":2,"title":"Tailscale DNS migration","text":"<ul> <li> <p>Ensure DNS Nameserver is no longer set to pi-hole/previous DNS sinkhole. If using OPNSense as primary DNS, set the Global Nameserver under Tailscale Admin Console &gt; DNS.</p> </li> <li> <p>Reconnect to tailscale using <code>sudo tailscale up --reset --accept-dns=false</code> - this will wipe all previous flags and allow use of the OPNSense DNS. </p> </li> </ul> <p>To install tailscale on OPNSense</p> <p>Tailscale docs   (old)</p> <ul> <li> <p>Go to System &gt; Firmware &gt; Plugins. (Enable community plugins)</p> </li> <li> <p>Install os-tailscale.</p> </li> <li> <p>Once installed, it will appear in VPN &gt; Tailscale. </p> </li> <li> <p>Generate an Auth Key (Tailscale admin &gt; settings &gt; keys)</p> </li> <li> <p>Paste the key, leave server blank </p> </li> <li> <p>Advertise Exit Node - route internet traffic through OPNsense. </p> </li> <li> <p>Accept Subnet Routes - LAN Access</p> </li> <li> <p>Advertised Routes tab &gt; Add your subnet to advertise (e.g. 192.168.1.0/24)</p> </li> <li> <p>In the TS admin console, should see the new machine with the \"subnets\" flag. Hit edit and approve the route. </p> </li> <li> <p>Advertise your LAN subnet (192.168.1.0/24) from OPNsense.</p> </li> </ul>","path":["Networking","Opnsense","OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsenseadguard/","level":1,"title":"Setting up AdGuard Home on OPNSense","text":"<p>This guide is long as shit, so just refer to it:</p> <p>https://windgate.net/setup-adguard-home-opnsense-adblocker/</p>","path":["Networking","Opnsense","Setting up AdGuard Home on OPNSense"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/","level":1,"title":"Tailscale &amp; OPNsense Configuration","text":"<p>This covers how to allow remote access to local network via Tailscale (Subnet Routing) and resolve local domains (Split DNS) via AdGuard Home/Unbound.</p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#fix-ip-access-subnet-routing","level":1,"title":"Fix IP Access (Subnet Routing)","text":"<ul> <li>Makes local IPs (e.g., <code>192.168.0.2:3000</code>) accessible from tailscale remote clients </li> </ul>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#1-advertise-the-route-opnsense","level":3,"title":"1. Advertise the Route (OPNsense)","text":"<ol> <li>Navigate to VPN &gt; Tailscale &gt; Settings.</li> <li>Check Advertise Routes.</li> <li>Enter your LAN subnet (e.g., <code>192.168.0.0/24</code>).</li> <li>Click Save.</li> </ol> <p>CLI Alternative</p> <p>If you are using the CLI instead of the plugin, run: <pre><code>tailscale up --advertise-routes=192.168.0.0/24\n</code></pre></p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#2-approve-the-route-tailscale-console","level":3,"title":"2. Approve the Route (Tailscale Console)","text":"<ol> <li>Open the Tailscale Admin Console.</li> <li>Find your OPNsense machine in the list.</li> <li>Click the Three Dots (...) &gt; Edit Route Settings.</li> <li>Toggle ON the subnet route (<code>192.168.0.0/24</code>) you previously advertised.</li> </ol>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#3-allow-traffic-opnsense-firewall","level":3,"title":"3. Allow Traffic (OPNsense Firewall)","text":"<p>Without this step, OPNsense will drop packets originating from the Tailscale interface.</p> <ol> <li>Go to Firewall &gt; Rules &gt; Tailscale.<ul> <li>Note: This interface might be named differently if you manually assigned it.</li> </ul> </li> <li>Add a new rule:<ul> <li>Action: Pass</li> <li>Source: <code>Tailscale net</code> (or <code>any</code>)</li> <li>Destination: <code>LAN net</code> (or <code>any</code>)</li> </ul> </li> <li>Click Save and Apply Changes.</li> </ol>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#configure-split-dns","level":1,"title":"Configure Split DNS","text":"<ul> <li> <p>Make local domains (e.g., <code>service.example.com</code>) resolve to the internal Nginx Proxy Manager IP over VPN.</p> </li> <li> <p>Configuring the Unbound/AGH DNS as the global DNS will also block all ads/trackers etc across the whole tailnet. </p> </li> </ul>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#1-get-opnsense-tailscale-ip","level":3,"title":"1. Get OPNsense Tailscale IP","text":"<ol> <li>In OPNsense, go to Interfaces &gt; Overview.</li> <li>Copy the Tailscale IP address (starts with <code>100.x.y.z</code>).</li> </ol>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#2-configure-split-dns-tailscale-console","level":3,"title":"2. Configure Split DNS (Tailscale Console)","text":"<ol> <li>Go to Tailscale Admin Console &gt; DNS.</li> <li>Scroll to Nameservers.</li> <li>Click Add Nameserver &gt; Custom.</li> <li>Configure the following:<ul> <li>Nameserver: Paste the OPNsense Tailscale IP (<code>100.x.y.z</code>) from Step 1.</li> <li>Restrict to domain: Toggle ON.</li> <li>Domain: Enter your local domain (e.g., <code>example.com</code>).</li> </ul> </li> <li>Click Save.</li> </ol> <p>Result</p> <p>When you type <code>example.com</code>, Tailscale will query OPNsense. For all other traffic (e.g., google.com), it will use the client's default DNS.</p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#3-allow-dns-queries-to-npm-for-local-domains-adguard-home","level":3,"title":"3. Allow DNS Queries to NPM for local domains (AdGuard Home)","text":"<p>Your Adguard Home / Unbound DNS resolver will, by default and by design, look to upstream interent for domain resolution when requested. AdGuard needs to know that <code>*.example.com</code> should be redirected to your Nginx Proxy Manager (NPM) internal IP. For this you need a DNS rewrite in Adguard Home.</p> <ol> <li> <p>Open your AdGuard Home dashboard.</p> </li> <li> <p>Go to Filters &gt; DNS Rewrites.</p> </li> <li> <p>Click Add DNS rewrite.</p> </li> <li> <p>Domain: *.example.com (The * is a wildcard, so it covers service.example.com, nas.example.com, etc.)</p> </li> </ol> <p>Note</p> <p>If AdGuard complains about the wildcard, just add example.com and individual subdomains, but wildcards usually work.</p> <ol> <li> <p>Enter: 192.168.0.x (Enter the LAN IP address of the device running Nginx Proxy Manager).</p> </li> <li> <p>Click Save.</p> </li> </ol>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#3a-allow-dns-queries-to-npm-for-local-domains-unbound","level":3,"title":"3A. Allow DNS Queries to NPM for local domains (Unbound)","text":"<p>If using unbound to forward local requests: </p> <ol> <li> <p>Go to Services &gt; Unbound DNS &gt; Overrides.</p> </li> <li> <p>Click + to add a new override.</p> </li> <li> <p>Domain: example.com</p> </li> <li> <p>IP: 192.168.0.x (Nginx Proxy Manager IP).</p> </li> <li> <p>Description: Local NPM. (or whatever)</p> </li> <li> <p>Click Save and Apply.</p> </li> </ol> <p>Note</p> <p>You usually have to add separate entries for subdomains (e.g., service) unless you check \"Host Overrides\" settings for wildcard support. Adguard works better, found these steps while fixing something so I figured I'd add it here. </p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#interfaces-rebinds","level":1,"title":"Interfaces &amp; Rebinds","text":"<p>If it still isn't working, check these common configuration errors:</p> <ol> <li>Interface Binding: Ensure AdGuard Home is actually listening on the Tailscale interface (or \"All Interfaces\").</li> <li>Bootstrap DNS: In AdGuard Home (Settings &gt; DNS Settings), set \"Bootstrap DNS servers\" to your OPNsense LAN IP or <code>127.0.0.1</code>.</li> <li>DNS Rebind Check:<ul> <li>Go to System &gt; Settings &gt; General.</li> <li>Ensure DNS Rebind Check is unchecked.</li> <li>Alternatively: Add your specific domain to the whitelist.</li> </ul> </li> </ol> <p>DNS Rebind Protection</p> <p>If \"DNS Rebind Check\" is active, OPNsense may block the DNS response because it resolves a public domain to a private RFC1918 IP address.</p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/opnsense/opnsensetailscale/#verification","level":2,"title":"Verification","text":"<p>To verify the setup is working:</p> <ol> <li>Disconnect your phone from WiFi (switch to 5G/LTE).</li> <li>Enable the Tailscale VPN.</li> <li>Attempt to access a service via IP: <code>http://192.168.0.2:3000</code> (Should load).</li> <li>Attempt to access a service via Domain: <code>http://service.example.com</code> (Should load).</li> </ol> <p>```</p>","path":["Networking","Opnsense","Tailscale &amp; OPNsense Configuration"],"tags":[]},{"location":"networking/vlan/vlan/","level":1,"title":"VLANs - OPNSense &amp; Omada Controller","text":"<p>OPNSense Docs  </p> <p>Zenarmor Tutorial  </p> <p>Home Network Guy Youtube  </p> <p>I am only using IPv4 for this guide. Not familiar enough with IPv6, and I don't feel like dealing with for now. I'll come around to it (maybe).</p> <p>This is made for DNSMasq and Unbound DNS specifically, but same ideas apply if using something else. Same for omada controller (for the most part) </p>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#vlan-structure","level":2,"title":"VLAN Structure","text":"<p>I currently have my network setup as this:</p> <p>You can set this however works for you. </p> <ul> <li> <p>VLAN 10: Management (Mgmt) - 10.10.10.x</p> <p>Devices: Omada Switches, WAPs, Omada Controller, Proxmox Web GUI, iDRAC/IPMI.</p> <p>Rules: strict access; only your main PC can reach this.</p> </li> <li> <p>VLAN 20: Trusted LAN - 10.10.20.x</p> <p>Devices: Personal, known devices. </p> </li> <li> <p>VLAN 30: Lab / Server - 10.10.30.x</p> <p>Devices: VMs, Docker Containers. </p> </li> <li> <p>VLAN 50: IoT - 10.10.50.x</p> <p>Devices: Smart plugs, Alexa, Thermostats, random IoT stuff. </p> <p>Rules: Can access Internet, CANNOT access other VLANs.</p> </li> <li> <p>VLAN 99: Guest - 10.10.99.x</p> <p>Devices: Random, new, unkown devices. </p> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#vlan-creation-in-opnsense","level":2,"title":"VLAN Creation in OPNSense","text":"","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#wan-settings","level":3,"title":"WAN Settings","text":"<p>Go to Interfaces &gt; [WAN]</p> <ul> <li>IPv4 Configuration Type = DHCP</li> </ul> <p>This will allow your OPNSense router to be able to hand out DHCP addresses to the rest of the network - assuming your ISP allows this.</p>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#create-a-new-vlan","level":3,"title":"Create a New VLAN","text":"<p>Go to Interfaces &gt; Devices &gt; VLAN. Configure each VLAN as:</p> <p>Note</p> <p>I am using the management interface for the examples, same ideas apply for the others. </p> <ul> <li> <p><code>Device</code>: [blank - will auto create name, can be custom if wanted.]</p> </li> <li> <p><code>Parent</code>: igb0 (12.34.56.mac.address) (physical interface that will carry traffic)</p> </li> <li> <p><code>VLAN tag</code>: 10 </p> </li> <li> <p><code>VLAN Priority:</code> Best effort (0, default) </p> </li> <li> <p><code>Description:</code> MANAGEMENT (or whatever you want)</p> </li> <li> <p>Repeat this for all other VLANs</p> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#interface-assignment","level":3,"title":"Interface Assignment","text":"<p>Go to Interface &gt; Assignments</p> <ul> <li> <p>Assign each new VLAN (device).</p> </li> <li> <p>Match the description name </p> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#vlan-ip-configuration-settings","level":3,"title":"VLAN IP Configuration Settings","text":"<p>Go to each new interface - [MANAGMENT], [TRSUTED]. etc.</p> <ul> <li> <p>Enable each interface</p> </li> <li> <p>Prevent interface removal </p> </li> <li> <p>IPv4 Configuration Type = Static IPv4</p> </li> <li> <p>Static IPv4 Configuration - this will assign the gateway IP address for your LAN. </p> <ul> <li>IPv4 Address = 10.10.10.1/24 (be sure to change this to /24).</li> </ul> </li> <li> <p>(Usually for LAN/VLANs) leave Block private networks and Block bogon networks unchecked. Enable them only on WAN-like interfaces.</p> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#dhcp-dns-settings","level":2,"title":"DHCP / DNS Settings","text":"","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#dhcp","level":3,"title":"DHCP","text":"<p>1. Go to Services &gt;  Dnsmasq DNS &amp; DHCP &gt; General</p> <p>Important</p> <p>Make sure the VLAN interfaces are selected uunder Interface.</p> <p>2. Go to Services &gt;  Dnsmasq DNS &amp; DHCP &gt; DHCP Ranges</p> <ul> <li> <p>Press the + button </p> </li> <li> <p>Select desired interface</p> </li> <li> <p>Start Address = 10.10.10.100</p> </li> <li> <p>End Address = 10.10.10.200</p> </li> <li> <p>Repeat this for all other interfaces, changing 3<sup>rd</sup> octect.</p> <ul> <li> <p>MANAGEMENT = 10.10.10.100 - 10.10.10.200</p> </li> <li> <p>TRUSTED = 10.10.20.100 - 10.10.20.200</p> </li> <li> <p>ETC.</p> </li> </ul> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#dns","level":3,"title":"DNS","text":"<p>For client configuration see: Client DHCP and DNS configuration</p> <p>1. Go to Services &gt; Unbound DNS &gt; General </p> <ul> <li> <p>Listen port: 53</p> </li> <li> <p>Enable DNSSEC Support: Enabled (if upstream DNS supports)</p> </li> <li> <p>Register DHCP Static Mappings: Enabled</p> <ul> <li>This will register all static DHCP reservations. This will auto create a DNS hostname. Handy.</li> </ul> </li> <li> <p>Do not register A/AAAA records: Enabled.</p> <ul> <li> <p>By default, OPNSense will register all IPs for all interfaces to routers hostname (e.g. OPNSense.internal). Hostname lookup will return all IPs, exposing all IPs and gets in the way.</p> </li> <li> <p>Enabling this also allows you to access router web interface more easily from seperate VLAN if using a DNS override to lookup hostname. </p> </li> </ul> </li> <li> <p>Flush DNS Cache during reload: Enabled </p> <ul> <li>Allows changes to apply immediately after reload, instead of waiting for new lease. </li> </ul> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#firewall-rules","level":2,"title":"Firewall Rules","text":"<p>Tip</p> <p>Basic setup for the rules is: all VLANs can access internet, but not eachother unless specified.</p> <p>1. Go to Firewall &gt; Aliases</p> <p>Aliases will group together IPs and networks, much easier to create rules</p> <ul> <li>Create new Alias by pressing + button. Configure as: </li> </ul> <p><pre><code>Name: PrivateNetworks\n\nType: Network(s) \n\nContent: 10.0.0./8 172.16.0.0/12 192.168.0.0/16 \n    # All private IP ranges\n\nDescription: All private IP addresses\n</code></pre> 2. Go to Firewall &gt; Groups</p> <ul> <li>Configure as:</li> </ul> <pre><code>Name: private\n    # This will generate a network name as \"private net\" like other interfaces, easier to manange. \n\nMembers: LAN and all VLANs - NOT WAN\n\n(no) GUI Groups: Enabled \n    # Will not group them under Interfaces page.\n\nDescription: All private interfaces. \n</code></pre> <p>3. Go to Firewall &gt; Rules </p> <p>This will create 2 rules. One to allow internet access without other VLAN access. The other will </p> <ul> <li>Select MANAGEMENT interface</li> </ul> <p>Note</p> <p>By default on a new interface, all access is blocked. Need to make rules to allow. </p> <ul> <li>Configure first rule as: </li> </ul> <p><pre><code>Action: Pass\n\nInterface: MANAGMENT (should auto fill)\n\nTCP/IP: IPv4 \n\nProtocol: any\n\nSource: MANAGEMENT net\n\nDestination: private net\n\nDestination / Invert: ENABLED # IMPORTANT \n    # This will allow access to internet, but blocking all other private IPs. Inverting the rule above. \n\nDestination Port Range: any | any (can only select if other protocols selected)\n\nDescription: Internet access - block local network. \n</code></pre> * Because first rule blocks all private networks, the MANAGEMENT interface's IP address (10.10.10.1), the second rule will correct that for selected interface only. </p> <ul> <li>Configure second rule as:</li> </ul> <pre><code>Action: Pass\n\nInterface: MANAGMENT (should auto fill)\n\nTCP/IP: IPv4 \n\nProtocol: UDP\n\nSource: MANAGEMENT net\n\nDestination: MANAGMENT address \n\nDestination / Invert: disabled\n\nDestination Port Range: DNS | DNS \n\nDescription: Allow DNS on MANAGMENT network \n</code></pre> <p>Important</p> <p>Repeat this for all other interfaces. Ensuring to change the selected net and addresses for each. For first rule, \"private net\" should be destination on all. You can clone the rules and make it faster. </p>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#switch-configuration-omada-controller","level":2,"title":"Switch Configuration - Omada Controller","text":"","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#vlan-profile-creation","level":3,"title":"VLAN Profile Creation","text":"<ul> <li> <p>Open the Omada Controller WebUI </p> </li> <li> <p>Go to Settings &gt; Wired and Wireless Networks &gt; LAN</p> </li> <li> <p>Select Create New LAN </p> </li> <li> <p>Configure as:</p> <ul> <li><code>Name</code>: VLAN Name </li> <li><code>Purpose</code>: VLAN</li> <li><code>VLAN</code>: Enter the tag defined in your OPNSense configuration above</li> <li>Other options availabe as needed for different configs are listed. </li> </ul> </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#vlan-assignment","level":3,"title":"VLAN assignment","text":"<ul> <li>Select Devices on sidebar</li> <li>Select Ports<ul> <li>Click the edit icon on the port you want to configure</li> <li>Set the profile to the VLAN profile created in Omada. </li> </ul> </li> <li>This should be working correctly now. </li> </ul>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlan/#testing","level":2,"title":"Testing","text":"<p>Testing confirms whether the VLAN works as expected before deploying it widely.</p> <ol> <li>Connect a device (e.g., laptop or VM) to a switch port assigned to the VLAN.</li> <li>Ensure it receives an IP address from the correct VLAN subnet.</li> <li>Test connectivity (e.g., ping the gateway or browse the internet).</li> <li>Verify that firewall rules are functioning as intended. This helps identify and fix any misconfigurations early.</li> </ol> <p>Linux - <code>ip a</code> should show newly assigned, correct, subnet</p> <p>Windows - <code>ipconfig</code> </p>","path":["Networking","Vlan","VLANs - OPNSense &amp; Omada Controller"],"tags":[]},{"location":"networking/vlan/vlandhcp/","level":1,"title":"DHCPv4 with DNS registration","text":"<p>OPNSense DNSMasq Docs  </p> <p>Dnsmasq can be used as a DNS forwarder. Though in our recommended setup, we will not use it as our default DNS server.</p> <p>We will use Unbound as primary DNS server for our clients, and only forward some internal zones to Dnsmasq which manages the hostnames of DHCP registered leases.</p> <p>This requires Dnsmasq to run with a non-standard port other than 53.</p>","path":["Networking","Vlan","DHCPv4 with DNS registration"],"tags":[]},{"location":"networking/vlan/vlandhcp/#dns-configuration","level":2,"title":"DNS Configuration","text":"<ul> <li>Go to Services ‣ Dnsmasq DNS &amp; DHCP ‣ General and set:</li> </ul> Option Value Enable <code>X</code> Listen Port <code>53053</code> <ul> <li>Press Apply</li> </ul> <p>Afterwards we can configure Unbound to forward the zones to Dnsmasq.</p> <ul> <li>Go to Services ‣ Unbound DNS ‣ General and set:</li> </ul> Option Value Enable <code>X</code> Listen Port <code>53</code> <ul> <li> <p>Press Apply</p> </li> <li> <p>Go to Services ‣ Unbound DNS ‣ Query Forwarding and create an entry for each DHCP range you plan to configure.</p> </li> </ul> <p>In our example, we configure query forwarding for 2 networks:</p> <ul> <li> <p><code>lan.internal</code> - <code>192.168.1.0/24</code></p> </li> <li> <p><code>guest.internal</code> - <code>192.168.10.0/24</code> </p> </li> </ul> <p>lan.internalguest.internal</p> Option Value Domain <code>lan.internal</code> Server IP <code>127.0.0.1</code> Server Port <code>53053</code> <ul> <li>Press Save and add next</li> </ul> Option Value Domain <code>1.168.192.in-addr.arpa</code> Server IP <code>127.0.0.1</code> Server Port <code>53053</code> <ul> <li>Press Save and Apply</li> </ul> <p>Note</p> <p>The first entry is for the forward lookup (A-Record), the second for the reverse lookup (PTR-Record).</p> <p>Tip</p> <p>If all PTR records for 192.168.0.0/16 should be handled by Dnsmasq, creating a single entry with <code>168.192.in-addr.arpa</code> is enough.</p> Option Value Domain <code>guest.internal</code> Server IP <code>127.0.0.1</code> Server Port <code>53053</code> <ul> <li>Press Save and add next</li> </ul> Option Value Domain <code>10.168.192.in-addr.arpa</code> Server IP <code>127.0.0.1</code> Server Port <code>53053</code> <ul> <li>Press Save and Apply</li> </ul> <p>Note</p> <p><code>.internal</code> is the IANA and ICANN approved TLD (Top Level Domain) for internal use. If you instead own a TLD, e.g., <code>example.com</code>, you could create a zone that is not used on the internet, e.g., <code>lan.internal.example.com</code>.</p>","path":["Networking","Vlan","DHCPv4 with DNS registration"],"tags":[]},{"location":"networking/vlan/vlandhcp/#dhcp-configuration","level":2,"title":"DHCP Configuration","text":"<p>Now that we have the DNS infrastructure set up, we can configure DHCP.</p> <ul> <li>Go to Services ‣ Dnsmasq DNS &amp; DHCP ‣ General and set:</li> </ul> Option Value Interface <code>LAN, GUEST</code> (The network interfaces which will serve DHCP, this registers firewall rules) Do not forward to system defined DNS servers <code>X</code> (Unless Domains are specified in Dnsmasq: Domains, this will disable forwarding behavior) DHCP fqdn <code>X</code> DHCP default domain <code>internal</code> (or leave empty to use this system’s domain) DHCP register firewall rules <code>X</code> <p>Note</p> <p>DHCP fqdn will do two things:</p> <ul> <li> <p>Make sure all devices are registered in DNS with the configured domain name appended, e.g. <code>smartphone.lan.internal</code>. This ensures that <code>smartphone</code> can exist in both <code>lan.internal</code> and <code>guest.internal</code>.</p> </li> <li> <p>Register the DHCP domain name as local, which will make Dnsmasq authoritative for this domain, ensuring <code>NXDOMAIN</code> is returned for devices querying unknown hostnames within this local domain.</p> </li> </ul> <ul> <li>Press Apply</li> </ul> <p>As next step we define the DHCP ranges for our interfaces.</p> <ul> <li>Go to Services ‣ Dnsmasq DNS &amp; DHCP ‣ DHCP ranges and set:</li> </ul> <p>LANGUEST</p> Option Value Interface <code>LAN</code> Start address <code>192.168.1.100</code> End address <code>192.168.1.199</code> Domain <code>lan.internal</code> <ul> <li>Press Save and Apply</li> </ul> <p>Note</p> <p>If a host receives a DHCP lease from this range, and it advertises a hostname, it will be registered under the chosen domain name. E.g., a host named <code>nas01</code> will become <code>nas01.lan.internal</code>. A client can query this FQDN to receive the current IP address.</p> <p>Attention</p> <p>If you plan to use partial IPv6 addresses in ranges with a constructor, enable the advanced mode and set Domain Type to <code>Interface</code>. This will register any subnets on the chosen interface to the selected domain. This is the only way dynamic DNS registration succeeds when the IPv6 prefix is dynamic.</p> Option Value Interface <code>GUEST</code> Start address <code>192.168.10.100</code> End address <code>192.168.10.199</code> Domain <code>guest.internal</code> <ul> <li>Press Save and Apply</li> </ul> <p>Tip</p> <p>Creating a DHCP range will automatically send out common DHCP options to requesting clients, without explicitly configuring them.</p> <p>This is an incomplete overview which highlights some default DHCP options:</p> DHCP Option Default Description router\\[3\\] IPv4 address of the interface that received the DHCP Request. The default gateway the client should use. In this case the OPNsense. dns-server\\[6\\] IPv4 address of the interface that received the DHCP Request. The DNS server the client should use. In this case Unbound on the OPNsense. domain-name\\[15\\] Domain set in a DHCP Range, or the default system domain if none could be matched. The domain name the client should use, to construct short names to FQDNs in DNS lookups client fqdn\\[81\\] A combination of client hostname and domain, the result of the DDNS registration. The full qualified domain name the client should use. <p>Note</p> <p>Only some usecases require setting these options manually, e.g., the IPv4 address of the router and dns-server in high availability setups with CARP.</p> <p>Attention</p> <p>If Dnsmasq does not start, check that ISC-DHCP and KEA DHCP are not active since they will block the bindable ports this DHCP server requires. It is also a good idea to check Services ‣ Dnsmasq DNS &amp; DHCP ‣ Log for the error message.</p> <p>Now that the setup is complete, the following will happen in regards of DHCP and DNS.</p> <ol> <li> <p>A new device (e.g. a smartphone) joins the LAN network and sends a DHCP Discover broadcast.</p> </li> <li> <p>Dnsmasq receives this broadcast on port 67 and responds with a DHCP offer, containing an available IP address and DHCP options for router[3] and dns-server[6].</p> </li> <li> <p>The device sends a DHCP request to request the available IP address, and possibly send its own hostname.</p> </li> <li> <p>Dnsmasq acknowledges the request.</p> </li> </ol> <p>Our smartphone now has the following IP configuration:</p> <ul> <li> <p>IP address: <code>192.168.1.100</code></p> </li> <li> <p>Default Gateway: <code>192.168.1.1</code></p> </li> <li> <p>DNS Server: <code>192.168.1.1</code></p> </li> </ul> <p>At the same time, Dnsmasq registers the DNS hostname of the smartphone (if it exists). Since we configured the FQDN option and domain in the DHCP range, the name of the smartphone will be: <code>smartphone.lan.internal.</code>.</p> <p>When a client queries Unbound for exactly <code>smartphone.lan.internal.</code>, the configured query forwarding sends the request to the DNS server responsible for <code>lan.internal.</code> which is our configured Dnsmasq listening on <code>127.0.0.1:53053</code>. <code>Dnsmasq</code> responds to this query and will resolve the current A record of <code>smartphone.lan.internal.</code> to <code>192.168.1.100</code>, sending this information to Unbound which in return sends the response back to the client that initially queried.</p> <p>Tip</p> <p>You can usually resolve a hostname in your network by querying for e.g. <code>smartphone</code>. This works because client systems recognize that a FQDN is not used, and will therefore suffix the request with their domain name received from Dnsmasq, transforming the query to <code>smartphone.lan.internal.</code>.</p> <p>As you can see, this is a highly integrated and simple setup which leverages just the available DHCP and DNS standards with no trickery involved.</p>","path":["Networking","Vlan","DHCPv4 with DNS registration"],"tags":[]},{"location":"networking/vlan/vlandhcp/#adguard-home-unbound","level":2,"title":"Adguard Home + Unbound","text":"<p>Using Adguard Home can be used to filter forwarded DNS requests, while Unbound + DNSmasq will forward using DNS over TLS as an upstream and resolve local IPs. These are useful to be used alongside eachother. The flow would be: </p> <p></p>","path":["Networking","Vlan","DHCPv4 with DNS registration"],"tags":[]},{"location":"networking/vlan/vlandhcp/#setup","level":3,"title":"Setup","text":"<p>To allow this, you need to set Adguard home as the Primary DNS under Services &gt; Adguard Home &gt; General</p> <ul> <li> <p>This will now set Adguard Home as primary DNS, running on port 53 </p> </li> <li> <p>Since Adguard Home will run on port 53, you need to change Unbound to run on a different port, I used 5335 </p> <ul> <li>Do not use 5353 - this is a reserved port for mDNS.</li> </ul> </li> <li> <p>Go to the Adguard WebUI at opnsense.internal:3000</p> </li> <li> <p>under Settings &gt; DNS Settings:</p> <ul> <li>Set you upstream DNS to <code>opnsense.internal:5335</code></li> <li>Set Private reverse DNS servers to <code>opnsense.internal:5335</code> </li> </ul> </li> </ul>","path":["Networking","Vlan","DHCPv4 with DNS registration"],"tags":[]},{"location":"services/backups/backrest/","level":1,"title":"Backrest","text":"<p>Github </p> <p>Backrest Docs  </p> <p>Backrest is a web-accessible backup solution built on top of restic. Backrest provides a WebUI which wraps the restic CLI and makes it easy to create repos, browse snapshots, and restore files. Additionally, Backrest can run in the background and take an opinionated approach to scheduling snapshots and orchestrating repo health operations.</p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#installation","level":2,"title":"Installation","text":"","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#running-with-docker-compose","level":3,"title":"Running with Docker Compose","text":"<p>Docker image: <code>ghcr.io/garethgeorge/backrest</code></p> <p>Example compose file:</p> <pre><code>### --- Backrest - Restic --- ### \n# https://github.com/garethgeorge/backrest\nservices:\n  backrest:\n    image: ghcr.io/garethgeorge/backrest:latest\n    container_name: backrest\n    hostname: backrest\n    volumes:\n      # Appdata mounts for backrest \n      - ./backrest/data:/data\n      - ./backrest/config:/config\n      - ./backrest/cache:/cache\n      - ./backrest/tmp:/tmp\n      - ./backrest/rclone:/root/.config/rclone # Mount for rclone config (needed when using rclone remotes)\n      - ./backrest/ssh:/root/.ssh:ro # Mount SSH directory (needed when using SSH remotes)\n      # Mounts for backup data - these can be anything needed to backup\n      - /opt/mkdocs:/mkdocs\n      - /opt/docker:/docker \n      - /mnt/appdata/backup:/repos     # Mount local repos (optional for remote storage)\n    environment:\n      - BACKREST_DATA=/data # appdata for running backrest\n      - BACKREST_CONFIG=/config/config.json \n      - XDG_CACHE_HOME=/cache\n      - TMPDIR=/tmp\n      - TZ=America/Chicago \n    ports:\n      - \"9898:9898\"\n    restart: unless-stopped\n</code></pre>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#environment-variables-unix","level":3,"title":"Environment Variables (Unix)","text":"Variable Description Default <code>BACKREST_PORT</code> Port to bind to 127.0.0.1:9898 (or 0.0.0.0:9898 for the docker images) <code>BACKREST_CONFIG</code> Path to config file <code>$HOME/.config/backrest/config.json</code> (or, if <code>$XDG_CONFIG_HOME</code> is set, <code>$XDG_CONFIG_HOME/backrest/config.json</code>) <code>BACKREST_DATA</code> Path to the data directory <code>$HOME/.local/share/backrest</code> (or, if <code>$XDG_DATA_HOME</code> is set, <code>$XDG_DATA_HOME/backrest</code>) <code>BACKREST_RESTIC_COMMAND</code> Path to restic binary Defaults to a Backrest managed version of restic at <code>$XDG_DATA_HOME/backrest/restic-x.x.x</code> <code>XDG_CACHE_HOME</code> Path to the cache directory","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#initial-setup","level":2,"title":"Initial Setup","text":"<p>Note</p> <p>After installation, access Backrest at <code>http://localhost:9898</code> (or your configured port). You'll need to complete the initial setup process below.</p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#1-instance-configuration","level":3,"title":"1. Instance Configuration","text":"","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#instance-id","level":4,"title":"Instance ID","text":"<ul> <li>A unique identifier for your Backrest installation</li> <li>Used to distinguish snapshots from different Backrest instances</li> <li>Important: Cannot be changed after initial setup</li> </ul>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#authentication","level":4,"title":"Authentication","text":"<ul> <li>Set your username and password during first launch</li> <li>To reset credentials: delete the <code>\"users\"</code> key from:<ul> <li>Linux/macOS: <code>~/.config/backrest/config.json</code></li> <li>Windows: <code>%appdata%\\backrest\\config.json</code></li> </ul> </li> <li>Authentication can be disabled for local installations or when using an authenticating reverse proxy</li> </ul>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#2-repository-setup","level":3,"title":"2. Repository Setup","text":"<p>Click \"Add Repo\" to configure your backup storage location. You can either create a new repository or connect to an existing one.</p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#essential-repository-settings","level":4,"title":"Essential Repository Settings","text":"<ol> <li>Repository Name<ul> <li>A human-readable identifier</li> <li>Cannot be changed after creation</li> </ul> </li> <li>Repository URI<ul> <li>Specifies the backup storage location</li> <li>Common formats:<ul> <li>Backblaze B2: <code>b2:bucket</code> or <code>b2:bucket/prefix</code></li> <li>AWS S3: <code>s3:bucket</code> or <code>s3:bucket/prefix</code></li> <li>Google Cloud: <code>gs:bucket:/</code> or <code>gs:bucket:/prefix</code></li> <li>SFTP: <code>sftp:user@host:/path/to/repo</code></li> <li>Local: <code>/mnt/backupdisk/repo1</code></li> <li>Rclone: <code>rclone:remote:path</code> (requires rclone installation)</li> </ul> </li> </ul> </li> <li>Environment Variables Storage provider credentials:<ul> <li>S3: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></li> <li>B2: <code>B2_ACCOUNT_ID</code>, <code>B2_ACCOUNT_KEY</code></li> <li>Google Cloud: <code>GOOGLE_PROJECT_ID</code>, <code>GOOGLE_APPLICATION_CREDENTIALS</code></li> </ul> </li> <li>Optional Flags Common examples:<ul> <li>SFTP key: <code>-o sftp.args=\"-i /path/to/key\"</code></li> <li>Disable locking: <code>--no-lock</code></li> <li>Bandwidth limits: <code>--limit-upload 1000</code>, <code>--limit-download 1000</code></li> </ul> </li> <li>Maintenance Policies<ul> <li>Prune Policy: Schedule for cleaning unreferenced data</li> <li>Check Policy: Schedule for backup integrity verification</li> </ul> </li> </ol> <p>After adding a repository, use \"Index Snapshots\" to import existing backups. Continue to the next section to set up your backup plan.</p> <p></p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#3-backup-plan-configuration","level":3,"title":"3. Backup Plan Configuration","text":"<p>Create a backup plan by clicking \"Add Plan\" and configuring these settings:</p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrest/#plan-settings","level":4,"title":"Plan Settings","text":"<ol> <li>Plan Name<ul> <li>Choose a descriptive, immutable name</li> <li>Recommended format: <code>[storage]-[content]</code> (e.g., <code>b2-documents</code>)</li> </ul> </li> <li>Repository<ul> <li>Select your target repository </li> <li>Cannot be changed after creation</li> </ul> </li> </ol> <p>Note</p> <p>Ensure to copy SSH keys to/from the remote repos if using SFTP. Location for the container is referenced in the compose file above. You can also call another key location using the flag <code>-o sftp.args=\"-i /path/to/key\"</code></p> <ol> <li>Backup Configuration<ul> <li>Paths: Directories/files to backup</li> <li>Excludes: Patterns or paths to skip (e.g., <code>*node_modules*</code>)</li> </ul> </li> <li>Schedule Choose one:<ul> <li>Hourly/daily intervals</li> <li>Cron expression (e.g., <code>0 0 * * *</code> for daily midnight backups)</li> <li>Clock options:<ul> <li>UTC/Local: Wall-clock time</li> <li>Last Run Time: Relative to previous execution</li> </ul> </li> </ul> </li> <li>Retention Policy Controls snapshot lifecycle:<ul> <li>Count-based: Keep N most recent snapshots</li> <li>Time-based: Keep snapshots by age (e.g., daily for 7 days, weekly for 4 weeks)</li> <li>None: Manual retention management</li> </ul> </li> </ol> <p>Success</p> <p>Success! Now that Backrest is configured, you can sit back and let it manage your backups. Monitor the status of your backups in the UI and restore files from snapshots as needed.</p> <p>Tip</p> <p>Make sure to save a copy of your repository credentials and encryption keys (e.g., password) in a safe place. Losing these will prevent you from restoring your data. Consider storing your entire Backrest configuration (typically <code>~/.config/backrest/config.json</code>) in a secure location, such as a password manager or encrypted storage.</p> <p></p> <p>SSH ISSUE</p>","path":["Services","Backups","Backrest"],"tags":[]},{"location":"services/backups/backrestssh/","level":1,"title":"Backrest - Using SSH (SFTP) Remotes with Docker Compose","text":"","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-1-create-a-local-directory-for-ssh-config","level":3,"title":"Step 1: Create a Local Directory for SSH Config","text":"<p>First, create a directory to store your SSH key and configuration files. This keeps your Backrest-related files organized.</p> <pre><code>mkdir -p ./backrest/ssh\n</code></pre>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-2-generate-an-ssh-key","level":3,"title":"Step 2: Generate an SSH Key","text":"<p>Next, generate a new SSH key pair specifically for Backrest.</p> <pre><code>ssh-keygen -t ed25519 -f ./backrest/ssh/id_rsa -C \"backrest-backup-key\"\n</code></pre> <p>When prompted for a passphrase, you can leave it empty by pressing Enter. Using a passphrase adds another layer of security but requires more complex setup to use with an automated tool like Backrest.</p>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-3-copy-the-public-key-to-your-remote-server","level":3,"title":"Step 3: Copy the Public Key to Your Remote Server","text":"<p>Copy the public key to your remote server's <code>authorized_keys</code> file. The <code>ssh-copy-id</code> command is the easiest way to do this.</p> <pre><code># Replace your-username and example.com with your remote server's details\nssh-copy-id -i ./backrest/ssh/id_rsa.pub your-username@example.com\n</code></pre>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-4-create-the-ssh-config-and-known-hosts-files","level":3,"title":"Step 4: Create the SSH Config and Known Hosts Files","text":"<p>Create an SSH configuration file that Restic (inside the container) will use to connect.</p> <pre><code># Create the config file\ncat &gt; ./backrest/ssh/config &lt;&lt; EOF\nHost backrest-remote\n    HostName example.com\n    User your-username\n    IdentityFile /root/.ssh/id_rsa\n    Port 22\nEOF\n\n# Add the server's fingerprint to known_hosts\nssh-keyscan -H example.com &gt;&gt; ./backrest/ssh/known_hosts\n</code></pre> <p>Copy to clipboard</p> <p>Important:</p> <ul> <li><code>Host backrest-remote</code>: This is a custom alias. You will use this name in the Backrest UI.</li> <li><code>HostName</code>: The actual IP address or hostname of your remote server.</li> <li><code>User</code>: The username on the remote server.</li> <li><code>IdentityFile</code>: This must be <code>/root/.ssh/id_rsa</code>. This is the path inside the container where the key will be mounted.</li> <li><code>Port</code>: The SSH port of your remote server.</li> </ul>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-5-set-secure-permissions","level":3,"title":"Step 5: Set Secure Permissions","text":"<p>SSH requires that key and configuration files have strict permissions. These permissions need to be set as root. To ensure this is read properly via backrest, you can run them via the container itself </p> <p>Fix Ownership: Make root the owner of the files inside the container.</p> <p><pre><code>docker exec -u 0 backrest chown -R root:root /root/.ssh\n</code></pre> Fix Directory Permissions: Ensure the folder is only accessible by the owner.</p> <p><pre><code>docker exec -u 0 backrest chmod 700 /root/.ssh\n</code></pre> Fix File Permissions: Ensure the config and keys are read-only for the owner.</p> <pre><code>docker exec -u 0 backrest chmod 600 /root/.ssh/config /root/.ssh/id_rsa\n</code></pre>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-6-mount-the-ssh-directory-in-docker-compose","level":3,"title":"Step 6: Mount the SSH Directory in Docker Compose","text":"<p>Now, edit your <code>docker-compose.yml</code> to mount the <code>backrest/ssh</code> directory into the container. We mount it as read-only (<code>:ro</code>) for better security.</p> <pre><code>version: \"3.8\"\nservices:\n  backrest:\n    image: garethgeorge/backrest:latest\n    container_name: backrest\n    # ... other configuration ...\n    volumes:\n      - ./backrest/data:/data\n      - ./backrest/config:/config\n      - ./backrest/cache:/cache\n      # ... other volumes ...\n      - ./backrest/ssh:/root/.ssh:ro # Add this line\n      - ./backrest/ssh:/.ssh:ro # Add this line if running rootless\n    # ... rest of configuration ...\n</code></pre> <p>After saving the file, restart your container for the changes to take effect:</p> <pre><code>docker compose up -d --force-recreate\n</code></pre>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#step-7-add-the-repository-in-backrest","level":3,"title":"Step 7: Add the Repository in Backrest","text":"<p>Important</p> <p>Ensure your user has write permissions to the destination repo. </p> <ol> <li>In the Backrest WebUI, navigate to Repositories and click Add Repository.</li> <li>For the Type, select Remote/Cloud.</li> <li>For the URL, enter <code>sftp:backrest-remote:/path/to/your/repo</code>.<ul> <li>Replace <code>backrest-remote</code> with the <code>Host</code> alias you defined in <code>backrest/ssh/config</code>.</li> <li>Replace <code>/path/to/your/repo</code> with the absolute path on the remote server where you want to store backups.</li> </ul> </li> <li>Enter a secure Password to encrypt your backup data. This is a new password for the repository itself, not your SSH key password.</li> <li>Click Initialize Repository.</li> </ol>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/backrestssh/#troubleshooting","level":2,"title":"Troubleshooting","text":"<ul> <li>Connection Errors: First, test your SSH connection from the host machine to isolate issues. This command uses the exact same configuration files that the container will use. Running this via the container itself verifies access. Official docs have this run locally, which did not work for me. </li> </ul> <pre><code>docker exec -it backrest ssh -F /root/.ssh/config backrest-remote\n</code></pre> <ul> <li>Permission Denied:<ul> <li>Double-check the file permissions set in Step 5.</li> <li>Ensure the user on the remote server has write permissions to the repository path.</li> </ul> </li> <li>Check Logs: Review the Backrest application logs for detailed error messages from Restic.</li> </ul>","path":["Services","Backups","Backrest - Using SSH (SFTP) Remotes with Docker Compose"],"tags":[]},{"location":"services/backups/restic/","level":1,"title":"Backrest - Restic with WebUI","text":"","path":["Services","Backups","Backrest - Restic with WebUI"],"tags":[]},{"location":"services/backups/restic/#official-docs-simple-book","level":2,"title":"Official docs :simple-book:","text":"","path":["Services","Backups","Backrest - Restic with WebUI"],"tags":[]},{"location":"services/git/","level":1,"title":"Index","text":"","path":["Services","Git","Index"],"tags":[]},{"location":"services/git/#icon-simplegit","level":2,"title":"icon: simple/git","text":"","path":["Services","Git","Index"],"tags":[]},{"location":"services/git/git/","level":1,"title":"Git Reference &amp; Version Control","text":"","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#overview","level":2,"title":"Overview","text":"<p>Git is a distributed version control system used to track changes in source code. In a homelab context, it serves as the backbone of \"Infrastructure as Code\" (IaC). By storing <code>docker-compose.yml</code> and configuration files in Git, the entire server state is backed up, versioned, and easily reproducible.</p> <p>This guide covers the initialization of a local repository, security practices for managing secrets, and daily workflows using both the Command Line Interface (CLI) and VS Code.</p> <p>I don't know much about the other stuff in git. I just use the basics, but this gets me by. </p>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#1-initialization-one-time-setup","level":2,"title":"1. Initialization (One-Time Setup)","text":"<p>Perform these steps when setting up the machine for the first time to establish the repository.</p>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#11-installation-identity","level":3,"title":"1.1. Installation &amp; Identity","text":"<p>First, ensure Git is installed and configure the user identity. This information is embedded in every commit.</p> <pre><code># Install Git\nsudo apt update &amp;&amp; sudo apt install git -y\n\n# Configure Global Identity\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\n# Set default branch to 'main' (Modern Standard)\ngit config --global init.defaultBranch main \n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#12-initialize-repo","level":3,"title":"1.2. Initialize Repo","text":"<p>Turn the Docker directory into a tracked repository.</p> <pre><code>cd ~/homelab/docker/\ngit init\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#13-the-ignore-rules-critical","level":3,"title":"1.3. The Ignore Rules (Critical)","text":"<p>A <code>.gitignore</code> file prevents sensitive data (secrets) and system clutter from being uploaded to the remote server.</p> <p>File Location: <code>~/homelab/docker/.gitignore</code></p> <pre><code># --- Secrets (NEVER COMMIT) ---\n**/.env\n**/.env.*\n**/secrets/\nid_rsa\n*.pem\n\n# --- System Junk ---\n.DS_Store\nThumbs.db\n**/.git/   # Ignore nested git repos (prevents submodule errors)\n\n# --- Docker Data (If mapped locally) ---\n**/data/\n**/mysql/\n**/influxdb/ \n\n# --- Custom Ignores ---\n**/.pastebin\n**/.pastebin/\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#14-connect-to-remote-github","level":3,"title":"1.4. Connect to Remote (GitHub)","text":"<p>Link the local folder to a GitHub repository for off-site backup.</p> <pre><code># 1. Link Remote\ngit remote add origin https://github.com/YourUser/homelab-backup.git\n\n# 2. First Push\ngit add .\ngit commit -m \"Initial homelab commit\"\ngit push -u origin main\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#2-daily-workflows-cli","level":2,"title":"2. Daily Workflows (CLI)","text":"","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#21-the-config-change-routine","level":3,"title":"2.1. The \"Config Change\" Routine","text":"<p>Execute this routine whenever a <code>compose.yaml</code> or configuration file is edited.</p> <ol> <li> <p>Check Status: <pre><code>git status\n</code></pre></p> </li> <li> <p>Verify: Ensure no <code>.env</code> files appear in the untracked list (Red).</p> </li> <li> <p>Stage &amp; Commit: <pre><code>git add .\ngit commit -m \"Added Tautulli to media stack\"\n</code></pre></p> </li> <li> <p>Push: <pre><code>git push\n</code></pre></p> </li> </ol>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#22-disaster-recovery-rebuild","level":3,"title":"2.2. Disaster Recovery (Rebuild)","text":"<p>If the host OS drive fails, use this workflow to restore the stack on a fresh installation.</p> <ol> <li> <p>Clone Configs: <pre><code>git clone https://github.com/YourUser/homelab-backup.git ~/homelab/docker\n</code></pre></p> </li> <li> <p>Restore Secrets (Manual): Since <code>.env</code> files are ignored for security, they must be manually recreated from a password manager (e.g., Bitwarden). <pre><code>cd ~/homelab/docker/core\nnano .env \n# Paste API keys/passwords here\n</code></pre></p> </li> <li> <p>Launch: <pre><code>docker compose up -d\n</code></pre></p> </li> </ol>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#3-security-secret-detection","level":2,"title":"3. Security: Secret Detection","text":"<p>Objective: Prevent accidental leakage of passwords, API keys, or <code>.env</code> contents into the public Git history.</p>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#31-visual-inspection","level":3,"title":"3.1. Visual Inspection","text":"<p>Before staging files, review changes to ensure no hardcoded credentials were added.</p> Command Description <code>git diff</code> Shows line-by-line changes for unstaged files. <code>git diff --staged</code> Shows changes for files already staged but not committed.","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#32-automated-scanning-gitleaks-via-docker","level":3,"title":"3.2. Automated Scanning (Gitleaks via Docker)","text":"<p>Use Gitleaks to scan the directory for high-entropy strings (random characters that look like API keys) without installing new software on the host.</p> <pre><code># Run inside ~/homelab/docker\ndocker run --rm \\\n  -v $(pwd):/path \\\n  zricethezav/gitleaks:latest \\\n  detect --source=\"/path\" -v\n</code></pre> <ul> <li>Success: \"No leaks found.\"</li> <li>Failure: Output lists the specific secret and file location.</li> </ul>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#4-vs-code-integration","level":2,"title":"4. VS Code Integration","text":"","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#41-visualizing-diffs","level":3,"title":"4.1. Visualizing Diffs","text":"<p>To view diffs directly in VS Code from the terminal:</p> <p>One-off:</p> <pre><code>git diff | code -\n</code></pre> <p>Permanent Configuration:</p> <pre><code>git config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\n# Usage:\ngit difftool\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#42-file-explorer-indicators","level":3,"title":"4.2. File Explorer Indicators","text":"<p>Understanding the colored badges in the VS Code file explorer.</p> Badge Color (Gruvbox) Meaning Action Needed U Green Untracked File is new. Needs <code>git add</code>. M Blue Modified File is edited but not staged. A Green Added File is staged and ready to commit. D Red Deleted File removed locally. - Gray Ignored Matches <code>.gitignore</code> rule.","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#43-gui-workflow","level":3,"title":"4.3. GUI Workflow","text":"<p>The \"Happy Path\" using the Source Control Tab (<code>Ctrl+Shift+G</code>):</p> <ol> <li>Edit &amp; Save: Make changes and save (<code>Ctrl+S</code>).</li> <li>Review: Click the file under Changes to see the Side-by-Side Diff.</li> <li>Stage: Click the <code>+</code> (Plus) icon next to the file.</li> <li>Commit: Type a message in the input box  Click Commit.</li> <li>Sync: Click Sync Changes (Auto Pulls &amp; Pushes).</li> </ol>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#5-troubleshooting","level":2,"title":"5. Troubleshooting","text":"","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#51-submodule-error-gray-folder-on-github","level":3,"title":"5.1. \"Submodule\" Error (Gray Folder on GitHub)","text":"<p>Symptom: A project (like a theme) was <code>git clone</code>d inside the main repo. GitHub sees the nested <code>.git</code> folder and treats it as a dead link (submodule).</p> <p>Fix:</p> <pre><code># 1. Remove the nested .git folder\nrm -rf path/to/subfolder/.git\n\n# 2. Remove the folder from Git's index (cache) only\ngit rm --cached path/to/subfolder\n\n# 3. Re-add the folder as standard files\ngit add .\ngit commit -m \"Converted submodule to standard files\"\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#52-credentials-issues-https","level":3,"title":"5.2. Credentials Issues (HTTPS)","text":"<p>Symptom: Git prompts for a username/password on every push.</p> <p>Fix (Cache Credentials):</p> <pre><code># Cache credentials in memory for 1 hour (3600 seconds)\ngit config --global credential.helper 'cache --timeout=3600'\n</code></pre>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#6-command-cheat-sheet","level":2,"title":"6. Command Cheat Sheet","text":"","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#basic-operations","level":3,"title":"Basic Operations","text":"Action Command Check State <code>git status</code> (Always run this first) View Changes <code>git diff</code> Stage Files <code>git add .</code> (Stage all) or <code>git add &lt;filename&gt;</code> Commit <code>git commit -m \"Your message here\"</code> Push <code>git push origin main</code> Pull Updates <code>git pull</code> Undo File Edit <code>git restore &lt;file&gt;</code> (Discards local changes) View History <code>git log --oneline --graph --all</code>","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/git/git/#emergency-fixes","level":3,"title":"Emergency Fixes","text":"Scenario Workflow / Command Typos in last commit <code>git add .</code> <code>git commit --amend -m \"New Message\"</code> <code>git push -f</code> Leaked Secret <code>git rm --cached .env</code> <code>echo \".env\" &gt;&gt; .gitignore</code> <code>git commit -m \"rm secret\"</code> Reset to Remote <code>git fetch origin</code> <code>git reset --hard origin/main</code> (Destructive: makes local match cloud)","path":["Services","Git","Git Reference &amp; Version Control"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/","level":1,"title":"Home Assistant &amp; Zigbee Stack","text":"<p>Home Assistant Github  </p> <p>Z2MQTT Github </p> <p>Eclipse Mosquitto Github </p>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#overview","level":2,"title":"Overview","text":"<p>This integrates Home Assistant, Zigbee2MQTT (Z2M), and an MQTT Broker (Mosquitto) to create a local smarthome. </p> <p>I am specifically configuring this for the Sonoff ZBDongle-MG24 (Model \"E\" or \"MG24\"), which runs on the Silicon Labs EFR32MG24 chip. This requires the specific <code>ember</code> driver in Zigbee2MQTT, unlike the older Texas Instruments-based dongles.</p> <p>The stack runs via Docker Compose, with Home Assistant running in <code>host</code> networking mode for optimal device discovery.</p> <p>I'm a big fan of the zigbee system as each device acts as a repeater for the other devices. Also, working of seperate protocol, your devices will continue working even without WiFi.</p>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#installation","level":2,"title":"Installation","text":"","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#1-hardware-preparation","level":3,"title":"1. Hardware Preparation","text":"<p>Identify the Dongle The Sonoff MG24 uses a different driver protocol than older models. Before configuring software, ensure the dongle is recognized.</p> <ol> <li>Plug the dongle into a USB 2.0 port (or use a USB 2.0 extension cable to avoid USB 3.0 interference).</li> <li> <p>Run the following to identify your specific device ID: <pre><code>ls -l /dev/serial/by-id/\n</code></pre></p> </li> <li> <p>Copy the output string. It usually looks like <code>usb-ITEAD_SONOFF_Zigbee_...</code> or similar. You will need this for your Compose file.</p> </li> </ol>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#2-directory-structure","level":3,"title":"2. Directory Structure","text":"<p>Create a persistent directory structure to store your configurations and data.</p> <pre><code>mkdir -p ~/homelab/smarthome/{homeassistant,zigbee2mqtt/data,mosquitto/config,mosquitto/data,mosquitto/log}\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#3-docker-compose","level":3,"title":"3. Docker Compose","text":"<p>Create your <code>compose.yaml</code> file in <code>~/homelab/smarthome/</code>.</p> <p>Port Mapping</p> <p>This configuration maps the Zigbee2MQTT frontend to port 8082 to avoid conflicts, and maps the USB device. Ensure the <code>devices</code> section matches the ID you found in Step 1.</p> <pre><code>### --- Smart Home Stack: MQTT Broker, Zigbee2MQTT, Home Assistant --- ###\n\nservices:\n  # MQTT Broker\n  mosquitto:\n    container_name: mosquitto\n    image: eclipse-mosquitto\n    restart: unless-stopped\n    ports:\n      - \"1883:1883\"\n      - \"9001:9001\"\n    volumes:\n      - ./mosquitto/config:/mosquitto/config\n      - ./mosquitto/data:/mosquitto/data\n      - ./mosquitto/log:/mosquitto/log\n\n  # Zigbee2MQTT\n  zigbee2mqtt:\n    container_name: zigbee2mqtt\n    image: koenkk/zigbee2mqtt\n    restart: unless-stopped\n    depends_on:\n      - mosquitto\n    volumes:\n      - ./zigbee2mqtt/data:/app/data\n      - /run/udev:/run/udev:ro\n    environment:\n      - TZ=America/Chicago # Update to your timezone\n    devices:\n      # Map the USB dongle directly. UPDATE THIS to match your 'ls -l' output\n      - /dev/serial/by-id/usb-youthings_Zigbee_Adapter-if00:/dev/ttyACM0\n    ports:\n      - \"8082:8080\" # Web Interface\n\n  # Home Assistant \n  homeassistant:\n    container_name: homeassistant\n    image: lscr.io/linuxserver/homeassistant:latest\n    network_mode: host # Critical for Home Assistant discovery\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=America/Chicago\n    volumes:\n      - ./homeassistant:/config\n    restart: unless-stopped\n    depends_on:\n      - mosquitto\n      - zigbee2mqtt\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#configuration","level":2,"title":"Configuration","text":"<p>You must create the configuration files for Mosquitto and Zigbee2MQTT before starting the containers, or they will likely crash on boot.</p>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#1-mosquitto-broker","level":3,"title":"1. Mosquitto Broker","text":"<p>Create the file: <code>~/homelab/smarthome/mosquitto/config/mosquitto.conf</code></p> <pre><code>persistence true\npersistence_location /mosquitto/data/\nlog_dest file /mosquitto/log/mosquitto.log\n\n# Listen on all interfaces\nlistener 1883\n\n# Allow anonymous for internal local network setup\nallow_anonymous true\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#2-zigbee2mqtt","level":3,"title":"2. Zigbee2MQTT","text":"<p>Create the file: <code>~/homelab/smarthome/zigbee2mqtt/data/configuration.yaml</code></p> <p>Driver Selection</p> <p>The <code>adapter: ember</code> setting is critical for the Sonoff MG24. If you omit this, Z2M will try to use the default driver and fail to start.</p> <pre><code># Home Assistant Integration\nhomeassistant: true\n\n# MQTT Settings\nmqtt:\n  base_topic: zigbee2mqtt\n  server: 'mqtt://mosquitto:1883'\n\n# Serial Settings for Sonoff MG24 (Ember Driver)\nserial:\n  # This path inside the container is mapped from your host in the compose file\n  port: /dev/ttyACM0 \n  adapter: ember\n\n# Frontend Settings (GUI)\nfrontend:\n  port: 8080\n\n# Zigbee Network\npermit_join: false\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#setup-device-pairing","level":2,"title":"Setup &amp; Device Pairing","text":"<p>Once the files are created, launch the stack:</p> <pre><code>docker compose up -d\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#pairing-devices-example-thirdreality-zl1-bulb","level":3,"title":"Pairing Devices (Example: ThirdReality ZL1 Bulb)","text":"<ol> <li>Access Z2M: Open <code>http://&lt;server-ip&gt;:8082</code>.</li> <li>Permit Join: Click Permit Join (All) in the top bar to allow new devices.</li> <li>Reset Bulb:</li> <li>Screw in the ThirdReality ZL1 bulb.</li> <li>Power Cycle 5 Times: Turn the switch OFF and ON 5 times quickly (approx. 1 second per toggle).</li> <li>Sequence: ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON.</li> <li> <p>Confirmation: The bulb will flash a sequence of colors (Warm White -&gt; Cool White -&gt; Red -&gt; Green -&gt; Blue) and stay solid Warm White.</p> </li> <li> <p>Rename: Once it appears in the Z2M dashboard, click the blue \"Edit\" icon. Give it a friendly name (e.g., \"Office_Lamp\") and ensure \"Update Home Assistant entity ID\" is checked.</p> </li> </ol>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#connecting-to-home-assistant","level":3,"title":"Connecting to Home Assistant","text":"<ol> <li>Access HA: Open <code>http://&lt;server-ip&gt;:8123</code>.</li> <li>Integrations: Go to Settings &gt; Devices &amp; Services.</li> <li>Discovery: HA should auto-discover \"MQTT\". If not, click Add Integration &gt; MQTT.</li> <li>Broker Settings:</li> <li>Broker: <code>localhost</code> (Since HA is on the host network).</li> <li>Port: <code>1883</code>.</li> <li> <p>Auth: Leave blank (I enabled <code>allow_anonymous</code>).</p> </li> <li> <p>Verify: Your \"Office_Lamp\" should now appear as a device in Home Assistant.</p> </li> </ol>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#maintenance","level":2,"title":"Maintenance","text":"","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#updates","level":3,"title":"Updates","text":"<p>To update the stack, pull the latest images and recreate the containers. Your data is safe in the persistent volumes.</p> <pre><code>docker compose pull\ndocker compose up -d\n</code></pre>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/homeassisstant/homeassisstant/#troubleshooting","level":3,"title":"Troubleshooting","text":"<p>If Zigbee2MQTT fails to start, check the logs immediately:</p> <pre><code>docker compose logs zigbee2mqtt\n</code></pre> <ul> <li>Error: \"Error: Error: No such file or directory...\": Check your USB mapping in <code>compose.yaml</code>. The ID <code>usb-youthings...</code> or <code>usb-ITEAD...</code> must match exactly what <code>ls -l /dev/serial/by-id/</code> shows.</li> <li>Error: \"Adapter failed to start\": Ensure <code>adapter: ember</code> is in your <code>configuration.yaml</code>.</li> </ul>","path":["Services","Homeassisstant","Home Assistant &amp; Zigbee Stack"],"tags":[]},{"location":"services/immich/immich/","level":1,"title":"Immich Photo Backup","text":"","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#overview","level":2,"title":"Overview","text":"<p>Immich is a self-hosted, high-performance photo and video backup solution that aims to directly replace Google Photos. Unlike traditional file managers, Immich relies heavily on machine learning to provide features like facial recognition, object detection (e.g., \"search for 'cat'\"), and map-based exploration.</p> <p>The architecture is microservice-based, meaning it runs several distinct containers (Server, Machine Learning, Postgres, Redis) that work in concert to handle massive upload queues and background processing tasks.</p>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#installation","level":2,"title":"Installation","text":"","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#docker-compose","level":3,"title":"Docker Compose","text":"<p>Immich is designed to be run via Docker Compose. While there is a monolithic \"all-in-one\" container available, it is not recommended for production or heavy use. This guide uses the official <code>ghcr.io</code> images.</p>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#prerequisites","level":4,"title":"Prerequisites","text":"<p>Because Immich uses modular configuration files for hardware acceleration, you must download two helper files into your directory before running the compose file.</p> <pre><code>wget [https://github.com/immich-app/immich/releases/latest/download/hwaccel.ml.yml](https://github.com/immich-app/immich/releases/latest/download/hwaccel.ml.yml)\nwget [https://github.com/immich-app/immich/releases/latest/download/hwaccel.transcoding.yml](https://github.com/immich-app/immich/releases/latest/download/hwaccel.transcoding.yml)\n</code></pre>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#compose-configuration","level":4,"title":"Compose Configuration","text":"<p>Create a <code>.env</code> file in the same directory to handle your storage locations and database passwords.</p> <p>Hardware Note</p> <p>This configuration is optimized for an Intel i5-8500T using Intel QuickSync and OpenVINO. If you have a dedicated GPU, you will need to adjust the <code>service: quicksync</code> and <code>service: openvino</code> lines.</p> <pre><code>#\n# WARNING: To install Immich, follow our guide: [https://docs.immich.app/install/docker-compose](https://docs.immich.app/install/docker-compose)\n#\n# Make sure to use the docker-compose.yml of the current release:\n#\n# [https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml](https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml)\n#\n# The compose file on main may not be compatible with the latest release.\n\nname: immich\n\nservices:\n  immich-server:\n    container_name: immich_server\n    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}\n    extends:\n      file: hwaccel.transcoding.yml\n      service: quicksync # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding\n    volumes:\n      # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file\n      - ${UPLOAD_LOCATION}:/data\n      - /etc/localtime:/etc/localtime:ro\n    env_file:\n      - .env\n    ports:\n      - '2283:2283'\n    depends_on:\n      - redis\n      - database\n    restart: always\n    healthcheck:\n      disable: false\n\n  immich-machine-learning:\n    container_name: immich_machine_learning\n    # For hardware acceleration, add one of -[armnn, cuda, rocm, openvino, rknn] to the image tag.\n    # Example tag: ${IMMICH_VERSION:-release}-cuda\n    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}-openvino\n    extends: # uncomment this section for hardware acceleration - see [https://docs.immich.app/features/ml-hardware-acceleration](https://docs.immich.app/features/ml-hardware-acceleration)\n      file: hwaccel.ml.yml\n      service: openvino # set to one of [armnn, cuda, rocm, openvino, openvino-wsl, rknn] for accelerated inference - use the `-wsl` version for WSL2 where applicable\n    volumes:\n      - model-cache:/cache\n    env_file:\n      - .env\n    restart: always\n    healthcheck:\n      disable: false\n\n  redis:\n    container_name: immich_redis\n    image: docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f\n    healthcheck:\n      test: redis-cli ping || exit 1\n    restart: always\n\n  database:\n    container_name: immich_postgres\n    image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USERNAME}\n      POSTGRES_DB: ${DB_DATABASE_NAME}\n      POSTGRES_INITDB_ARGS: '--data-checksums'\n      # Uncomment the DB_STORAGE_TYPE: 'HDD' var if your database isn't stored on SSDs\n      # DB_STORAGE_TYPE: 'HDD'\n    volumes:\n      # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file\n      - ${DB_DATA_LOCATION}:/var/lib/postgresql/data\n    shm_size: 128mb\n    restart: always\n\nvolumes:\n  model-cache:\n</code></pre>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#initial-setup","level":2,"title":"Initial Setup","text":"<p>After ensuring the helper YAML files and <code>.env</code> are present, run <code>docker compose up -d</code>.</p> <p>Navigate to <code>http://your.server.ip:2283</code> to see the \"Getting Started\" wizard.</p> <ol> <li>Click Getting Started.</li> <li>Create Admin Account: Enter your email, name, and password.</li> <li>Log in: You will be taken to the main timeline.</li> </ol>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#hardware-acceleration","level":2,"title":"Hardware Acceleration","text":"","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#video-transcoding-intel-quicksync","level":3,"title":"Video Transcoding (Intel QuickSync)","text":"<p>Why use it: When you view a 4K video on a phone over a slow connection, Immich must transcode (shrink) that video in real-time. Doing this on the CPU will pin your processor to 100% usage and create stuttering. QuickSync offloads this to the iGPU.</p> <p>How to configure: The <code>hwaccel.transcoding.yml</code> in the compose file handles the backend permissions, but you must enable it in the UI.</p> <ol> <li>Go to Administration (top right) -&gt; Settings -&gt; Video Transcoding.</li> <li>Find Hardware Acceleration.</li> <li>Select Quick Sync from the dropdown.</li> <li>Click Save.</li> </ol>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#machine-learning-openvino","level":3,"title":"Machine Learning (OpenVINO)","text":"<p>Why use it: Immich scans every photo for faces and objects. On a raw CPU, scanning 1,000 photos can take hours. OpenVINO optimizes these mathematical operations for Intel CPUs, significantly increasing indexing speed.</p> <p>How to configure: There is no UI toggle for this. It is controlled entirely by the image tag (<code>-openvino</code>) and the <code>extends</code> block in the compose file.</p> <p>Verification: Run the following to check if it loaded correctly:</p> <pre><code>docker compose logs immich-machine-learning | grep \"OpenVINO\"\n</code></pre> <ul> <li>Success: You will see a line stating <code>Loaded execution provider: OpenVINO</code>.</li> <li>Failure: If you only see <code>CPUExecutionProvider</code>, check that your image tag ends in <code>-openvino</code>.</li> </ul>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#database-storage","level":2,"title":"Database &amp; Storage","text":"","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#postgresql-with-pgvector","level":3,"title":"PostgreSQL with pgvector","text":"<p>Immich requires a specific version of Postgres equipped with <code>pgvector</code>. This extension allows the database to store \"vector embeddings\"—mathematical representations of images. This enables smart searches (e.g., searching for \"red truck\" finds trucks without manual tags).</p> <p>Performance Tip</p> <p>Unlike Nextcloud, you rarely touch the DB config manually. However, ensure <code>DB_DATA_LOCATION</code> in your <code>.env</code> points to fast storage (SSD/NVMe). Vector searches on HDDs are noticeably slower.</p>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#storage-template","level":3,"title":"Storage Template","text":"<p>By default, Immich uploads files into a generic folder structure. If you ever want to access your files directly on the disk (outside of Immich), this is difficult to navigate.</p> <p>The Fix: Configure this before uploading your library.</p> <ol> <li>Go to Administration -&gt; Settings -&gt; Storage Template.</li> <li>Enabled: Toggle ON.</li> <li>Template: Select or customize your structure.</li> <li>Recommendation: <code>{{y}}/{{y}}-{{MM}}-{{dd}}/{{filename}}</code></li> <li> <p>Result: <code>/2024/2024-12-25/IMG_1234.jpg</code></p> </li> <li> <p>Click Save.</p> </li> </ol>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#post-install-monitoring","level":2,"title":"Post-Install Monitoring","text":"","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/immich/#gpu-utilization","level":3,"title":"GPU Utilization","text":"<p>Since you are using hardware acceleration, verify the load is hitting the GPU and not the CPU.</p> <p>The Tool: <code>intel_gpu_top</code></p> <ol> <li> <p>Install on Host: <pre><code>sudo apt install intel-gpu-tools\n</code></pre></p> </li> <li> <p>Run Monitor: <pre><code>sudo intel_gpu_top\n</code></pre></p> </li> <li> <p>Test: Open Immich and scroll rapidly through the timeline (triggers thumbnail generation) or play a large video. You should see the <code>Video</code> or <code>Render</code> bars spike.</p> </li> </ol> <p>Sources:</p> <ul> <li>Immich Hardware Acceleration Guide</li> <li>Recommended Compose File</li> </ul>","path":["Services","Immich","Immich Photo Backup"],"tags":[]},{"location":"services/immich/takeoutimport/","level":1,"title":"Importing your photos from Google Takeout","text":"","path":["Services","Immich","Importing your photos from Google Takeout"],"tags":[]},{"location":"services/immich/takeoutimport/#creating-takeout-link","level":2,"title":"Creating takeout link","text":"<ul> <li> <p>Go to https://takeout.google.com/</p> </li> <li> <p>Deselect all, then select Google Photos. Scroll to bottom and select \"next step\"</p> </li> <li> <p>Get the links via email, choose 50GB file size. </p> </li> </ul> <p>Note</p> <p>It may take awhile to generate the link</p>","path":["Services","Immich","Importing your photos from Google Takeout"],"tags":[]},{"location":"services/immich/takeoutimport/#downloading-the-files","level":2,"title":"Downloading the files","text":"","path":["Services","Immich","Importing your photos from Google Takeout"],"tags":[]},{"location":"services/immich/takeoutimport/#standard-method","level":3,"title":"Standard method","text":"<p>Download the links like any other download, save to wherever works, and import to immich using <code>immich-go</code> </p>","path":["Services","Immich","Importing your photos from Google Takeout"],"tags":[]},{"location":"services/immich/takeoutimport/#headless-download","level":3,"title":"Headless download","text":"<p>Note</p> <p>you can only download a link 5 times. After that, you must regenerate the takeout link again.</p> <ul> <li> <p>Go to the generated takeout link. </p> </li> <li> <p>Press <code>F12</code> to open the developer tools of browser, then choose 'network'</p> </li> <li> <p>Start a download on browser, then pause it.</p> </li> <li> <p>Find the download from the domain <code>https://takeout-download.usercontent.google.com</code> - IT MUST BE THIS DOMAIN</p> </li> <li> <p>Right click -&gt; 'Copy as cURL'</p> </li> <li> <p>Create a ssh session using <code>tmux</code> or <code>screen</code> to keep download going if terminal dies.</p> </li> <li> <p>This will make a long string, paste to a text editor and add <code>-O</code> (- Capital O). This will write to file instead of stdout.</p> </li> <li> <p>This will begin download. Then after all <code>.zip</code> files are there, import using <code>immich-go</code></p> </li> </ul>","path":["Services","Immich","Importing your photos from Google Takeout"],"tags":[]},{"location":"services/nextcloud/nextcloud/","level":1,"title":"Nextcloud Storage","text":"<p>NextCloud Github  </p> <p>Nextcloud linuxserverio image  </p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#overview","level":2,"title":"Overview","text":"<p>Nextcloud is a self-hosted, open-source productivity platform that functions as a private alternative to services like Google Drive or Dropbox. It provides file storage, synchronization, and sharing, but can be expanded via \"apps\" to include contacts, calendars, office suites, and more.</p> <p>While the core functionality manages files, the architecture requires a web server, PHP runtime, a database for metadata, and a caching mechanism for performance.</p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#installation","level":2,"title":"Installation","text":"","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#docker-compose","level":3,"title":"Docker Compose","text":"<p>While there are multiple ways to install Nextcloud, this guide uses Docker Compose for easier configuration and maintenance.</p> <p>Image Choice</p> <p>I read a lot of people having issues with the official nextcloud docker image so I used the LinuxServer.io image. It is apparently generally more stable. </p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#compose-configuration","level":4,"title":"Compose Configuration","text":"<p>Create a <code>compose.yaml</code> file. You should also create a <code>.env</code> file in the same directory to store your secrets (passwords and users).</p> <pre><code>services:\n  nextcloud:\n    image: lscr.io/linuxserver/nextcloud:latest\n    container_name: nextcloud\n    environment:\n      - PUID=1000   # Change to your user ID (run 'id' in terminal)\n      - PGID=1000   # Change to your group ID\n      - TZ=America/Chicago\n    volumes:\n      - /mnt/appdata/nextcloud/config:/config # Configuration files (appdata ssd)\n      - /mnt/storage/nextcloud:/data  # This is where your actual files/photos will live\n    ports:\n      # - 443:443 # Default HTTPS port (uncomment if you want to use it)\n      - 4043:443 \n    depends_on:\n      - nextcloud_db\n      - nextcloud_redis\n    restart: unless-stopped\n\n  nextcloud_db:\n    image: postgres:15-alpine\n    container_name: nextcloud_db\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}  # CHANGE THIS in .env!\n      - POSTGRES_DB=${POSTGRES_DB} \n    volumes:\n      - /mnt/appdata/nextcloud/db_data:/var/lib/postgresql/data\n    restart: unless-stopped\n\n  nextcloud_redis:\n    image: redis:alpine\n    container_name: nextcloud_redis\n    restart: unless-stopped\n\nvolumes:\n  db_data:\n</code></pre>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#initial-setup","level":2,"title":"Initial Setup","text":"<p>After creating your <code>compose.yaml</code> and <code>.env</code> files, run <code>docker compose up -d</code> to start the containers.</p> <p>Navigate to <code>https://your.server.ip:4043</code> to access the setup wizard.</p> <ol> <li>Create Admin Account: Enter your desired username and password.</li> <li>Database Configuration: Click \"Storage &amp; Database\" to expand the options.</li> <li>Select Database: Choose PostgreSQL.</li> </ol> <p>Database Hostname</p> <p>When asked for \"Database host\", do not use <code>localhost</code> or an IP. Use the container name: <code>nextcloud_db</code>.</p> <p>Ensure the user, password, and database name match exactly what you defined in your <code>.env</code> file.</p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#database-caching","level":2,"title":"Database &amp; Caching","text":"","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#postgresql","level":3,"title":"PostgreSQL","text":"<p>Why use it: By default, Nextcloud can run on SQLite (file-based). However, SQLite performs poorly with concurrent connections and slows down significantly as your file count grows. PostgreSQL is an enterprise-class database that handles concurrent connections efficiently, providing stability for metadata operations.</p> <p>How to configure: This is handled during the initial setup wizard (see above).</p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#redis","level":3,"title":"Redis","text":"<p>Why use it: Redis is an in-memory data structure store used for: 1.  Memory Caching: Speeds up the web interface by storing frequently accessed data in RAM. 2.  Transactional File Locking: Prevents file corruption when multiple devices (phone, desktop) access the same file simultaneously.</p> <p>How to configure: The <code>nextcloud_redis</code> container is running, but Nextcloud needs to be told to use it.</p> <ol> <li>Locate your <code>config.php</code> file (mapped in your volume, e.g., <code>/mnt/appdata/nextcloud/config/www/nextcloud/config/config.php</code>).</li> <li>Add or modify the following lines inside the <code>$CONFIG = array ( ... );</code> block:</li> </ol> <pre><code>  'memcache.local' =&gt; '\\\\OC\\\\Memcache\\\\APCu',\n  'memcache.distributed' =&gt; '\\\\OC\\\\Memcache\\\\Redis',\n  'memcache.locking' =&gt; '\\\\OC\\\\Memcache\\\\Redis',\n  'redis' =&gt; \n  array (\n    'host' =&gt; 'nextcloud_redis',\n    'port' =&gt; 6379,\n  ),\n</code></pre>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#a-php-memory-limit","level":3,"title":"A. PHP Memory Limit","text":"<p>By default, the container often limits PHP to 512MB, which causes sluggishness or crashes during heavy photo uploads.</p> <p>The Fix: Set the limit via environment variables in your <code>compose.yaml</code>:</p> <p><pre><code>services:\n  nextcloud:\n    # ... existing config ...\n    environment:\n      - PHP_MEMORY_LIMIT=2G\n      - PHP_UPLOAD_LIMIT=16G # Adjust based on your needs\n</code></pre> Run <code>docker compose up -d</code> to apply changes.</p>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/nextcloud/nextcloud/#b-preview-generator","level":3,"title":"B. Preview Generator","text":"<p>Nextcloud generates image thumbnails \"on the fly\" when you open a folder. If you open a folder with 1,000 photos, the server will freeze trying to generate them all at once.</p> <p>The Fix: Install the Preview Generator app to pre-render thumbnails in the background.</p> <ol> <li>Install App: Go to Apps (top right menu) &gt; Search \"Preview Generator\" &gt; Download &amp; Enable.</li> <li> <p>Run Initial Scan:</p> <p>Tip</p> <p>This command can take a long time. It is recommended to run this inside a <code>screen</code> or <code>tmux</code> session so it doesn't fail if your SSH connection drops.</p> <pre><code>docker exec -it nextcloud occ preview:generate-all -vvv\n</code></pre> </li> <li> <p>Add to Cron:     To ensure new photos get thumbnails automatically, open your host crontab (<code>crontab -e</code>) and add this line to run every 10 minutes:</p> <pre><code>*/10 * * * * docker exec -u abc nextcloud occ preview:pre-generate\n</code></pre> </li> </ol> <p>Sources:</p> <ul> <li> <p>Nextcloud Database Configuration</p> </li> <li> <p>Nextcloud Caching &amp; Locking</p> </li> </ul>","path":["Services","Nextcloud","Nextcloud Storage"],"tags":[]},{"location":"services/ollama/ollama/","level":1,"title":"Ollama - Self Hosted AI","text":"<p>Github  </p>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#ollama-installation","level":2,"title":"Ollama Installation","text":"<p>Official script:</p> <p>This will auto detect hardware and install necessary dependancies </p> <p><code>curl -fsSL https://ollama.com/install.sh | sh</code></p> <p>To run a model, use:</p> <p><code>ollama run gemma3</code> or <code>ollama run qwen2.5-coder:14b</code></p>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#models","level":2,"title":"Models","text":"<p>Current Best models would be: </p> <p>Here is the table formatted in Markdown.</p> Category Model Size Speed Use Case Coding Qwen 2.5 Coder 32B 19GB Medium Complex scripts, Docker, Python, Arch Linux help. Logic DeepSeek R1 32B 19GB Medium Debugging weird errors, math, logic puzzles. Writing Gemma 2 27B 16GB Fast/Med Emails, documentation, stories, cover letters. Speed Llama 3.1 8B 5GB Instant Quick facts, summarization, simple chat.","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#quick-pull-list","level":3,"title":"Quick Pull List","text":"<p>You can copy and paste this block to download them all at once:</p> <pre><code>ollama pull qwen2.5-coder:32b\nollama pull deepseek-r1:32b\nollama pull gemma2:27b\nollama pull llama3.1\n</code></pre>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#open-webui","level":2,"title":"Open-WebUI","text":"<p>For a nice GUI, you can use Open-WebUI.</p> <p>This compose assumes you have ollama running on host machines. For other configs, check the docs</p> <p>https://github.com/open-webui/open-webui</p> <pre><code>### --- OpenWebUI - Interface for Ollama --- ###\n# https://github.com/open-webui/open-webui\nservices:\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    restart: always\n    network_mode: host\n    #ports:\n    # - \"3000:8080\"\n    volumes:\n      - ./open-webui-data:/app/backend/data\n    extra_hosts:\n      - host.docker.internal:host-gateway\n</code></pre> <p>Access this at http://localhost:8080.</p> <p>Note</p> <p>To correctly show models in the WebUI, I had to manually set the connections (admin page &gt; settings) for Ollama API as <code>http://127.0.0.1:11434</code></p>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#rocm","level":2,"title":"ROCm","text":"<p>ROCm (Radeon Open Compute) is AMD's open-source software stack for GPU computing.</p> <p>This is needed to use the GPU as compute instead of just for graphical reasons. </p> <p>Installation is here: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html</p> <p>After installation, to verift it is working run:</p> <p><code>watch -n 1 rocm-smi</code> </p> <p>This will monitor the GPU so you can watch it work. </p> <p>While monitoring, run: </p> <p><code>ollama run qwen2.5-coder:14b \"some fancy shit here in rust\"</code></p> <p>You should see the GPU spike. </p> <p>OR</p> <p>Check the boot logs. Ollama is pretty clear on what is being used.</p> <p><code>sudo journalctl -u ollama --no-pager | grep \"rocm\"</code></p>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#optimization","level":2,"title":"Optimization","text":"","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#xpand-context-window","level":3,"title":"xpand Context Window","text":"<p>By default, Ollama uses a 4,096 token context window. This is too small for large code files or pasting logs. 6800 XT (16GB VRAM) has enough room to run the 14B model with a much larger \"memory\" of your conversation.</p> <ul> <li>Goal: Increase context to 16k or 32k.</li> <li>Why: Allows the model to see your entire config file or script at once.</li> </ul> <p>How to set it permanently:</p> <ol> <li> <p>Edit the Ollama service: <pre><code>sudo systemctl edit ollama.service\n</code></pre></p> </li> <li> <p>Add these environment variables to the <code>[Service]</code> section: <pre><code>[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n# Enable Flash Attention (Crucial for RDNA2 performance at high context)\nEnvironment=\"OLLAMA_FLASH_ATTENTION=1\"\n# Set default context to 32k (fits in your 16GB VRAM with 14B model)\nEnvironment=\"OLLAMA_NUM_CTX=32768\"\n</code></pre></p> </li> <li> <p>Save, exit, and restart: <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart ollama\n</code></pre></p> </li> </ol>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#model-selection","level":3,"title":"Model Selection","text":"<p>You have 64GB RAM, which puts you in a unique position. You can run models larger than your GPU VRAM.</p> <ul> <li>The \"Daily Driver\" (Fast &amp; Capable): <code>qwen2.5-coder:14b</code></li> <li>VRAM Usage: ~9GB (Model) + ~2-4GB (Context). Fits 100% on GPU.</li> <li>Speed: Instant.</li> <li>Use case: General questions, writing small scripts, Linux commands.</li> <li> <p>Optimization: Create a custom model file to lock in parameters (see below).</p> </li> <li> <p>The \"Heavy Lifter\" (Maximum Accuracy): <code>qwen2.5-coder:32b</code></p> </li> <li>VRAM Usage: ~20GB.</li> <li>Your Setup: 16GB on GPU + ~4GB spillover to System RAM.</li> <li>Speed: Slower (expect 4-8 tokens/sec), but significantly smarter at complex logic.</li> <li>Use case: Debugging a complex race condition, architecting a new system, or when 14B gets stuck.</li> </ul> <p>Recommendation: Keep both. Use 14B for speed, switch to 32B when you need a \"senior engineer\" to look at your code.</p>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#model-file","level":3,"title":"Model File","text":"<p>Don't just use the raw model. Create a custom \"Modelfile\" to bake in your preferences (Linux, Docker, minimal fluff).</p> <ol> <li>In Open WebUI, go to Workspace (left sidebar) -&gt; Models -&gt; Create a Model.</li> <li>Name: <code>Goose-Coder-14B</code></li> <li>Base Model: <code>qwen2.5-coder:14b</code></li> <li>System Prompt: <p>You are an expert Linux flight simulation technician and DevOps engineer. You prefer \"Gruvbox\" aesthetics where applicable. You specialize in Docker, Self-Hosting, and System Administration. Guidelines:</p> </li> </ol> <ol> <li>When providing code, use strictly official documentation sources.</li> <li>Do not use conversational filler (\"Here is the code you asked for\"). Just provide the solution.</li> <li>Prefer <code>docker compose</code> over <code>docker run</code>.</li> <li>Assume a modern Linux environment (Arch/Debian/Ubuntu).</li> </ol> <ol> <li>Advanced Params:</li> <li>Temperature: <code>0.2</code> (Lower is better for code; prevents hallucinations).</li> <li> <p>Context Length: <code>32768</code> (Ensure this matches your hardware capability).</p> </li> <li> <p>Save &amp; Use.</p> </li> </ol>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/ollama/ollama/#specific-model-locations","level":2,"title":"Specific model locations","text":"<pre><code>    sudo systemctl stop ollama\n</code></pre> <pre><code># Create the folder\nsudo mkdir -p /mnt/fast_ssd/ollama_models\n\n# (Optional) Move existing models if you already downloaded some\nsudo mv /usr/share/ollama/.ollama/models/* /mnt/fast_ssd/ollama_models/\n</code></pre> <pre><code>sudo chown -R ollama:ollama /mnt/fast_ssd/ollama_models\nsudo chmod -R 775 /mnt/fast_ssd/ollama_models\n</code></pre> <p>Update systemd service:</p> <pre><code>sudo systemctl edit ollama.service\n</code></pre> <p>Add this to .service </p> <pre><code>[Service]\nEnvironment=\"OLLAMA_MODELS=/mnt/fast_ssd/ollama_models\"\n</code></pre> <p>restart</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start ollama\n</code></pre> <p>Verify</p> <pre><code>ollama list\n\n# pull new model\nollama pull tinyllama\n</code></pre> <p>Check folder </p> <pre><code>ls -lh /mnt/fast_ssd/ollama_models/blobs\n</code></pre>","path":["Services","Ollama","Ollama - Self Hosted AI"],"tags":[]},{"location":"services/omada/omada/","level":1,"title":"TP-Link Omada Controller","text":"","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#overview","level":2,"title":"Overview","text":"<p>The Omada Software Defined Networking (SDN) platform is TP-Link's centralized management solution. It manages all your TP-Link hardwaer from a single interface.</p> <p>I have not gotten too into the deeper settings of omada, but this is basics from what I did. </p>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#installation","level":2,"title":"Installation","text":"","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#1-volume-creation","level":3,"title":"1. Volume Creation","text":"<p>The <code>compose.yaml</code> configuration defines the storage volumes as <code>external: true</code>. This means Docker will not create them automatically. You must create them manually before launching the container, or the stack will fail to start.</p> <pre><code>docker volume create omada_data\ndocker volume create omada_logs\ndocker volume create omada_work\n</code></pre>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#2-docker-compose","level":3,"title":"2. Docker Compose","text":"<p>This setup uses the community-standard <code>mbentley/omada-controller</code> image. The container is configured with <code>network_mode: host</code> to ensure the controller can easily discover devices on the local network via Layer 2 broadcasts.</p> <p><code>compose.yaml</code></p> <pre><code>### Omada Controller - TP-Link / Omada network controller ###\n\nservices:\n  omada-controller:\n    image: mbentley/omada-controller:latest\n    container_name: omada-controller\n    restart: always\n    network_mode: host\n    environment:\n      - MANAGE_HTTP_PORT=8088\n      - MANAGE_HTTPS_PORT=8043\n      - PORTAL_HTTP_PORT=8088\n      - PORTAL_HTTPS_PORT=8843\n      - SHOW_SERVER_LOGS=true\n      - SHOW_MONGODB_LOGS=false\n      - TZ=America/Chicago\n    # networks: \n    #   - proxy_net  \n    volumes:   \n      - omada_data:/opt/tplink/EAPController/data\n      - omada_logs:/opt/tplink/EAPController/logs\n      - omada_work:/opt/tplink/EAPController/work\n\nvolumes:\n  omada_data:\n    external: true\n  omada_logs:\n    external: true\n  omada_work:\n    external: true\n\n# # --- Network Definitions --- #\n# networks: \n#   proxy_net: \n#     external: true\n</code></pre> <p>Start the controller:</p> <pre><code>docker compose up -d\n</code></pre>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#initial-setup-adoption","level":2,"title":"Initial Setup &amp; Adoption","text":"<p>Once the controller is running, follow this workflow to bring the network online.</p>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#1-access-the-interface","level":3,"title":"1. Access the Interface","text":"<ul> <li>URL: <code>https://&lt;server-ip&gt;:8043</code></li> <li>Note: You may receive a security warning because the SSL certificate is self-signed. This is normal; proceed past it.</li> <li>First Login: The default is often <code>admin</code> / <code>admin</code>, but the setup wizard will force a password change immediately.</li> </ul>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#2-device-adoption","level":3,"title":"2. Device Adoption","text":"<p>The controller does not automatically manage devices plugged into the network; they must be explicitly \"Adopted\".</p> <ol> <li>Go to the Devices tab (Access Point icon).</li> <li>Devices (Gateway, Switches, APs) should be listed as \"Pending\".</li> <li>Click the Adopt button on the right.</li> <li>The status will cycle: Provisioning -&gt; Configuring -&gt; Connected.</li> </ol>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#3-wanlan-configuration","level":3,"title":"3. WAN/LAN Configuration","text":"<p>If using an Omada Gateway (like the ER605):</p> <ul> <li>Navigate to Settings (Gear icon) &gt; Wired Networks &gt; Internet.</li> <li>Configure the WAN type provided by the ISP (DHCP, PPPoE, or Static).</li> </ul>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#configuration-features","level":2,"title":"Configuration Features","text":"","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#wireless-networks-ssids","level":3,"title":"Wireless Networks (SSIDs)","text":"<ul> <li>Location: Settings &gt; Wireless Networks.</li> <li>Usage: Create multiple SSIDs here (e.g., Main, IoT, Guest).</li> <li>Guest Network: Checking the \"Guest Network\" box automatically applies isolation rules, preventing those clients from accessing the internal LAN.</li> </ul>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#vlans-virtual-lans","level":3,"title":"VLANs (Virtual LANs)","text":"<p>To segment traffic (e.g., separating IoT devices from the main PC):</p> <ol> <li>Create Interface: Settings &gt; Wired Networks &gt; LAN. Create a new Interface with a VLAN ID (e.g., <code>20</code>).</li> <li>Tag Ports: Ensure managed switches utilize the correct profiles to pass this VLAN tag to the APs.</li> <li>Assign to Wi-Fi: In Wireless settings, assign an SSID to use that specific VLAN ID.</li> </ol>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#acls-access-control-lists","level":3,"title":"ACLs (Access Control Lists)","text":"<ul> <li>Location: Settings &gt; Network Security &gt; ACL.</li> <li>Usage: Use \"Switch ACLs\" or \"Gateway ACLs\" to block traffic between VLANs (e.g., Block the IoT VLAN from accessing the Main VLAN).</li> </ul>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/omada/omada/#maintenance","level":2,"title":"Maintenance","text":"<ul> <li>Backups: Go to Settings &gt; Maintenance &gt; Backup &amp; Restore. Set up \"Auto Backup\" to save the config locally. Since the <code>omada_data</code> volume is mapped, these backups persist on the disk.</li> <li>Roaming: Enable Fast Roaming (802.11r) in Site Settings to help devices switch between APs smoothly. Note: Test this first, as some older IoT devices struggle with it.</li> <li>AI Optimization: The \"AI WLAN Optimization\" tool scans the RF environment and automatically adjusts channels and transmit power to reduce interference. Run this once after the initial deployment.</li> </ul>","path":["Services","Omada","TP-Link Omada Controller"],"tags":[]},{"location":"services/pihole/pihole/","level":1,"title":"Pi-hole DNS Blocker","text":"","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#overview","level":2,"title":"Overview","text":"<p>The Pi-hole® is a DNS sinkhole that protects your devices from unwanted content without installing any client-side software.</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#installation-methods","level":2,"title":"Installation Methods","text":"","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#option-1-docker-compose-recommended","level":3,"title":"Option 1: Docker Compose (Recommended)","text":"<p>This is the preferred method for this homelab setup. It integrates easily with existing Docker networks and storage.</p> <pre><code># More info at [https://github.com/pi-hole/docker-pi-hole/](https://github.com/pi-hole/docker-pi-hole/) and [https://docs.pi-hole.net/](https://docs.pi-hole.net/)\nservices:\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:latest\n    ports:\n      # DNS Ports\n      - \"53:53/tcp\"\n      - \"53:53/udp\"\n      # Default HTTP Port\n      - \"80:80/tcp\"\n      # Default HTTPs Port. FTL will generate a self-signed certificate\n      - \"443:443/tcp\"\n      # Uncomment the below if using Pi-hole as your DHCP Server\n      #- \"67:67/udp\"\n      # Uncomment the line below if you are using Pi-hole as your NTP server\n      #- \"123:123/udp\"\n    environment:\n      # Set the appropriate timezone for your location from\n      # [https://en.wikipedia.org/wiki/List_of_tz_database_time_zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g:\n      TZ: 'Europe/London'\n      # Set a password to access the web interface. Not setting one will result in a random password being assigned\n      FTLCONF_webserver_api_password: 'correct horse battery staple'\n      # If using Docker's default `bridge` network setting the dns listening mode should be set to 'ALL'\n      FTLCONF_dns_listeningMode: 'ALL'\n    # Volumes store your data between container upgrades\n    volumes:\n      # For persisting Pi-hole's databases and common configuration file\n      - './etc-pihole:/etc/pihole'\n      # Uncomment the below if you have custom dnsmasq config files that you want to persist. Not needed for most starting fresh with Pi-hole v6. If you're upgrading from v5 you and have used this directory before, you should keep it enabled for the first v6 container start to allow for a complete migration. It can be removed afterwards. Needs environment variable FTLCONF_misc_etc_dnsmasq_d: 'true'\n      #- './etc-dnsmasq.d:/etc/dnsmasq.d'\n    cap_add:\n      # See [https://github.com/pi-hole/docker-pi-hole#note-on-capabilities](https://github.com/pi-hole/docker-pi-hole#note-on-capabilities)\n      # Required if you are using Pi-hole as your DHCP server, else not needed\n      - NET_ADMIN\n      # Required if you are using Pi-hole as your NTP client to be able to set the host's system time\n      - SYS_TIME\n      # Optional, if Pi-hole should get some more processing time\n      - SYS_NICE\n    restart: unless-stopped\n</code></pre> <p>Note On Capabilities</p> <p>FTLDNS expects capabilities like <code>CAP_NET_BIND_SERVICE</code> (binding port 53) and <code>CAP_NET_ADMIN</code> (DHCP).</p> <pre><code>The docker image automatically grants these if available. We explicitly add `NET_ADMIN` in the compose file above to ensure smooth operation.\n</code></pre>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#option-2-automated-install-bare-metal","level":3,"title":"Option 2: Automated Install (Bare Metal)","text":"<p>If you prefer to install directly on the OS:</p> <pre><code>curl -sSL [https://install.pi-hole.net](https://install.pi-hole.net) | bash\n</code></pre> <p>Security Warning</p> <p>Piping to <code>bash</code> is a controversial topic. It prevents you from reading code before it runs. If you prefer to review the code first, use the methods below.</p> <p>Alternatives:</p> <ul> <li> <p>Clone &amp; Run: <pre><code>git clone --depth 1 [https://github.com/pi-hole/pi-hole.git](https://github.com/pi-hole/pi-hole.git) Pi-hole\ncd \"Pi-hole/automated install/\"\nsudo bash basic-install.sh\n</code></pre></p> </li> <li> <p>Download Manually: <pre><code>wget -O basic-install.sh [https://install.pi-hole.net](https://install.pi-hole.net)\nsudo bash basic-install.sh\n</code></pre></p> </li> </ul>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#configuration-network-setup","level":2,"title":"Configuration &amp; Network Setup","text":"<p>Once installed, you must configure your network to actually use the Pi-hole.</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#router-configuration","level":3,"title":"Router Configuration","text":"<p>Configure your router's DHCP settings to use the Pi-hole's IP address as the Primary DNS Server. This ensures all connected devices are automatically protected.</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#host-configuration-the-server-itself","level":3,"title":"Host Configuration (The Server itself)","text":"<p>The host machine running the Pi-hole container does not automatically use it.</p> <p>Method 1: DHCPCD (Standard Linux) Add to <code>/etc/dhcpcd.conf</code>:</p> <pre><code>static domain_name_servers=127.0.0.1\n</code></pre> <p>Warning</p> <p>If your Pi-hole host uses itself as upstream DNS and Pi-hole fails, the host loses DNS. This can prevent repair attempts (e.g., <code>pihole -r</code>) because the internet connection will appear \"down.\"</p> <p>Method 2: Permissions Pi-hole v6 uses a new API. To allow your user to run CLI commands without a password:</p> <pre><code># Debian/Ubuntu/RasPi OS\nsudo usermod -aG pihole $USER\n\n# Alpine\nsudo addgroup pihole $USER\n</code></pre>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#advanced-local-dns-wildcards","level":2,"title":"Advanced: Local DNS &amp; Wildcards","text":"<p>To make your local services reachable via a custom domain (e.g., <code>service.vanth.me</code>) while on your LAN, we use <code>dnsmasq</code> to intercept requests and point them to your Reverse Proxy (Nginx Proxy Manager).</p> <ol> <li>Create Config: Create <code>/etc/dnsmasq.d/99-wildcard.conf</code> inside the container (or mapped volume): <pre><code>address=/.vanth.me/192.168.0.116\n</code></pre></li> </ol> <p>(Replace <code>192.168.0.116</code> with your Nginx Proxy Manager Host IP) 2. Activate: In Pi-hole Settings &gt; Misc &gt; Enable misc.etc_dnsmasq_d. 3. Flush: Run <code>pihole restartdns</code> in the container or <code>ipconfig /flushdns</code> on your client machine.</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#tailscale-subnet-router-integration","level":2,"title":"Tailscale Subnet Router Integration","text":"<p>This setup allows you to access your home network (and use your Pi-hole for DNS) from anywhere in the world, without opening ports on your firewall.</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#1-enable-ip-forwarding","level":3,"title":"1. Enable IP Forwarding","text":"<p>You must allow the host to route traffic.</p> <pre><code># Enable IPv4 forwarding immediately\nsudo sysctl -w net.ipv4.ip_forward=1\n\n# Make the change permanent across reboots\necho 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n</code></pre>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#2-advertise-subnet-routes","level":3,"title":"2. Advertise Subnet Routes","text":"<p>Tell Tailscale that this machine can route traffic to your LAN (192.168.0.x).</p> <pre><code># Advertise the 192.168.0.0/24 subnet\nsudo tailscale up --advertise-routes=192.168.0.0/24\n</code></pre>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#3-approve-route","level":3,"title":"3. Approve Route","text":"<ol> <li>Go to the Tailscale Admin Console.</li> <li>Find your router node (the machine you just configured).</li> <li>Click the ... menu -&gt; Edit route settings.</li> <li>Approve the <code>192.168.0.0/24</code> route.</li> </ol>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/pihole/pihole/#4-how-it-works","level":3,"title":"4. How it works","text":"<ol> <li>You set Pi-hole as your Nameserver in Tailscale DNS settings.</li> <li>Your remote phone queries <code>service.vanth.me</code>.</li> <li>Pi-hole resolves this to the LAN IP (<code>192.168.0.116</code>).</li> <li>Because the Subnet Route is active, your phone sends the traffic through the Tailscale tunnel to your home server.</li> </ol> <p>```</p>","path":["Services","Pihole","Pi-hole DNS Blocker"],"tags":[]},{"location":"services/tailscale/tailscale/","level":1,"title":"Tailscale Quick Guide","text":"","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#overview","level":2,"title":"Overview","text":"<p>Tailscale is a zero-configuration VPN that creates a secure private network (a \"Tailnet\") between devices. Unlike traditional VPNs that route traffic through a central gateway, Tailscale establishes a mesh network where devices connect directly to each other (peer-to-peer) using the WireGuard protocol.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#key-benefits","level":3,"title":"Key Benefits","text":"<ul> <li>No Port Forwarding: It traverses firewalls and NATs automatically. No router ports need to be opened.</li> <li>WireGuard Based: utilizes modern, high-performance encryption.</li> <li>Identity Based: Authentication is handled via existing identity providers (Google, Microsoft, GitHub) rather than managing separate VPN keys.</li> </ul>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#installation-setup","level":2,"title":"Installation &amp; Setup","text":"<p>Setting up a basic mesh network involves three steps.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#1-account-creation","level":3,"title":"1. Account Creation","text":"<p>Navigate to tailscale.com and sign up using an existing Single Sign-On (SSO) provider.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#2-client-installation","level":3,"title":"2. Client Installation","text":"<p>Install the application on every device intended for the mesh (Windows, macOS, Linux, iOS, Android).</p> <p>Linux Installation Command:</p> <pre><code>curl -fsSL https://tailscale.com/install.sh | sh\n</code></pre>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#3-authentication","level":3,"title":"3. Authentication","text":"<p>Open the application on the device and log in. The device will immediately join the Tailnet.</p> <p>MagicDNS: Tailscale automatically assigns a readable domain name to every device (e.g., <code>server-pc</code> or <code>iphone</code>). Devices can be accessed via <code>ping</code> or SSH using this hostname, eliminating the need to memorize IP addresses.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#advanced-features","level":2,"title":"Advanced Features","text":"<p>Once connected, several configuration options are available to extend the network's utility.</p> Feature Description Best Use Case Exit Nodes Routes all internet traffic through a specific device on the network. Securing traffic on public Wi-Fi; appearing to be at a specific location while traveling. Subnet Routers Allows access to LAN devices that cannot run Tailscale (printers, IoT, legacy servers). Remote access to an entire home LAN without installing the client on every device. Taildrop A peer-to-peer file transfer tool. Transferring files between different operating systems instantly.","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#configuring-an-exit-node-linux","level":3,"title":"Configuring an Exit Node (Linux)","text":"<p>To configure a Linux server to act as an Exit Node:</p> <ol> <li>Enable IP Forwarding: Ensure the host allows packet forwarding.</li> <li> <p>Advertise the Node: <pre><code>sudo tailscale up --advertise-exit-node\n</code></pre></p> </li> <li> <p>Approve: In the Admin Console (web), locate the machine, open the ... menu &gt; Edit route settings, and check \"Use as exit node.\"</p> </li> </ol>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#subnet-router-configuration-pi-hole-integration","level":2,"title":"Subnet Router Configuration (Pi-hole Integration)","text":"<p>This setup allows remote access to local LAN services using a Pi-hole for DNS resolution.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#1-enable-ip-forwarding","level":3,"title":"1. Enable IP Forwarding","text":"<p>Packet forwarding must be enabled at the kernel level.</p> <pre><code># 1. Enable IPv4 forwarding immediately\nsudo sysctl -w net.ipv4.ip_forward=1\n\n# 2. Make the change permanent across reboots\necho 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n</code></pre>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#2-advertise-subnet-routes","level":3,"title":"2. Advertise Subnet Routes","text":"<p>Run the following command to advertise the local subnet (replace <code>192.168.0.0/24</code> with the actual LAN subnet):</p> <pre><code>sudo tailscale up --advertise-routes=192.168.0.0/24\n</code></pre>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#3-approve-route","level":3,"title":"3. Approve Route","text":"<ol> <li>Navigate to the Tailscale Admin Console.</li> <li>Go to Machines -&gt; Select the Router Node -&gt; Edit route settings.</li> <li>Approve the previously advertised subnet.</li> </ol>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#4-dns-resolution-flow","level":3,"title":"4. DNS Resolution Flow","text":"<p>Once configured, the resolution flow works as follows:</p> <ol> <li>DNS Config: The Pi-hole is set as the Nameserver in Tailscale settings.</li> <li>Query: A remote client queries <code>service.domain.me</code>.</li> <li>Resolution: The Pi-hole responds with the service's local LAN IP.</li> <li>Routing: Since the subnet route is approved, the remote client traffic is automatically routed through the Tailscale tunnel to the Subnet Router, which accesses the device on the LAN.</li> </ol>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#access-control-lists-acls","level":2,"title":"Access Control Lists (ACLs)","text":"<p>By default, every device in a Tailnet can communicate with every other device. ACLs (defined in JSON) are used to restrict this access.</p> <p>Example: Server Isolation This rule allows Admins to access everything, but prevents servers from initiating connections to personal devices.</p> <pre><code>{\n  \"acls\": [\n    // Admins can access everything\n    { \"action\": \"accept\", \"src\": [\"group:admin\"], \"dst\": [\"*:*\"] },\n\n    // Servers can only talk to other servers (tagged devices)\n    { \"action\": \"accept\", \"src\": [\"tag:server\"], \"dst\": [\"tag:server:*\"] }\n  ]\n}\n</code></pre> <p>Note: Tailscale uses \"Tags\" (e.g., <code>tag:server</code>) to manage permissions for headless devices, rather than relying on user identities. This ensures services do not lose access if a specific user account is removed.</p>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/tailscale/tailscale/#maintenance-best-practices","level":2,"title":"Maintenance &amp; Best Practices","text":"<ul> <li>Key Expiry: By default, authentication keys expire every 180 days.</li> <li> <p>Recommendation: In the Admin Console, select \"Disable Key Expiry\" for servers and headless devices to prevent remote lockouts.</p> </li> <li> <p>Tailscale SSH: Enable this feature to allow SSH access between Tailscale devices using Tailscale identity verification, removing the need to manage local SSH keys.</p> </li> <li>Battery Life: While efficient, toggling Tailscale off on mobile devices when not accessing home services can conserve battery life.</li> </ul>","path":["Services","Tailscale","Tailscale Quick Guide"],"tags":[]},{"location":"services/utilities/convertx/","level":1,"title":"ConvertX","text":"<p>Github  </p> <p>A self-hosted online file converter. Supports over a thousand different formats. Written with TypeScript, Bun and Elysia.</p>","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/convertx/#features","level":2,"title":"Features","text":"<ul> <li>Convert files to different formats</li> <li>Process multiple files at once</li> <li>Password protection</li> <li>Multiple accounts</li> </ul>","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/convertx/#converters-supported","level":2,"title":"Converters supported","text":"Converter Use case Converts from Converts to Inkscape Vector images 7 17 libjxl JPEG XL 11 11 resvg SVG 1 1 Vips Images 45 23 libheif HEIF 2 4 XeLaTeX LaTeX 1 1 Calibre E-books 26 19 LibreOffice Documents 41 22 Dasel Data Files 5 4 Pandoc Documents 43 65 msgconvert Outlook 1 1 dvisvgm Vector images 4 2 ImageMagick Images 245 183 GraphicsMagick Images 167 130 Assimp 3D Assets 77 23 FFmpeg Video ~472 ~199 Potrace Raster to vector 4 11 VTracer Raster to vector 8 1 Markitdown Documents 6 1","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/convertx/#deployment","level":2,"title":"Deployment","text":"<p>[!WARNING] If you can't login, make sure you are accessing the service over localhost or https otherwise set HTTP_ALLOWED=true</p> <pre><code># docker-compose.yml\nservices:\n  convertx:\n    image: ghcr.io/c4illin/convertx\n    container_name: convertx\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - JWT_SECRET=aLongAndSecretStringUsedToSignTheJSONWebToken1234 # will use randomUUID() if unset\n      # - HTTP_ALLOWED=true # uncomment this if accessing it over a non-https connection\n    volumes:\n      - ./data:/app/data\n</code></pre> <p>or</p> <pre><code>docker run -p 3000:3000 -v ./data:/app/data ghcr.io/c4illin/convertx\n</code></pre> <p>Then visit <code>http://localhost:3000</code> in your browser and create your account. Don't leave it unconfigured and open, as anyone can register the first account.</p> <p>If you get unable to open database file run <code>chown -R $USER:$USER path</code> on the path you choose.</p>","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/convertx/#environment-variables","level":3,"title":"Environment variables","text":"<p>All are optional, JWT_SECRET is recommended to be set.</p> Name Default Description JWT_SECRET when unset it will use the value from randomUUID() A long and secret string used to sign the JSON Web Token ACCOUNT_REGISTRATION false Allow users to register accounts HTTP_ALLOWED false Allow HTTP connections, only set this to true locally ALLOW_UNAUTHENTICATED false Allow unauthenticated users to use the service, only set this to true locally AUTO_DELETE_EVERY_N_HOURS 24 Checks every n hours for files older then n hours and deletes them, set to 0 to disable WEBROOT The address to the root path setting this to \"/convert\" will serve the website on \"example.com/convert/\" FFMPEG_ARGS Arguments to pass to the input file of ffmpeg, e.g. <code>-hwaccel vaapi</code>. See https://github.com/C4illin/ConvertX/issues/190 for more info about hw-acceleration. FFMPEG_OUTPUT_ARGS Arguments to pass to the output of ffmpeg, e.g. <code>-preset veryfast</code> HIDE_HISTORY false Hide the history page LANGUAGE en Language to format date strings in, specified as a BCP 47 language tag UNAUTHENTICATED_USER_SHARING false Shares conversion history between all unauthenticated users MAX_CONVERT_PROCESS 0 Maximum number of concurrent conversion processes allowed. Set to 0 for unlimited.","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/convertx/#docker-images","level":3,"title":"Docker images","text":"<p>There is a <code>:latest</code> tag that is updated with every release and a <code>:main</code> tag that is updated with every push to the main branch. <code>:latest</code> is recommended for normal use.</p> <p>The image is available on GitHub Container Registry and Docker Hub.</p> Image What it is <code>image: ghcr.io/c4illin/convertx</code> The latest release on ghcr <code>image: ghcr.io/c4illin/convertx:main</code> The latest commit on ghcr <code>image: c4illin/convertx</code> The latest release on docker hub <code>image: c4illin/convertx:main</code> The latest commit on docker hub <p> </p>","path":["Services","Utilities","ConvertX"],"tags":[]},{"location":"services/utilities/obslivesync/","level":1,"title":"Self-Hosted Obsidian LiveSync with Tailscale","text":"<p>Github </p>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#overview","level":2,"title":"Overview","text":"<p>This covers setting up a self-hosted synchronization backend for Obsidian using CouchDB and Obsidian LiveSync. I used the offical guide and it...did not go well, so here's an adjusted version </p> <p>Tailscale is used to secure the connection, allowing devices to sync remotely without opening ports on the router or exposing the database to the public internet.</p>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#installation","level":2,"title":"Installation","text":"","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#1-file-permissions-critical","level":3,"title":"1. File Permissions (Critical)","text":"<p>The official CouchDB image runs as a specific internal user with ID <code>5984</code>. Docker typically creates volume directories as <code>root</code>, which causes the container to crash with <code>Permission denied</code> errors.</p> <p>You must pre-create the directories and assign the correct ownership before starting the container.</p> <pre><code>mkdir -p ./couchdb-data ./couchdb-etc\nsudo chown -R 5984:5984 ./couchdb-data\nsudo chown -R 5984:5984 ./couchdb-etc\n</code></pre> <p>Note</p> <p>If you need to edit files in <code>couchdb-etc</code> manually later, you may need to temporarily change ownership back to your user (<code>sudo chown -R $USER:$USER ...</code>) or use <code>sudo</code>.</p>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#2-configuration-file-dockerini","level":3,"title":"2. Configuration File (<code>docker.ini</code>)","text":"<p>A configuration file must be injected to handle Large File Support (LFS), CORS headers (essential for remote access), and authentication limits.</p> <p>Create the file at <code>./couchdb-etc/docker.ini</code>:</p> <pre><code>[couchdb]\nsingle_node = true\nmax_document_size = 50000000\n\n[chttpd]\nbind_address = 0.0.0.0\nport = 5984\nrequire_valid_user = true\nmax_http_request_size = 4294967296\nenable_cors = true\n\n[chttpd_auth]\nrequire_valid_user = true\nauthentication_redirect = /_utils/session.html\n\n[httpd]\nWWW-Authenticate = Basic realm=\"couchdb\"\nenable_cors = true\n\n[cors]\n# Wildcard required for Tailscale/VPN roaming where Client IPs change\norigins = *\ncredentials = true\n# Explicit headers required for Obsidian LiveSync Plugin PUT/Auth operations\nheaders = accept, authorization, content-type, origin, referer\nmethods = GET, PUT, POST, HEAD, DELETE\nmax_age = 3600\n</code></pre>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#3-docker-compose","level":3,"title":"3. Docker Compose","text":"<p>Create the <code>compose.yaml</code> file.</p> <p>Environment Variables</p> <p>This compose file uses <code>${COUCHDB_USER}</code> and <code>${COUCHDB_PASSWORD}</code>. You must either create a <code>.env</code> file in the same directory with these values or replace them directly in the file.</p> <pre><code>services:\n  couchdb:\n    image: couchdb:latest\n    container_name: couchdb-for-ols\n    user: 5984:5984\n    environment:\n      - COUCHDB_USER=${COUCHDB_USER}  #Please change as you like.\n      - COUCHDB_PASSWORD=${COUCHDB_PASS} #Please change as you like.\n    volumes:\n      - ./couchdb-data:/opt/couchdb/data\n      - ./couchdb-etc:/opt/couchdb/etc/local.d\n    ports:\n      - 5984:5984\n    restart: unless-stopped\n</code></pre> <p>Start the container:</p> <pre><code>docker compose up -d\n</code></pre>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#configuration","level":2,"title":"Configuration","text":"","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#1-database-initialization","level":3,"title":"1. Database Initialization","text":"<p>Once the container is running, the database structure must be initialized.</p> <p>Option A: Automatic Script Run this command on the host machine:</p> <pre><code>curl -s https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/couchdb/couchdb-init.sh | bash\n</code></pre> <ul> <li>Success: The output will end with <code>&lt;-- Configuring CouchDB by REST APIs Done!</code>.</li> </ul> <p>Option B: Manual Credential Injection If the script fails with errors like <code>ERROR: Hostname missing</code> (common in some Docker environments), run the command with specific credentials injected:</p> <pre><code>curl -s https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/couchdb/couchdb-init.sh | hostname=http://&lt;YOUR SERVER IP&gt;:5984 username=&lt;INSERT USERNAME HERE&gt; password=&lt;INSERT PASSWORD HERE&gt; bash\n</code></pre>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#2-generate-setup-uri","level":3,"title":"2. Generate Setup URI","text":"<p>The easiest way to configure devices is to generate a \"Setup URI\" that contains all connection strings and encryption keys. This can be run using <code>deno</code> on a desktop or server.</p> <pre><code># Set variables\nexport hostname=https://you.tailscale.ip:5984 # Use Tailscale IP\nexport database=obsidiannotes \nexport passphrase=dfsapkdjaskdjasdas # This is for encryption\nexport username=johndoe\nexport password=abc123 # Matches COUCHDB_PASSWORD in compose.yaml\n\n# Run generator\ndeno run -A https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/flyio/generate_setupuri.ts\n</code></pre> <p>Output:</p> <pre><code>obsidian://setuplivesync?settings=%5B%22tm2DpsOE74nJAryprZO2M93wF%2Fvg.......\n\nYour passphrase of Setup-URI is: patient-haze\nThis passphrase is never shown again, so please note it in a safe place.\n</code></pre> <ul> <li>Save the Setup URI: This is needed to configure every device.</li> <li>Save the Passphrase: This is needed to decrypt data on new devices.</li> </ul>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#client-setup","level":2,"title":"Client Setup","text":"<p>CRITICAL WARNING: Customization Sync</p> <p>During setup, the plugin may ask to enable \"Customization Sync\" or \"Configuration Sync\" (plugins, themes, settings). DO NOT ENABLE THIS. Syncing configuration files between different platforms (e.g., Windows vs. Android) causes boot loops, UI corruption, and file path conflicts. Decline all pop-ups related to syncing hidden folders (<code>.obsidian</code>). Only sync markdown notes and media.  When I used this originally it was a fucking nightmare to get things syncing correctly, so I'd avoid it. </p>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#network-configuration-tailscale","level":3,"title":"Network Configuration (Tailscale)","text":"<p>When connecting via mobile devices over Tailscale, mobile OSs often block cleartext HTTP traffic to hostnames.</p> <ul> <li>Do NOT use: <code>http://my-server-name:5984</code></li> <li>USE: <code>http://100.x.y.z:5984</code> (The specific Tailscale IP of the server).</li> </ul>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#android-optimization","level":3,"title":"Android Optimization","text":"<p>To prevent \"Silent Drift\" (where sync stops quietly in the background):</p> <ol> <li>Battery: Set the Obsidian app to Unrestricted / No Optimization in Android settings.</li> <li>Plugin Settings: In Obsidian LiveSync settings, switch the sync mode to \"Periodic\" (30s) or \"Adaptive\".</li> </ol>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#primary-device-setup-first-time","level":3,"title":"Primary Device Setup (First Time)","text":"<p>Perform these steps on the first device to initialize the database structure.</p> <ol> <li>In Obsidian, open the LiveSync plugin.</li> <li>Select \"I am setting this up for the first time\".</li> <li>Paste the Setup URI generated earlier.</li> <li>Select \"Restart and initialize server\".</li> <li>Confirmation: Select YES on all overwrite prompts.</li> <li>Remote Config Fail: If you see \"Fetch remote config failed\", select \"Skip and Proceed\" (this is normal for an empty database).</li> <li>Send Chunks: Select NO.</li> <li>Config Doctor: Select NO on all prompts.</li> <li>Enable: Under Sync Settings, enable the LiveSync preset configuration.</li> </ol>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#adding-additional-devices","level":3,"title":"Adding Additional Devices","text":"<p>Perform these steps for additional phones, laptops, or tablets.</p> <ol> <li>Open the LiveSync plugin.</li> <li>Paste the Setup URI and enter the Passphrase (<code>patient-haze</code> in the example above).</li> <li>Select \"Restart and fetch data\".</li> <li>Reset Sync: Choose Merge or Discard Local depending on preference.</li> <li>Optional Features: Select OK.</li> <li>Config Doctor: Select NO on all prompts.</li> <li>Enable: Under Sync Settings, enable the LiveSync preset configuration.</li> </ol>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#maintenance","level":2,"title":"Maintenance","text":"","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#updating-couchdb","level":3,"title":"Updating CouchDB","text":"<p>To update the database version, pull the latest image and restart the container.</p> <pre><code>docker compose pull\ndocker compose up -d\n</code></pre>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]},{"location":"services/utilities/obslivesync/#troubleshooting","level":3,"title":"Troubleshooting","text":"<ul> <li>Permission Denied: If the container loops on startup, check that the <code>./couchdb-data</code> folder is owned by <code>5984:5984</code>.</li> <li>CORS Errors: If connection fails from a new device, ensure <code>origins = *</code> is set in the <code>docker.ini</code>.</li> </ul>","path":["Services","Utilities","Self-Hosted Obsidian LiveSync with Tailscale"],"tags":[]}]}